{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#ovn-kubernetes-a-robust-kubernetes-networking-provider","title":"OVN Kubernetes: A Robust Kubernetes Networking Provider","text":"<p>OVN Kubernetes (Open Virtual Networking - Kubernetes) is an open-source project that provides a robust networking solution for Kubernetes clusters with OVN (Open Virtual Networking) and Open vSwitch (Open Virtual Switch) at its core. It is a Kubernetes networking conformant plugin written according to the CNI (Container Network Interface) specifications.</p>"},{"location":"#challenges-for-cluster-networking-in-kubernetes-ecosystem","title":"Challenges for Cluster Networking in Kubernetes Ecosystem","text":"<ul> <li>Kubernetes at its core is a bunch of powerful APIs like Pods, Services, EndpointSlices   and NetworkPolicies.<ul> <li>If we look at The Kubernetes Network Model it imposes a set of fundamental requirements for how networking is expected to behave for pods in a cluster.</li> <li>At the end of the day we need a robust networking platform to fulfill those requirements.</li> </ul> </li> <li>Handling complicated telco and enterprise networking scenarios.<ul> <li>Kubernetes Cluster Networking Model addresses the basic set of problems such as communication between containers in a pod, communication between pods, allowing external (outside the cluster) entities to talk to pods via services, ingress and gateway-api on a primary network level using clusterCIDRs and serviceCIDRs.</li> <li>However when we start talking about more powerful networking abstractions like secondary networks, multi-homing and more fine grained cluster egress controls outside the normal node SNATing the Kubernetes Networking Model falls short</li> </ul> </li> <li>Lifecycle management of networking infrastructure.<ul> <li>In addition to having an implementation that fulfills the basic requirements of the Kubernetes Networking model, we also need a level driven controller that automatically takes care of the lifecycle and health of the critical networking aspects.</li> </ul> </li> <li>Networking on Kubernetes is not simple.<ul> <li>There are complicated features such as service traffic policies, terminating endpoints, topology aware hints which involve multiple moving pieces and components that needs a \"driver\" that ensures they work as expected.</li> </ul> </li> </ul>"},{"location":"#what-is-ovn-kubernetes","title":"What is OVN Kubernetes?","text":"<p>OVN Kubernetes was designed to precisely solve the above problems in a Kubernetes cluster. The OVN Kubernetes plugin watches the Kubernetes API. It acts on the generated Kubernetes cluster events by creating and configuring the corresponding OVN logical constructs in the OVN database for those events. OVN (which is an abstraction on top of Open vSwitch) converts these logical constructs into logical flows in its database and programs the OpenFlow flows on the node, which enables networking on a Kubernetes cluster.</p> <p> </p> <p>The key functionalities and features that OVN Kubernetes provides include:</p> <ul> <li>Kubernetes Core Networking Conformance<ul> <li>Creates pod networking including IP Address Management (IPAM) allocation and virtual ethernet (veth) interface for the pod.</li> <li>Programs overlay based networking implementation for Kubernetes clusters using Generic Network Virtualization Encapsulation GENEVE tunnels that enables pod-to-pod communication.</li> <li>Implements Kubernetes Services &amp; EndpointSlices through OVN Load Balancers.</li> <li>Implements Kubernetes NetworkPolicies and AdminNetworkPolicies through OVN Access Control Lists (ACLs).</li> <li>Supports IPv4/IPv6 Dual-Stack clusters.</li> </ul> </li> <li>Fine grained Cluster Egress Traffic Controls<ul> <li>Multiple External Gateways (MEG) allows for multiple dynamically or statically assigned egress next-hop gateways by utilizing OVN ECMP routing features.</li> <li>Implements Quality of Service (QoS) Differentiated Services Code Point (DSCP) for traffic egressing the cluster through OVN QoS.</li> <li>Provides ability to send egress traffic from cluster workloads using an admin-configured source IP (EgressIP) to outside the cluster using OVN Logical Router Policies and Network Address Translations.</li> <li>Provides ability to send egress traffic from cluster workloads using the service load balancer IP (EgressService) to outside the cluster using OVN Logical Router Policies and Network Address Translations.</li> <li>Provides the ability to restrict egress traffic from cluster workloads (Egress Firewall) using OVN Access Control Lists.</li> </ul> </li> <li>Advanced Networking Features<ul> <li>Implements Hybrid Networking to provide support for mixed Windows/Linux clusters using VXLAN tunnels.</li> <li>Provides IP Multicast using OVN IGMP snooping and relays.</li> <li>Provides ability to offload networking tasks from CPU to NIC using OVS Hardware Offload thus providing increased data-plane performance.</li> <li>Adds support for creating secondary and local networks in addition to the default primary networks</li> </ul> </li> </ul>"},{"location":"#why-choose-ovn-kubernetes-in-kubernetes-ecosystem","title":"Why choose OVN Kubernetes in Kubernetes ecosystem?","text":"<p>Networking is the backbone for any Kubernetes cluster. Kubernetes at its core provides an extensive set of Networking APIs and features that need to be implemented in a Kubernetes conformant manner for networking to work properly in a Kubernetes cluster. The aim of OVN Kubernetes project is to be able to provide a pluggable and seamless networking platform for Kubernetes end users. The project focuses strictly on enhancing networking for the Kubernetes platform and includes a wide variety of features that are critical to enterprise and telco users. OVN-Kubernetes community members are active in upstream Kubernetes (particularly in SIG Network) to create new features and then realize them in OVN Kubernetes. In addition to a feature rich platform, the project also aims to be a highly scalable and performant Networking Platform.</p> <p>For more details, please see the following:</p> <ul> <li>OVN Kubernetes Overview for an overview of ovn-kubernetes.</li> <li>Deploying OVN Kubernetes cluster using KIND</li> <li>Deploying OVN Kubernetes CNI using Helm</li> <li>Setup and Building OVN Kubernetes for instructions   on setting up your development environment and building ovn kubernetes.</li> <li>Contributing to OVN Kubernetes for how to get involved   in our project</li> <li>Meet the Community for details on community   meeting details.</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"ci/ci/","title":"CI Tests","text":"<p>For CI, OVN-Kubernetes runs the Kubernetes E2E tests and some locally defined tests.  GitHub Actions are used to run a subset of the Kubernetes E2E tests on each pull request. The local workflow that controls the test run is located in ovn-kubernetes/.github/workflows/test.yml.</p> <p>The following tasks are performed: - Build OVN-Kubernetes - Check out the Kubernetes source tree and compiles some dependencies - Install KIND - Run a matrix of End-To-End Tests using KIND</p> <p>The following sections should help you understand (and if needed modify) the set of tests that run and how to run these tests locally.</p>"},{"location":"ci/ci/#understanding-the-ci-test-suite","title":"Understanding the CI Test Suite","text":"<p>The tests are broken into 2 categories, <code>shard</code> tests which execute tests from the Kubernetes E2E test suite and the <code>control-plane</code> tests which run locally defined tests.</p>"},{"location":"ci/ci/#shard-tests","title":"Shard tests","text":"<p>The shard tests are broken into a set of shards, which is just a grouping of tests, and each shard is run in a separate job in parallel. Shards execute the <code>shard-%</code> target in  ovn-kubernetes/test/Makefile. The set of shards may change in the future. Below is an example of the shards at time of this writing: - shard-network   - All E2E tests that match <code>[sig-network]</code> - shard-conformance   - All E2E tests that match <code>[Conformance]|[sig-network]</code> - shard-test   - Single E2E test that matches the name of the test specified with a regex.    - When selecting the <code>shard-test</code> target, you focus on a specific test by appending <code>WHAT=&lt;test name&gt;</code> to the make command.   - See bottom of this document for an example.</p> <p>Shards use the E2E framework. By selecting a specific shard, you modify ginkgo's <code>--focus</code> parameter.</p> <p>The regex expression for determining which E2E test is run in which shard, as well as the list of skipped tests is defined in ovn-kubernetes/test/scripts/e2e-kind.sh.</p>"},{"location":"ci/ci/#control-plane-tests","title":"Control-plane tests","text":"<p>In addition to the <code>shard-%</code> tests, there is also a <code>control-plane</code> target in  ovn-kubernetes/test/Makefile. Below is a description of this target: - control-plane   - All locally defined tests by default.   - You can focus on a specific test by appending <code>WHAT=&lt;test name&gt;</code> to the make command.   - See bottom of this document for an example.</p> <p>All local tests are run by <code>make control-plane</code>. The local tests are controlled in ovn-kubernetes/test/scripts/e2e-cp.sh and the actual tests are defined in the directory ovn-kubernetes/test/e2e/.</p>"},{"location":"ci/ci/#node-ip-migration-tests","title":"Node IP migration tests","text":"<p>The node IP migration tests are part of the control-plane tests but due to their impact they cannot be run concurrently with other tests and they are disabled when running <code>make control-plane</code>. Instead, they must explicitly be requested with <code>make -C test control-plane WHAT=\"Node IP address migration\"</code>.</p>"},{"location":"ci/ci/#github-ci-integration-through-github-actions-matrix","title":"Github CI integration through Github Actions Matrix","text":"<p>Each of these shards and control-plane tests can then be run in a Github Actions matrix of: * HA setup (3 masters and 0 workers) and a non-HA setup (1 master and 2 workers) * Local Gateway Mode and Shared Gateway Mode. See: Enable Node-Local Services Access in Shared Gateway Mode * IPv4 Only, IPv6 Only and Dualstack * Disabled SNAT Multiple Gateways or Enabled SNAT Gateways * Single bridge or two bridges</p> <p>To reduce the explosion of tests being run in CI, the test cases run are limited using an <code>exclude:</code> statement in  ovn-kubernetes/.github/workflows/test.yml.</p>"},{"location":"ci/ci/#running-ci-locally","title":"Running CI Locally","text":"<p>This section describes how to run CI tests on a local deployment. This may be useful for expanding the CI test coverage or testing a private fix before creating a pull request.</p>"},{"location":"ci/ci/#download-and-build-kubernetes-components","title":"Download and Build Kubernetes Components","text":""},{"location":"ci/ci/#go-version","title":"Go Version","text":"<p>Older versions of Kubernetes do not build with newer versions of Go, specifically Kubernetes v1.16.4 doesn't build with Go version 1.13.x. If this is a version of Kubernetes that needs to be tested with, as a workaround, Go version 1.12.1 can be downloaded to a local directory and the $PATH variable updated only where kubernetes is being built. </p> <pre><code>$ go version\ngo version go1.13.8 linux/amd64\n\n$ mkdir -p /home/$USER/src/golang/go1-12-1/; cd /home/$USER/src/golang/go1-12-1/\n$ wget https://dl.google.com/go/go1.12.1.linux-amd64.tar.gz\n$ tar -xzf go1.12.1.linux-amd64.tar.gz\n$ PATH=/home/$USER/src/golang/go1-12-1/go/bin:$GOPATH/src/k8s.io/kubernetes/_output/local/bin/linux/amd64:$PATH\n</code></pre>"},{"location":"ci/ci/#download-and-build-kubernetes-components-e2e-tests-ginkgo-kubectl","title":"Download and Build Kubernetes Components (E2E Tests, ginkgo, kubectl):","text":"<p>Determine which version of Kubernetes is currently used in CI (See ovn-kubernetes/.github/workflows/test.yml) and set the environmental variable <code>K8S_VERSION</code> to the same value. Also make sure to export a GOPATH which points to your go directory with <code>export GOPATH=(...)</code>.</p> <pre><code>K8S_VERSION=v1.28.0\ngit clone --single-branch --branch $K8S_VERSION https://github.com/kubernetes/kubernetes.git $GOPATH/src/k8s.io/kubernetes/\npushd $GOPATH/src/k8s.io/kubernetes/\nmake WHAT=\"test/e2e/e2e.test vendor/github.com/onsi/ginkgo/ginkgo cmd/kubectl\"\nrm -rf .git\n\nsudo cp _output/local/go/bin/e2e.test /usr/local/bin/.\nsudo cp _output/local/go/bin/kubectl /usr/local/bin/kubectl-$K8S_VERSION\nsudo ln -s /usr/local/bin/kubectl-$K8S_VERSION /usr/local/bin/kubectl\ncp _output/local/go/bin/ginkgo /usr/local/bin/.\npopd\n</code></pre> <p>If you have any failures during the build, verify $PATH has been updated to point to correct GO version. Also may need to change settings on some of the generated binaries. For example:</p> <pre><code>chmod +x $GOPATH/src/k8s.io/kubernetes/_output/bin/deepcopy-gen\n</code></pre>"},{"location":"ci/ci/#export-environment-variables","title":"Export environment variables","text":"<p>Before setting up KIND and before running the actual tests, export essential environment variables.</p> <p>The environment variables and their values depend on the actual test scenario that you want to run.</p> <p>Look at the <code>e2e</code> action (search for <code>name: e2e</code>) in ovn-kubernetes/.github/workflows/test.yml. Prior to installing kind, set the following environment variables according to your needs: <pre><code>export KIND_CLUSTER_NAME=ovn\nexport KIND_INSTALL_INGRESS=[true|false]\nexport KIND_ALLOW_SYSTEM_WRITES=[true|false]\nexport PARALLEL=[true|false]\nexport JOB_NAME=(... job name ...)\nexport OVN_HYBRID_OVERLAY_ENABLE=[true|false]\nexport OVN_MULTICAST_ENABLE=[true|false]\nexport OVN_EMPTY_LB_EVENTS=[true|false]\nexport OVN_HA=[true|false]\nexport OVN_DISABLE_SNAT_MULTIPLE_GWS=[true|false]\nexport OVN_GATEWAY_MODE=[\"local\"|\"shared\"]\nexport KIND_IPV4_SUPPORT=[true|false]\nexport KIND_IPV6_SUPPORT=[true|false]\n# not required for the OVN Kind installation script, but export this already for later\nOVN_SECOND_BRIDGE=[true|false]\n</code></pre></p> <p>You can refer to a recent CI run from any pull request in https://github.com/ovn-org/ovn-kubernetes/actions to get a valid set of settings.</p> <p>As an example for the <code>control-plane-noHA-local-ipv4-snatGW-1br</code> job, the settings are at time of this writing: <pre><code>export KIND_CLUSTER_NAME=ovn\nexport KIND_INSTALL_INGRESS=true\nexport KIND_ALLOW_SYSTEM_WRITES=true\nexport PARALLEL=true\nexport JOB_NAME=control-plane-noHA-local-ipv4-snatGW-1br\nexport OVN_HYBRID_OVERLAY_ENABLE=true\nexport OVN_MULTICAST_ENABLE=true\nexport OVN_EMPTY_LB_EVENTS=true\nexport OVN_HA=false\nexport OVN_DISABLE_SNAT_MULTIPLE_GWS=false\nexport OVN_GATEWAY_MODE=\"local\"\nexport KIND_IPV4_SUPPORT=true\nexport KIND_IPV6_SUPPORT=false\n# not required for the OVN Kind installation script, but export this already for later\nexport OVN_SECOND_BRIDGE=false\n</code></pre></p>"},{"location":"ci/ci/#kind","title":"KIND","text":"<p>Kubernetes in Docker (KIND) is used to deploy Kubernetes locally where a docker container is created per Kubernetes node. The CI tests run on this Kubernetes deployment. Therefore, KIND will need to be installed locally.</p> <p>Generic instructions for installing and running OVNKubernetes with KIND can be found at:  https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md</p> <p>Make sure to set the required environment variables first (see section above). Then, deploy kind: <pre><code>$ pushd\n$ ./kind.sh\n$ popd\n</code></pre></p>"},{"location":"ci/ci/#run-tests","title":"Run Tests","text":"<p>To run the tests locally, run a KIND deployment as described above. The E2E tests look for the kube config file in a special location, so make a copy:</p> <pre><code>cp ~/ovn.conf ~/.kube/kind-config-kind\n</code></pre> <p>To run the desired shard, first make sure that the necessary environment variables are exported (see section above). Then, go to the location of your local copy of the <code>ovn-kubernetes</code> repository: <pre><code>$ REPO=$GOPATH/src/github.com/ovn-org/ovn-kubernetes\n$ cd $REPO\n</code></pre></p>"},{"location":"ci/ci/#running-a-suite-of-shards-or-control-plane-tests","title":"Running a suite of shards or control-plane tests","text":"<p>Finally, run the the shard that you want to test against (each shard can take 30+ minutes to complete) <pre><code>$ pushd test\n# run either\n$ make shard-network\n# or\n$ make shard-conformance\n# or\n$ GITHUB_WORKSPACE=\"$REPO\" make control-plane\n# or\n$ make conformance\n$ popd\n</code></pre></p>"},{"location":"ci/ci/#running-a-single-e2e-test","title":"Running a single E2E test","text":"<p>To run a single E2E test instead, target the shard-test action, as follows:</p> <pre><code>$ cd $REPO\n$ pushd test\n$ make shard-test WHAT=\"should enforce egress policy allowing traffic to a server in a different namespace based on PodSelector and NamespaceSelector\"\n$ popd\n</code></pre> <p>As a reminder, shards use the E2E framework. The value of <code>WHAT=</code> will be used to modify the <code>--focus</code> parameter. Individual tests can be retrieved from https://github.com/kubernetes/kubernetes/tree/master/test/e2e. For network tests, one could run: <pre><code>grep ginkgo.It $GOPATH/src/k8s.io/kubernetes/test/e2e/network/ -Ri\n</code></pre></p> <p>For example: <pre><code># grep ginkgo.It $GOPATH/src/k8s.io/kubernetes/test/e2e/network/ -Ri | head -1\n/root/go/src/k8s.io/kubernetes/test/e2e/network/conntrack.go:   ginkgo.It(\"should be able to preserve UDP traffic when server pod cycles for a NodePort service\", func() {\n# make shard-test WHAT=\"should enforce policy to allow traffic from pods within server namespace based on PodSelector\"\n(...)\n+ case \"$SHARD\" in\n++ echo should be able to preserve UDP traffic when server pod cycles for a NodePort service\n++ sed 's/ /\\\\s/g'\n+ FOCUS='should\\sbe\\sable\\sto\\spreserve\\sUDP\\straffic\\swhen\\sserver\\spod\\scycles\\sfor\\sa\\sNodePort\\sservice'\n+ export KUBERNETES_CONFORMANCE_TEST=y\n+ KUBERNETES_CONFORMANCE_TEST=y\n+ export KUBE_CONTAINER_RUNTIME=remote\n+ KUBE_CONTAINER_RUNTIME=remote\n+ export KUBE_CONTAINER_RUNTIME_ENDPOINT=unix:///run/containerd/containerd.sock\n+ KUBE_CONTAINER_RUNTIME_ENDPOINT=unix:///run/containerd/containerd.sock\n+ export KUBE_CONTAINER_RUNTIME_NAME=containerd\n+ KUBE_CONTAINER_RUNTIME_NAME=containerd\n+ export FLAKE_ATTEMPTS=5\n+ FLAKE_ATTEMPTS=5\n+ export NUM_NODES=20\n+ NUM_NODES=20\n+ export NUM_WORKER_NODES=3\n+ NUM_WORKER_NODES=3\n+ ginkgo --nodes=20 '--focus=should\\sbe\\sable\\sto\\spreserve\\sUDP\\straffic\\swhen\\sserver\\spod\\scycles\\sfor\\sa\\sNodePort\\sservice' '--skip=Networking\\sIPerf\\sIPv[46]|\\[Feature:PerformanceDNS\\]|Disruptive|DisruptionController|\\[sig-apps\\]\\sCronJob|\\[sig-storage\\]|\\[Feature:Federation\\]|should\\shave\\sipv4\\sand\\sipv6\\sinternal\\snode\\sip|should\\shave\\sipv4\\sand\\sipv6\\snode\\spodCIDRs|kube-proxy|should\\sset\\sTCP\\sCLOSE_WAIT\\stimeout|should\\shave\\ssession\\saffinity\\stimeout\\swork|named\\sport.+\\[Feature:NetworkPolicy\\]|\\[Feature:SCTP\\]|service.kubernetes.io/headless|should\\sresolve\\sconnection\\sreset\\sissue\\s#74839|sig-api-machinery|\\[Feature:NoSNAT\\]|Services.+(ESIPP|cleanup\\sfinalizer)|configMap\\snameserver|ClusterDns\\s\\[Feature:Example\\]|should\\sset\\sdefault\\svalue\\son\\snew\\sIngressClass|should\\sprevent\\sIngress\\screation\\sif\\smore\\sthan\\s1\\sIngressClass\\smarked\\sas\\sdefault|\\[Feature:Networking-IPv6\\]|\\[Feature:.*DualStack.*\\]' --flake-attempts=5 /usr/local/bin/e2e.test -- --kubeconfig=/root/ovn.conf --provider=local --dump-logs-on-failure=false --report-dir=/root/ovn-kubernetes/test/_artifacts --disable-log-dump=true --num-nodes=3\nRunning Suite: Kubernetes e2e suite\n(...)\n\u2022 [SLOW TEST:28.091 seconds]\n[sig-network] Conntrack\n/root/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23\n  should be able to preserve UDP traffic when server pod cycles for a NodePort service\n  /root/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/conntrack.go:128\n------------------------------\n{\"msg\":\"PASSED [sig-network] Conntrack should be able to preserve UDP traffic when server pod cycles for a NodePort service\",\"total\":-1,\"completed\":1,\"skipped\":229,\"failed\":0}\nAug 17 14:46:42.842: INFO: Running AfterSuite actions on all nodes\n\n\nAug 17 14:46:15.264: INFO: Running AfterSuite actions on all nodes\nAug 17 14:46:42.885: INFO: Running AfterSuite actions on node 1\nAug 17 14:46:42.885: INFO: Skipping dumping logs from cluster\n\n\nRan 1 of 5667 Specs in 30.921 seconds\nSUCCESS! -- 1 Passed | 0 Failed | 0 Flaked | 0 Pending | 5666 Skipped\n\n\nGinkgo ran 1 suite in 38.489055861s\nTest Suite Passed\n</code></pre></p>"},{"location":"ci/ci/#running-a-control-plane-test","title":"Running a control-plane test","text":"<p>All local tests are defined as <code>control-plane</code> tests. To run a single <code>control-plane</code> test, target the <code>control-plane</code> action and append the <code>WHAT=&lt;test name&gt;</code> parameter, as follows:</p> <pre><code>$ cd $REPO\n$ pushd test\n$ make control-plane WHAT=\"should be able to send multicast UDP traffic between nodes\"\n$ popd\n</code></pre> <p>The value of <code>WHAT=</code> will be used to modify the <code>-ginkgo.focus</code> parameter. Individual tests can be retrieved from this repository under test/e2e. To see a list of individual tests, one could run: <pre><code>grep -R ginkgo.It test/\n</code></pre></p> <p>For example: <pre><code># grep -R ginkgo.It . | head -1\n./e2e/multicast.go: ginkgo.It(\"should be able to send multicast UDP traffic between nodes\", func() {\n# make control-plane WHAT=\"should be able to send multicast UDP traffic between nodes\"\n(...)\n+ go test -timeout=0 -v . -ginkgo.v -ginkgo.focus 'should\\sbe\\sable\\sto\\ssend\\smulticast\\sUDP\\straffic\\sbetween\\snodes' -ginkgo.flakeAttempts 2 '-ginkgo.skip=recovering from deleting db files while maintain connectivity|Should validate connectivity before and after deleting all the db-pods at once in HA mode|Should be allowed to node local cluster-networked endpoints by nodeport services with externalTrafficPolicy=local|e2e ingress to host-networked pods traffic validation|host to host-networked pods traffic validation' -provider skeleton -kubeconfig /root/ovn.conf --num-nodes=2 --report-dir=/root/ovn-kubernetes/test/_artifacts --report-prefix=control-plane_\nI0817 15:26:21.762483 1197731 test_context.go:457] Tolerating taints \"node-role.kubernetes.io/control-plane\" when considering if nodes are ready\n=== RUN   TestE2e\nI0817 15:26:21.762635 1197731 e2e_suite_test.go:67] Saving reports to /root/ovn-kubernetes/test/_artifacts\nRunning Suite: E2e Suite\n(...)\n\u2022 [SLOW TEST:12.332 seconds]\nMulticast\n/root/ovn-kubernetes/test/e2e/multicast.go:25\n  should be able to send multicast UDP traffic between nodes\n  /root/ovn-kubernetes/test/e2e/multicast.go:75\n------------------------------\nSSSSSSSSSSSS\nJUnit report was created: /root/ovn-kubernetes/test/_artifacts/junit_control-plane_01.xml\n\nRan 1 of 60 Specs in 12.333 seconds\nSUCCESS! -- 1 Passed | 0 Failed | 0 Flaked | 0 Pending | 59 Skipped\n--- PASS: TestE2e (12.34s)\nPASS\nok      github.com/ovn-org/ovn-kubernetes/test/e2e  12.371s\n+ popd\n~/ovn-kubernetes/test\n</code></pre></p>"},{"location":"ci/ci/#ipv6-tests","title":"IPv6 tests","text":"<p>To skip the IPv4 only tests (in a IPv6 only deployment), pass the <code>KIND_IPV6_SUPPORT=true</code> environmental variable to <code>make</code>:</p> <pre><code>$ cd $GOPATH/src/github.com/ovn-org/ovn-kubernetes\n\n$ pushd test\n$ KIND_IPV6_SUPPORT=true make shard-conformance\n$ popd\n</code></pre> <p>Github CI doesn\u00b4t offer IPv6 connectivity, so IPv6 only tests are always skipped. To run those tests locally, comment out the following line from ovn-kubernetes/test/scripts/e2e-kind.sh</p> <pre><code># Github CI doesn\u00b4t offer IPv6 connectivity, so always skip IPv6 only tests.\nSKIPPED_TESTS=$SKIPPED_TESTS$IPV6_ONLY_TESTS\n</code></pre>"},{"location":"ci/ci/#conformance-tests","title":"Conformance Tests","text":"<p>We have a conformance test suit that can be invoked using the <code>make conformance</code> command. Currently we run the <code>TestNetworkPolicyV2Conformance</code> tests there. The actual tests are defined in https://github.com/kubernetes-sigs/network-policy-api/tree/master/conformance and then invoked from this repo. Any changes to the tests first have to be submitted upstream to <code>network-policy-api</code> repo and then brought downstream into the ovn-kubernetes repo through version bump.</p>"},{"location":"design/acls/","title":"ACLs","text":""},{"location":"design/acls/#introduction","title":"Introduction","text":"<p>ovn-k uses ACLs to implement multiple features, this doc is intended to list all of them, explaining their IDs,  priorities, and dependencies.</p> <p>OVN has 3 independent sets of ACLs, based on direction and pipeline stage, which are applied in the following order:  1. <code>direction=\"from-lport\"</code>  2. <code>direction=\"from-lport\", options={\"apply-after-lb\": \"true\"}</code> 3. <code>direction=\"to-lport\"</code></p> <p>The priorities between these stages are independent!</p> <p>OVN will apply the <code>from-lport</code> ACLs in two stages. ACLs without <code>apply-after-lb</code> set, will be applied before the  load balancer stage, and ACLs with this option set will be applied after the load balancer stage. <code>to-lport</code> are always  applied after the load balancer stage.</p> <p>For now, ovn-k doesn't use <code>direction=\"from-lport\"</code> ACLs, since most of the time we need to apply ACLs after loadbalancing. Here is the current order in which ACLs for different objects are applied (check the rest of the doc for details)</p>"},{"location":"design/acls/#directionfrom-lport-optionsapply-after-lb-true","title":"<code>direction=\"from-lport\", options={\"apply-after-lb\": \"true\"}</code>","text":"<ol> <li>egress multicast, allow priority = <code>1012</code>,  deny priority = <code>1011</code></li> <li>egress network policy, default deny priority = <code>1000</code>, allow priority = <code>1001</code></li> </ol>"},{"location":"design/acls/#directionto-lport","title":"<code>direction=\"to-lport\"</code>","text":"<ol> <li>egress firewall, priorities = <code>2000</code>-<code>10000</code> (egress firewall is applied on this stage to be independently applied after egress network policy)</li> <li>ingress multicast, allow priority = <code>1012</code>,  deny priority = <code>1011</code></li> <li>ingress network policy, default deny priority = <code>1000</code>, allow priority = <code>1001</code></li> </ol>"},{"location":"design/acls/#egress-firewall","title":"Egress Firewall","text":"<p>Egress Firewall creates 1 ACL for every specified rule, with <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=EgressFirewall</code> e.g. given object:</p> <pre><code>kind: EgressFirewall\napiVersion: k8s.ovn.org/v1\nmetadata:\n  name: default\n  namespace: default\nspec:\n  egress:\n    - type: Allow\n      to:\n        dnsName: www.openvswitch.org\n    - type: Allow\n      to:\n        cidrSelector: 1.2.3.0/24\n      ports:\n        - protocol: UDP\n          port: 55\n    - type: Deny\n      to:\n        cidrSelector: 0.0.0.0/0\n</code></pre> <p>will create 3 ACLs:</p> <pre><code>action              : allow\ndirection           : to-lport\nexternal_ids        : {\n    \"k8s.ovn.org/id\"=\"default-network-controller:EgressFirewall:default:10000\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=EgressFirewall, \n    rule-idx=\"0\"\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.dst == $a5272457940446250407) &amp;&amp; ip4.src == $a4322231855293774466\"\nmeter               : acl-logging\nname                : \"EF:default:10000\"\noptions             : {}\npriority            : 10000\nseverity            : []\n\naction              : allow\ndirection           : to-lport\nexternal_ids        : {\n    \"k8s.ovn.org/id\"=\"default-network-controller:EgressFirewall:default:9999\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=EgressFirewall, \n    rule-idx=\"1\"\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.dst == 1.2.3.0/24) &amp;&amp; ip4.src == $a4322231855293774466 &amp;&amp; ((udp &amp;&amp; ( udp.dst == 55 )))\"\nmeter               : acl-logging\nname                : \"EF:default:9999\"\noptions             : {}\npriority            : 9999\nseverity            : []\n\naction              : drop\ndirection           : to-lport\nexternal_ids        : {\n    \"k8s.ovn.org/id\"=\"default-network-controller:EgressFirewall:default:9998\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=EgressFirewall, \n    rule-idx=\"2\"\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.dst == 0.0.0.0/0 &amp;&amp; ip4.dst != 10.244.0.0/16) &amp;&amp; ip4.src == $a4322231855293774466\"\nmeter               : acl-logging\nname                : \"EF:default:9998\"\noptions             : {}\npriority            : 9998\nseverity            : []\n</code></pre> <p>Egress firewall should be applied after egress network policy independently, to make sure that connection that are allowed by network policy, but denied by egress firewall will be dropped.</p>"},{"location":"design/acls/#multicast","title":"Multicast","text":"<p>For more details about Multicast in ovn see Multicast docs Multicast creates 2 types of ACLs: global default ACLs with <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=MulticastCluster</code> and per-namespace ACLs with <code>ExternalIDs[\"owner_object_type\"]=\"MulticastNS\"</code>. </p> <p>When multicast is enabled for ovn-k, you will find 4 default global ACLs: - 2 ACLs (ingress/egress) dropping all multicast traffic - on all switches (via clusterPortGroup) - 2 ACLs (ingress/egress) allowing all multicast traffic - on clusterRouterPortGroup   (that allows multicast between pods that reside on different nodes, see   https://github.com/ovn-org/ovn-kubernetes/commit/3864f2b6463392ae2d80c18d06bd46ec44e639f9 for more details)</p> <pre><code>action              : allow\ndirection           : from-lport\nexternal_ids        : {\n  direction=Egress, \n  \"k8s.ovn.org/id\"=\"default-network-controller:MulticastCluster:AllowInterNode:Ingress\", \n  \"k8s.ovn.org/owner-controller\"=default-network-controller, \n  \"k8s.ovn.org/owner-type\"=MulticastCluster, \n  type=AllowInterNode\n}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @clusterRtrPortGroup &amp;&amp; (ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\nmeter               : acl-logging\nname                : []\noptions             : {apply-after-lb=\"true\"}\npriority            : 1012\nseverity            : []\n\naction              : allow\ndirection           : to-lport\nexternal_ids        : {\n  direction=Ingress, \n  \"k8s.ovn.org/id\"=\"default-network-controller:MulticastCluster:AllowInterNode:Ingress\", \n  \"k8s.ovn.org/owner-controller\"=default-network-controller, \n  \"k8s.ovn.org/owner-type\"=MulticastCluster, \n  type=AllowInterNode\n}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @clusterRtrPortGroup &amp;&amp; (ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1012\nseverity            : []\n\naction              : drop\ndirection           : from-lport\nexternal_ids        : {\n  direction=Egress, \n  \"k8s.ovn.org/id\"=\"default-network-controller:MulticastCluster:DefaultDeny:Egress\", \n  \"k8s.ovn.org/owner-controller\"=default-network-controller, \n  \"k8s.ovn.org/owner-type\"=MulticastCluster, \n  type=DefaultDeny\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\nmeter               : acl-logging\nname                : []\noptions             : {apply-after-lb=\"true\"}\npriority            : 1011\nseverity            : []\n\naction              : drop\ndirection           : to-lport\nexternal_ids        : {\n  direction=Ingress, \n  \"k8s.ovn.org/id\"=\"default-network-controller:MulticastCluster:DefaultDeny:Egress\", \n  \"k8s.ovn.org/owner-controller\"=default-network-controller, \n  \"k8s.ovn.org/owner-type\"=MulticastCluster, \n  type=DefaultDeny\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1011\nseverity            : []\n</code></pre> <p>For every namespace with enabled multicast, there are 2 more ACLs, e.g. for default namespace: <pre><code>action              : allow\ndirection           : to-lport\nexternal_ids        : {\n    \"k8s.ovn.org/id\"=\"default-network-controller:MulticastNS:default:Allow_Ingress\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=MulticastNS, \n    direction=Ingress\n}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a16982411286042166782 &amp;&amp; (igmp || (ip4.src == $a4322231855293774466 &amp;&amp; ip4.mcast))\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1012\nseverity            : []\n\naction              : allow\ndirection           : from-lport\nexternal_ids        : {\n    \"k8s.ovn.org/id\"=\"default-network-controller:MulticastNS:default:Allow_Egress\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=MulticastNS, \n    tdirection=Egress\n}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a16982411286042166782 &amp;&amp; ip4.mcast\"\nmeter               : acl-logging\nname                : []\noptions             : {apply-after-lb=\"true\"}\npriority            : 1012\nseverity            : []\n</code></pre></p>"},{"location":"design/acls/#network-policy","title":"Network Policy","text":"<p>Every node has 1 ACL to allow traffic from that node's management port IP with <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=NetpolNode</code>, like</p> <pre><code>action              : allow-related\ndirection           : to-lport\nexternal_ids        : {\n    ip=\"10.244.2.2\", \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolNode:ovn-worker:10.244.2.2\", \n    \"k8s.ovn.org/name\"=ovn-worker, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolNode\n}\nlabel               : 0\nlog                 : false\nmatch               : \"ip4.src==10.244.2.2\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1001\nseverity            : []\n</code></pre> <p>There are also 2 Default Allow ACLs for the hairpinned traffic (pod-&gt;svc-&gt;same pod) with <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=NetpolDefault</code>, which match on the special V4OVNServiceHairpinMasqueradeIP and V6OVNServiceHairpinMasqueradeIP addresses. These IPs are assigned as src IP to the hairpinned packets.</p> <pre><code>action              : allow-related\ndirection           : to-lport\nexternal_ids        : {\n    direction=Ingress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolDefault:allow-hairpinning:Ingress\", \n    \"k8s.ovn.org/name\"=allow-hairpinning, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolDefault\n}\nlabel               : 0\nlog                 : false\nmatch               : \"ip4.src == 169.254.169.5\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1001\nseverity            : []\n\naction              : allow-related\ndirection           : from-lport\nexternal_ids        : {\n    direction=Egress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolDefault:allow-hairpinning:Egress\", \n    \"k8s.ovn.org/name\"=allow-hairpinning, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolDefault\n}\nlabel               : 0\nlog                 : false\nmatch               : \"ip4.src == 169.254.169.5\"\nmeter               : acl-logging\nname                : []\noptions             : {apply-after-lb=\"true\"}\npriority            : 1001\nseverity            : []\n</code></pre> <p>Network Policy creates default deny ACLs for every namespace that has at least 1 network policy with  <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=NetpolNamespace</code>. There are 2 ACL types (<code>defaultDeny</code> and <code>arpAllow</code>) for every policy direction (Ingress/Egress) e.g. for <code>default</code> namespace,</p> <p>Egress: <pre><code>action              : allow\ndirection           : from-lport\nexternal_ids        : {\n    direction=Egress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolNamespace:default:Egress:arpAllow\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolNamespace, \n    type=arpAllow\n}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a16982411286042166782_egressDefaultDeny &amp;&amp; (arp || nd)\"\nmeter               : acl-logging\nname                : \"NP:default:Egress\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 1001\nseverity            : []\n\naction              : drop\ndirection           : from-lport\nexternal_ids        : {\n    direction=Egress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolNamespace:default:Egress:defaultDeny\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolNamespace, \n    type=defaultDeny\n}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a16982411286042166782_egressDefaultDeny\"\nmeter               : acl-logging\nname                : \"NP:default:Egress\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 1000\nseverity            : []\n</code></pre></p> <p>Ingress: <pre><code>_uuid               : a6ffa9d4-e811-4aaf-9505-87cc0b2f442a\naction              : allow\ndirection           : to-lport\nexternal_ids        : {\n    direction=Ingress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolNamespace:default:Ingress:arpAllow\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolNamespace, \n    type=arpAllow\n}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a16982411286042166782_ingressDefaultDeny &amp;&amp; (arp || nd)\"\nmeter               : acl-logging\nname                : \"NP:default:Ingress\"\noptions             : {}\npriority            : 1001\nseverity            : []\n\naction              : drop\ndirection           : to-lport\nexternal_ids        : {\n    direction=Ingress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolNamespace:default:Ingress:defaultDeny\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolNamespace, \n    type=defaultDeny\n}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a16982411286042166782_ingressDefaultDeny\"\nmeter               : acl-logging\nname                : \"NP:default:Ingress\"\noptions             : {}\npriority            : 1000\nseverity            : []\n</code></pre></p> <p>There are also ACLs owned by every network policy object with <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=NetworkPolicy</code>, e.g. for the following object</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n name: test-policy\n namespace: default\nspec:\n podSelector: {}\n policyTypes:\n - Ingress\n - Egress\n ingress:\n - from:\n   - namespaceSelector:\n       matchLabels:\n         kubernetes.io/metadata.name: default\n     podSelector:\n       matchLabels:\n         app: demo\n egress:\n  - to:\n    - ipBlock:\n        cidr: 10.244.1.5/32\n</code></pre> <p>2 ACLs will be created:</p> <pre><code>action              : allow-related\ndirection           : from-lport\nexternal_ids        : {\n    direction=egress, \n    gress-index=\"0\", \n    ip-block-index=\"0\", \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetworkPolicy:default:test-policy:egress:0:-1:0\", \n    \"k8s.ovn.org/name\"=\"default:test-policy\", \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetworkPolicy, \n    port-policy-index=\"-1\"\n}\nlabel               : 0\nlog                 : false\nmatch               : \"ip4.dst == 10.244.1.5/32 &amp;&amp; inport == @a2653181086423119552\"\nmeter               : acl-logging\nname                : \"NP:default:test-policy:egress:0:-1:0\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 1001\nseverity            : []\n\naction              : allow-related\ndirection           : to-lport\nexternal_ids        : {\n    direction=ingress, \n    gress-index=\"0\", \n    ip-block-index=\"-1\", \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetworkPolicy:default:test-policy:ingress:0:-1:-1\", \n    \"k8s.ovn.org/name\"=\"default:test-policy\", \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetworkPolicy, \n    port-policy-index=\"-1\"\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.src == {$a3733136965153973077} || (ip4.src == 169.254.169.5 &amp;&amp; ip4.dst == {$a3733136965153973077})) &amp;&amp; outport == @a2653181086423119552\"\nmeter               : acl-logging\nname                : \"NP:default:test-policy:ingress:0:-1:-1\"\noptions             : {}\npriority            : 1001\nseverity            : []\n</code></pre> <p><code>\"k8s.ovn.org/name\"</code> is the <code>&lt;namespace&gt;:&lt;name&gt;</code> of network policy object, <code>gress-index</code> is the index of gress policy in the <code>NetworkPolicy.Spec.[In/E]gress</code>, check <code>gress_policy.go:getNetpolACLDbIDs</code> for more details on the rest of the fields.</p>"},{"location":"design/architecture/","title":"OVN Kubernetes Architecture","text":"<p>There are two deployment modes for ovn kubernetes depending on which the architecture is drastically different:</p> <ul> <li>default mode (centralized control plane architecture)</li> <li>interconnect mode (distributed control plane architecture)</li> </ul> <p>End users can pick either of these modes depending on their use cases and what suits them well. Let's look at both these modes in depth so that you are empowered to make your choice between these two modes of deployment.</p>"},{"location":"design/architecture/#ovn-kubernetes-components-default-mode","title":"OVN Kubernetes Components - Default Mode","text":"<p>The control plane has the <code>ovnkube-master</code> pod in the <code>ovn-kubernetes</code> namespace which are running only on the control plane nodes in your cluster:</p> <ul> <li>ovnkube-master pod<ul> <li>ovnkube-master container:<ul> <li>OVN-Kubernetes component</li> <li>Watches K8s API for objects - namespaces, pods, services, endpoints, network policies, CRs</li> <li>Translates K8s objects into OVN logical entities</li> <li>Stores OVN entities in NorthBound Database (NBDB)</li> <li>Manages pod subnet allocation to nodes (pod IPAM)</li> </ul> </li> <li>nbdb container:<ul> <li>Native OVN component</li> <li>Runs the OVN NBDB database</li> <li>Stores the logical elements created by ovnkube-master</li> <li>3 replicas across control plane nodes running using RAFT leadership algorithm in HA mode</li> </ul> </li> <li>northd container:<ul> <li>Native OVN component</li> <li>Converts the OVN logical elements from NBDB to OVN logical flows in SBDB</li> </ul> </li> <li>sbdb container:<ul> <li>Native OVN component</li> <li>Stores the logical flows created by northd</li> <li>3 replicas across control plane nodes running using RAFT leadership algorithm in HA mode</li> </ul> </li> </ul> </li> </ul> <p>The data plane includes the <code>ovnkube-node</code> and <code>ovs-node</code> pods in the <code>ovn-kubernetes</code> namespace which are running on all your nodes in the cluster.</p> <ul> <li>ovnkube-node pod<ul> <li>ovnkube-node container:<ul> <li>OVN Kubernetes component</li> <li>Runs the CNI executable (CNI ADD/DEL)</li> <li>Digests the IPAM annotation set on pod by ovnkube-master</li> <li>Creates the veth pair for the pod</li> <li>Creates the ovs port on bridge</li> <li>Programs the necessary iptables and gateway service flows on a per node basis.</li> </ul> </li> <li>ovn-controller container:<ul> <li>Native OVN component</li> <li>Connects to SBDB running in control plane using TLS</li> <li>Converts SBDB logical flows into openflows</li> <li>Write them to OVS</li> </ul> </li> </ul> </li> <li>ovs-node pod<ul> <li>ovs-daemons container:<ul> <li>OVS Native component</li> <li>OVS daemon and database running as a container</li> <li>virtual switch that pushes the network plumbing to the edge on the node</li> </ul> </li> </ul> </li> </ul>"},{"location":"design/architecture/#default-mode-architecture","title":"Default Mode Architecture","text":"<p>Now that we know the pods and components running in the default mode, let's tie up loose ends and show how these components run on a standard HA Kubernetes cluster.</p>"},{"location":"design/architecture/#control-plane-nodes","title":"Control Plane Nodes:","text":""},{"location":"design/architecture/#worker-nodes","title":"Worker Nodes:","text":""},{"location":"design/architecture/#ovn-kubernetes-components-interconnect-mode","title":"OVN Kubernetes Components - Interconnect mode","text":"<p>The control plane has the <code>ovnkube-control-plane</code> pod in the <code>ovn-kubernetes</code> namespace which is super light weight and running only on the control plane nodes in your cluster:</p> <ul> <li>ovnkube-control-plane pod<ul> <li>ovnkube-cluster-manager container:<ul> <li>OVN-Kubernetes component</li> <li>Watches K8s API for objects - nodes mainly</li> <li>Allocates pod subnet to each node</li> <li>Allocates join subnet IP to each node</li> <li>Allocates transit subnet IP to each node</li> <li>Consolidates zone statuses across all nodes for features like EgressFirewall and EgressQoS</li> </ul> </li> </ul> </li> </ul> <p>The data plane includes the <code>ovnkube-node</code> and <code>ovs-node</code> pods in the <code>ovn-kubernetes</code> namespace running on all your nodes in the cluster making this architecture localized and more distributed.</p> <ul> <li>ovnkube-node pod<ul> <li>ovnkube-controller container:<ul> <li>OVN-Kubernetes component</li> <li>Allocates podIP from the podSubnet to each pod in its zone (IPAM)</li> <li>Watches K8s API for objects - nodes, namespaces, pods, services, endpoints, network policies, CRs</li> <li>Translates K8s objects into OVN logical entities - stores them in OVN databases</li> <li>Stores OVN entities in NorthBound Database (NBDB)</li> <li>Manages pod subnet allocation to nodes (pod IPAM)</li> <li>Runs the CNI executable (CNI ADD/DEL)</li> <li>Digests the IPAM annotation set on pod</li> <li>Creates the veth pair for the pod</li> <li>Creates the ovs port on bridge</li> <li>Programs the necessary iptables and gateway service flows on a </li> </ul> </li> <li>nbdb container:<ul> <li>Native OVN component</li> <li>Runs the OVN NBDB database</li> <li>Stores the logical elements created by ovnkube-controller</li> <li>runs only 1 replica, contains information local to this node</li> </ul> </li> <li>northd container:<ul> <li>Native OVN component</li> <li>Converts the OVN logical elements from NBDB to OVN logical flows in SBDB</li> </ul> </li> <li>sbdb container:<ul> <li>Native OVN component</li> <li>Stores the logical flows created by northd</li> <li>runs only 1 replica, contains information local to this node</li> </ul> </li> <li>ovn-controller container:<ul> <li>Native OVN component</li> <li>Connects to SBDB running in control plane using TLS</li> <li>Converts SBDB logical flows into openflows</li> <li>Write them to OVS</li> </ul> </li> </ul> </li> <li>ovs-node pod<ul> <li>ovs-daemons container:<ul> <li>OVS Native component</li> <li>OVS daemon and database running as a container</li> <li>virtual switch that pushes the network plumbing to the edge on the node</li> </ul> </li> </ul> </li> </ul> <p>Thus as we can see, the databases, northd and ovn kubernetes controller components now run per zone rather than only on the control-plane.</p>"},{"location":"design/architecture/#interconnect-mode-architecture","title":"Interconnect Mode Architecture","text":""},{"location":"design/architecture/#what-is-interconnect","title":"What is Interconnect?","text":"<p>OVN Interconnection is a feature that allows connecting multiple OVN deployments with OVN managed GENEVE tunnels. Native ovn-ic feature allows for an <code>ovn-ic</code>, OVN interconnection controller, that is a centralized daemon which communicates with global interaction databases (IC_NB/IC_SB) to configure and exchange data with local NB/SB databases for interconnecting with other OVN deployments. See this for more details.</p>"},{"location":"design/architecture/#adopting-ovn-interconnect-into-ovn-kubernetes","title":"Adopting OVN-Interconnect into OVN-Kubernetes","text":"<p>In order to effectively adapt the capabilities of the interconnect feature in the kubernetes world, ovn-kubernetes components will replace <code>ovn-ic</code> daemon. Also note that the term <code>zone</code> which will be used heavily in these docs just refers to a single OVN deployment. Now that we know the pods and components running in the interconnect mode, let's tie up loose ends and show how these components run on a standard HA Kubernetes cluster. By default each node in the cluster is a <code>zone</code>, so each <code>zone</code> contains 1 node. There is no more RAFT since each node has its own database.</p>"},{"location":"design/architecture/#control-plane-nodes_1","title":"Control Plane Nodes:","text":""},{"location":"design/architecture/#worker-nodes_1","title":"Worker Nodes:","text":""},{"location":"design/architecture/#why-do-we-need-interconnect-mode-in-ovn-kubernetes","title":"Why do we need Interconnect mode in OVN-Kubernetes?","text":"<p>This architecture brings about several improvements:</p> <ul> <li>Stability: The OVN Northbound and Southbound databases are local to each node. Since they are running in the standalone mode, that eliminates the need for RAFT, thus avoiding all the \u201csplit-brain\u201d issues. If one of the databases goes down, the impact is now isolated to only that node. This has led to improved stability of the OVN Kubernetes stack and simpler customer escalation resolution.</li> <li>Scale: As seen in the above diagram, the ovn-controller container connects to the local Southbound database for logical flow information. On large clusters with N nodes, this means each Southbound database is handling only one connection from its own local ovn-controller. This has removed the scale bottlenecks that were present in the centralized model helping us to scale horizontally with node count.</li> <li>Performance: The OVN Kubernetes brain is now local to each node in the cluster and it is storing and processing changes to only those Kubernetes pods, services, endpoints objects that are relevant for that node (note: some features like NetworkPolicies need to process pods running on other nodes). This in turn means the OVN stack is also processing less data thus leading to improved operational latency. Another benefit is that the control plane stack is now lighter-weight.</li> <li>Security: Since the infrastructure network traffic between ovn-controller and OVN Southbound database is now contained within each node, overall cross-node and cross-cluster (HostedControlPlane, ManagedSaaS) chatter is decreased and traffic security can be increased.</li> </ul>"},{"location":"design/architecture/#default-mode-versus-interconnect-mode","title":"Default Mode versus Interconnect Mode","text":"<ul> <li>When you want your databases to stay centralized and don't mind much about linear scaling of number of nodes in your cluster, choose the default mode</li> <li>Note that there is no different to OVS between the two deployment modes.</li> <li>FIXME: This section needs to be written well</li> </ul>"},{"location":"design/external-ip-and-loadbalancer-ingress/","title":"External IP and LoadBalancer Ingress","text":"<p>OVN Kubernetes implements both External IPs and LoadBalancer Ingress IPs (<code>service.Status.LoadBalancer.Ingress</code>) in the form of OVN load balancers. These OVN load balancers live on all of the Kubernetes nodes and are thus highly available and ready for load sharing. It is the administrator's responsibility to route traffic to the Kubernetes nodes for both of these VIP types. </p> <p>In an environment where External IPs and LoadBalancer Ingress VIPs happen to be part of the nodes' subnets, administrators might expect that OVN Kubernetes answer to ARP requests to these VIPs. However, this is not the case. The administrator is responsible for routing packets to the Kubernetes nodes and they cannot rely on the network plugin for this to happen.</p>"},{"location":"design/external-ip-and-loadbalancer-ingress/#external-ip","title":"External IP","text":"<p>The Kubernetes documentation states that External IPs are to be handled by the cluster administrator and are not the responsibility of the network plugin. See the following quote from the kubernetes documentation: <pre><code>External IPs\n\nIf there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on those externalIPs. Traffic that ingresses into the cluster with the external IP (as destination IP), on the Service port, will be routed to one of the Service endpoints. externalIPs are not managed by Kubernetes and are the responsibility of the cluster administrator.\n</code></pre></p> <p>Source: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips</p>"},{"location":"design/external-ip-and-loadbalancer-ingress/#loadbalancer-ingress-servicestatusloadbalanceringress","title":"LoadBalancer Ingress (service.Status.LoadBalancer.Ingress)","text":"<p>For a service's Status LoadBalancer Ingress field <code>service.Status.LoadBalancer.Ingress</code>, the aforementioned statement applies in exactly the same manner. Both External IP and <code>service.Status.LoadBalancer.Ingress</code> should behave the same from the network plugin's behavior, and it is the administrator's responsibility to get traffic for the VIPs into the cluster. </p>"},{"location":"design/external-ip-and-loadbalancer-ingress/#implementation-details","title":"Implementation details","text":"<p>OVN Kubernetes exposes External IPs and <code>service.Status.LoadBalancer.Ingress</code> VIPs as OVN load balancers on every node in the cluster. However, OVN Kubernetes will not answer to ARP requests to these VIP types, even if they reside on a node local subnet. This is because otherwise, every node in the cluster would answer with its own ARP reply to the same ARP request, leading to potential issues with stateful network flows that are tracked by conntrack. See the discussion in https://github.com/ovn-org/ovn-kubernetes/issues/2407 for further details.</p> <p>In fact, OVN Kubernetes implements explicit bypass rules for ARP requests to these VIP types on the external bridge (<code>br-ex</code> or <code>breth0</code> in most deployments). Any ARP request to such an IP that comes in from the physical port will bypass the OVN dataplane and it will be sent to host's networking stack on purpose. If an ARP reponse to a VIP is expeced, make sure the VIP is added to the host's networking stack.</p> <p>For implementation details, see:  * https://github.com/ovn-org/ovn-kubernetes/blob/00925a6c64f57f03b2918eb48ff589c3417ddaa9/go-controller/pkg/node/gateway_shared_intf.go#L336 * https://github.com/ovn-org/ovn-kubernetes/blob/00925a6c64f57f03b2918eb48ff589c3417ddaa9/go-controller/pkg/node/gateway_shared_intf.go#L344 * https://github.com/ovn-org/ovn-kubernetes/pull/2540 * https://github.com/ovn-org/ovn-kubernetes/pull/2394</p>"},{"location":"design/external-ip-and-loadbalancer-ingress/#guidance-for-administrators","title":"Guidance for administrators","text":"<p>This absence of ARP replies from OVN Kubernetes means that administrators must take extra actions to make External IPs and LoadBalancer Ingress VIPs work, even when these VIPs reside on one of the node local subnets.</p> <p>For External IPs, administrators can either assign the External IP to one of the nodes' Linux networking stacks if the External IP falls into one of the node's subnets. In this case, ARP requests to the External IP will be answered with ARP replies by the node that was assigned the External IP. For example, an admin could run <code>ip address add &lt;externalIP&gt;/32 dev lo</code> to make this work, assuming that <code>arp_ignore</code> is at its default setting of <code>0</code> and thus the Linux networking stack uses the default weak host model for ARP replies. An alternative could be to point one or multiple static routes for the External IP to one or several of the Kubernetes nodes. </p> <p>For LoadBalancer Ingress VIPs, an administrator will either use a tool such as MetalLB L2 mode. Or, they can configure ECMP load-sharing. ECMP load-sharing can be implemented via static routes which point to all Kubernetes nodes or via BGP route injection (e.g., MetalLB's BGP mode).</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/","title":"Host -&gt; Services using OpenFlow with shared gateway bridge","text":""},{"location":"design/host-to-node-port-hairpin-trafficflow/#background","title":"Background","text":"<p>In order to allow host to Kubernetes service access packets originating from the host must go into OVN in order to hit load balancers and be routed to the appropriate destination. Typically when accessing a services backed by an OVN networked pod, a single interface can be used (ovn-k8s-mp0) in order to get into the local worker switch, hit the load balancer, and reach the pod endpoint. However, when a service is backed by host networked pods, the behavior becomes more complex. For example, if the host access a service, where the backing endpoint is the host itself, then the packet must hairpin back to the host after being load balanced. There are additional complexities to consider, such as when an host network endpoint is using a secondary IP on a NIC. To be able to solve all of the potential use cases for service traffic, OpenFlow is leveraged with OVN-Kubernetse programmed flows in order to steer service traffic.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#introduction","title":"Introduction","text":"<p>OpenFlow on the OVS gateway bridge to handle all host to service traffic and this methodology is used for both gateway modes (local and shared). However, the paths that local and shared take may be slightly different as local gateway mode requires that all service traffic uses host's routing stack as a next hop before leaving the node.</p> <p>Accessing services from the host via the shared gateway bridge means that traffic from the host enters the shared  gateway bridge and is then forwarded into the Gateway Router (GR). The complexity with this solution revolves around  the fact that the host and the GR both share the same node IP address. Due to this fact, the host and OVN can never know the other is using the same IP address and thus requires masquerading done inside of the shared gateway bridge.  Additionally, we want to avoid the shared gateway having to act like a router and managing control plane functions like ARP. This would add a bunch of overhead to managing the shared gateway bridge and greatly increase the cost of this implementation. </p> <p>In order to avoid ARP, both the OVN GR and the host think that the service CIDR is an externally routed network. In other words this both OVN and the host think that to reach the service CIDR they need to route their packet to some next hop external to the node. In the past OVN-Kubernetes has leveraged the default gateway as that next hop, routing all service traffic towards that default gateway. However, this behavior relied on the default gateway existing and being able to ARP for its MAC address. In the current implementation this dependency on the gateway has been removed, and a secondary network is configured on OVN and the host with a fake next hop in that secondary network. The default secondary networks for IPv4 and IPv6 are:  - <code>169.254.169.0/29</code>  - <code>fd69::/125</code></p> <p>OVN is assigned 169.254.169.1 and fd69::1, while the Host uses 169.254.169.2 and fd69::2. The next hop address used is 169.254.169.4 and fd69::4.</p> <p>This subnet is only used for services communication, and service access from the host will be routed via: <pre><code>10.96.0.0/16 via 169.254.169.4 dev breth0 mtu 1400\n</code></pre></p> <p>By doing this the destination MAC address will be the next hop MAC and can be manipulated to act as masquerade MAC to the host.  As the host goes to send traffic to the next hop, OpenFlow rules in br-ex hijack the packet, modify it, and redirect it  to the OVN GR. Similarly the reply packets from OVN GR are modified and masqueraded, before being sent back to the host.</p> <p>Since the next hop is not a real address it cannot perform ARP response, so static ARP entries are added to the host and OVN GR.</p> <p>For Host to pod access (and vice versa) the management port (ovn-k8s-mp0) is still used.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#new-load-balancer-behavior","title":"New Load Balancer Behavior","text":"<p>Load balancers with OVN are placed either on a router or worker switch. Previously, the \"node port\" load balancers (created on a per node basis) were applied to the GR and the worker switch, while singleton cluster wide load balancers for Cluster IP services were applied across all worker switches. With the new implementation, Cluster IP service traffic is destined for GR from the host and therefore the GR requires load balancers that can handle Cluster IP. In addition, the load balancer must not have the node's IP address as and endpoint. This is due to the fact that on a GR the node IP address is used. Therefore if a load balancer on the GR were to DNAT to its own node IP, the packet would be dropped.</p> <p>To solve this problem, an additional load balancer is added with this implementation. The purpose of this new load balancer is to accommodate host endpoints. If endpoints are added for a service that contain a host endpoint, that VIP is moved to the new load balancer. Additionally, if one of those endpoints contain this node's IP address, it is replaced with the host's special masqueraded IP (IPv4: 169.254.169.2, IPv6: fd69::2).</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#use-cases","title":"Use Cases","text":"<p>The following sections go over each potential traffic path originating from Host to Service. Note Host -&gt; node port or external IP services are DNAT'ed in iptables to the cluster IP address before being sent out. Therefore the behavior of any host to service is essentially the same, forwarded towards the Cluster IP via the shared gateway bridge.</p> <p>For all of the following use cases, follow this topology:</p> <pre><code>          host (ovn-worker, 172.18.0.3, 169.254.169.2) \n           |\neth0----|breth0| ------ 172.18.0.3   OVN GR 100.64.0.4 --- join switch --- ovn_cluster_router --- 10.244.1.3 pod\n                        169.254.169.1\n</code></pre> <p>The service used in the following use cases is: <pre><code>[trozet@trozet contrib]$ kubectl get svc web-service\nNAME          TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                       AGE\nweb-service   NodePort   10.96.146.87   &lt;none&gt;        80:30015/TCP,9999:32326/UDP   4m54s\n</code></pre></p> <p>OVN masquerade IPs are 169.254.169.1, and fd69::1</p> <p>Another worker node exists, ovn-worker2 at 172.18.0.4.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#ovn-masquerade-addresses","title":"OVN masquerade addresses","text":"<ul> <li>IPv4</li> <li>169.254.169.1: OVN masquerade address</li> <li>169.254.169.2: host masquerade address</li> <li>169.254.169.3: local ETP masquerade address (not used for shared GW, kept for parity)</li> <li>169.254.169.4: dummy next-hop masquerade address</li> <li>IPv6</li> <li>fd69::1: OVN masquerade address</li> <li>fd69::2: host masquerade address</li> <li>fd69::3: local ETP masquerade address (not used for shared GW, kept for parity)</li> <li>fd69::4: dummy next-hop masquerade address</li> </ul>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#configuration-details","title":"Configuration Details","text":""},{"location":"design/host-to-node-port-hairpin-trafficflow/#host","title":"Host","text":"<p>As previously mentioned the host is configured with the <code>169.254.169.2</code> and <code>fd69::2</code> addresses. These are configured on the shared gateway bridge interface (typically br-ex or breth0) in the kernel. Additionally a route for service traffic is added as well as a static ARP/IPv6 neighbor entry.</p> <p>The services will now be reached from the nodes via this dummy next hop masquerade IP (i.e. <code>169.254.169.4</code>).</p> <p>Additionally, to enable the hairpin scenario, we need to provision a route specifying traffic originating from the host (acting as an host networked endpoint reply) must be routed to the OVN masquerade address (i.e. <code>169.254.169.1</code>) using the source IP address of the real host IP. This route looks like: <code>169.254.169.1/32 dev breth0 mtu 1400 src &lt;node IP address&gt;</code></p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#ovn","title":"OVN","text":"<p>In OVN we do not explicitly configure the <code>169.254.169.1</code> and <code>fd69::1</code> addresses on the GR interface. OVN is only able to have a single primary IP on the interface. Instead, we simply add a MAC Binding (ARP entry equivalent) for the next hop address, as well as a route for the secondary subnets to the next hop. This is all that is required in order to allow OVN to route the packet towards the fake next hop.</p> <p>The SBDB MAC binding will look like this (one per node): <pre><code>_uuid               : f244faba-19ad-4b08-bffe-d0b52b270410\ndatapath            : 7cc30875-0c68-4ecc-8193-a9fe3abb6cd5\nip                  : \"169.254.169.4\"\nlogical_port        : rtoe-GR_ovn-worker\nmac                 : \"0a:58:a9:fe:a9:04\"\ntimestamp           : 0\n</code></pre></p> <p>Each OVN GR will then have a route table that looks like this: <pre><code>[root@ovn-control-plane ~]# ovn-nbctl lr-route-list GR_ovn-worker\nIPv4 Routes\nRoute Table &lt;main&gt;:\n         169.254.169.0/29             169.254.169.4 dst-ip rtoe-GR_ovn-worker\n            10.244.0.0/16                100.64.0.1 dst-ip\n                0.0.0.0/0                172.18.0.1 dst-ip rtoe-GR_ovn-worker\n</code></pre></p> <p>Notice 169.254.169.0 route via 169.254.169.4 works, even though OVN does not have an IP on that subnet: <pre><code>[root@ovn-control-plane ~]# ovn-nbctl show GR_ovn-worker\nrouter 7c1323d5-f388-449f-94cb-51216194c606 (GR_ovn-worker)\n    port rtoj-GR_ovn-worker\n        mac: \"0a:58:64:40:00:03\"\n        networks: [\"100.64.0.3/16\"]\n    port rtoe-GR_ovn-worker\n        mac: \"02:42:ac:12:00:02\"\n        networks: [\"172.18.0.2/16\"]\n    nat 98e32e9b-e8f1-413c-881d-cfdfd5a02d43\n        external ip: \"172.18.0.3\"\n        logical ip: \"10.244.0.0/16\"\n        type: \"snat\"\n</code></pre></p> <p>This works because the output port <code>rtoe_GR_ovn-worker</code> is configured on the route. OVN will simply lookup the MAC binding for 169.254.169.4, and then forward it out the rtoe_GR_ovn-worker interface, while SNAT'ing the source IP of the packet to its primary address of 1721.8.0.3.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#ovs","title":"OVS","text":"<p>The gateway bridge flows are managed by OVN-Kubernetes. Priority 500 flows are added which are specifically there to handle service traffic for this design. These flows handle masquerading between the host and OVN, while flows in later tables take care of rewriting MAC addresses.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#openflow-flows","title":"OpenFlow Flows","text":"<p>With the new implementation comes new OpenFlow rules in the shared gateway bridge. The following flows are added and used: <pre><code> cookie=0xdeff105, duration=793.273s, table=0, n_packets=0, n_bytes=0, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=172.18.0.3,nw_dst=169.254.169.2 actions=ct(commit,table=4,zone=64001,nat(dst=172.18.0.3))\n cookie=0xdeff105, duration=793.273s, table=0, n_packets=0, n_bytes=0, priority=500,ip,in_port=LOCAL,nw_dst=169.254.169.1 actions=ct(table=5,zone=64002,nat)\n cookie=0xdeff105, duration=793.273s, table=0, n_packets=0, n_bytes=0, priority=500,ip,in_port=LOCAL,nw_dst=10.96.0.0/16 actions=ct(commit,table=2,zone=64001,nat(src=169.254.169.2))\n cookie=0xdeff105, duration=793.273s, table=0, n_packets=0, n_bytes=0, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=10.96.0.0/16,nw_dst=169.254.169.2 actions=ct(table=3,zone=64001,nat)\n\n cookie=0xdeff105, duration=5.507s, table=2, n_packets=0, n_bytes=0, actions=set_field:02:42:ac:12:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n cookie=0xdeff105, duration=5.507s, table=3, n_packets=0, n_bytes=0, actions=move:NXM_OF_ETH_DST[]-&gt;NXM_OF_ETH_SRC[],set_field:02:42:ac:12:00:03-&gt;eth_dst,LOCAL\n cookie=0xdeff105, duration=793.273s, table=4, n_packets=0, n_bytes=0, ip actions=ct(commit,table=3,zone=64002,nat(src=169.254.169.1))\n cookie=0xdeff105, duration=793.273s, table=5, n_packets=0, n_bytes=0, ip actions=ct(commit,table=2,zone=64001,nat)\n</code></pre></p> <p>How these flows are used will be explained in more detail in the following sections. The main thing to remember for now is that the shared gateway bridge will use Conntrack zones 64001 and 64002 to handle the new masquerading functionality.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#host-service-ovn-pod","title":"Host -&gt; Service -&gt; OVN Pod","text":"<p>This is the most simple case where a host wants to reach a service backed by an OVN Networked pod. For this example the service contains a single endpoint as an ovn networked pod on ovn-worker node: <pre><code>[trozet@trozet contrib]$ kubectl get ep web-service\nNAME          ENDPOINTS                       AGE\nweb-service   10.244.1.3:80,10.244.1.3:9999   6m13s\n</code></pre></p> <p>The general flow is:</p> <ol> <li>TCP Packet is sent via the host (ovn-worker) to an service: CT Entry: <pre><code>tcp      6 114 TIME_WAIT src=172.18.0.3 dst=10.96.146.87 sport=47108 dport=80 src=10.96.146.87 dst=172.18.0.3 sport=80 dport=47108 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 use=1\n</code></pre></li> <li>The host routes this packet towards the next hop's (169.254.169.4) MAC address via shared gateway bridge breth0.</li> <li>Flows in breth0 hijack the packet, SNAT to the host's masquerade IP (169.254.169.2) and send it to OF table 2: <pre><code>cookie=0xdeff105, duration=1136.261s, table=0, n_packets=12, n_bytes=884, priority=500,ip,in_port=LOCAL,nw_dst=10.96.0.0/16 actions=ct(commit,table=2,zone=64001,nat(src=169.254.169.2))\n</code></pre></li> <li>In table 2, the destination MAC address is modified to be the MAC of the OVN GR. Note, although the source MAC is the host's, OVN does not care so this is left unmodified. <pre><code>cookie=0xdeff105, duration=1.486s, table=2, n_packets=12, n_bytes=884, actions=set_field:02:42:ac:12:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n</code></pre> CT Entry: <pre><code>tcp      6 114 TIME_WAIT src=172.18.0.3 dst=10.96.146.87 sport=47108 dport=80 src=10.96.146.87 dst=169.254.169.2 sport=80 dport=47108 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=64001 use=1\n</code></pre></li> <li>OVN GR receives the packet, DNAT's to pod endpoint IP: CT Entry: <pre><code>tcp      6 114 TIME_WAIT src=169.254.169.2 dst=10.96.146.87 sport=47108 dport=80 src=10.244.1.3 dst=169.254.169.2 sport=80 dport=47108 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=16 use=1\n</code></pre></li> <li>OVN GR SNAT's to the join switch IP and sends the packet towards the pod: CT Entry: <pre><code>tcp      6 114 TIME_WAIT src=169.254.169.2 dst=10.244.1.3 sport=47108 dport=80 src=10.244.1.3 dst=100.64.0.4 sport=80 dport=47108 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 use=1\n</code></pre></li> </ol>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#reply","title":"Reply","text":"<p>The reply packet simply uses same reverse path and packets are unNAT'ed on their way back towards breth0 and eventually  the LOCAL host port. The OVN GR will think it is routing towards the next hop and set the dest MAC to be the MAC of 169.254.169.4. In OpenFlow, the return packet will hit this first flow in the shared gateway bridge: <pre><code>cookie=0xdeff105, duration=1136.261s, table=0, n_packets=11, n_bytes=1670, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=10.96.0.0/16,nw_dst=169.254.169.2 actions=ct(table=3,zone=64001,nat)\n</code></pre> This flow will unDNAT the packet, and send to table 3: <pre><code>cookie=0xdeff105, duration=1.486s, table=3, n_packets=11, n_bytes=1670, actions=move:NXM_OF_ETH_DST[]-&gt;NXM_OF_ETH_SRC[],set_field:02:42:ac:12:00:03-&gt;eth_dst,LOCAL\n</code></pre> This flow will move the dest MAC (next hop MAC) to be the source, and set the new dest MAC to be the MAC of the host. This ensures the Linux host thinks it is still talking to the external next hop. The packet is then delivered to the host.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#host-service-host-endpoint-on-different-node","title":"Host -&gt; Service -&gt; Host Endpoint on Different Node","text":"<p>This example becomes slightly more complex as the packet must be hairpinned by the GR and then sent out of the node. In this example the backing service endpoint will be a pod running on the other worker node (ovn-worker2) at 172.18.0.4: <pre><code>[trozet@trozet contrib]$ kubectl get ep web-service\nNAME          ENDPOINTS                       AGE\nweb-service   172.18.0.4:80,172.18.0.4:9999   32m\n</code></pre></p> <p>Steps 1 through 4 in this example are the same as the previous use case.</p> <ol> <li>OVN GR receives the packet, DNAT's to ovn-worker2's endpoint IP: CT Entry: <pre><code>tcp      6 116 TIME_WAIT src=169.254.169.2 dst=10.96.146.87 sport=55978 dport=80 src=172.18.0.4 dst=169.254.169.2 sport=80 dport=55978 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=16 use=1\n</code></pre></li> <li>OVN GR hairpins the packet back towards breth0 while SNAT'ing to the GR IP 172.18.0.3, and forwarding towards 172.18.0.4: <pre><code>tcp      6 116 TIME_WAIT src=172.18.0.3 dst=172.18.0.4 sport=55978 dport=80 src=172.18.0.4 dst=172.18.0.3 sport=80 dport=55978 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=64000 use=1\n</code></pre></li> <li>As this packet comes back into breth0 it is treated like any other normal packet going from OVN to the external network. The packet is Conntracked in zone 64000, marked with 1, and sent out of the eth0 interface: <pre><code>cookie=0xdeff105, duration=15820.270s, table=0, n_packets=0, n_bytes=0, priority=100,ip,in_port=\"patch-breth0_ov\" actions=ct(commit,zone=64000,exec(load:0x1-&gt;NXM_NX_CT_MARK[])),output:eth0\n</code></pre> CT Entry: <pre><code>tcp      6 116 TIME_WAIT src=172.18.0.3 dst=172.18.0.4 sport=55978 dport=80 src=172.18.0.4 dst=172.18.0.3 sport=80 dport=55978 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=64000 use=1\n</code></pre></li> </ol>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#reply_1","title":"Reply","text":"<p>When the reply packet comes back into ovn-worker via the eth0 interface, the packet will be forwarded back into OVN GR  due to a CT match in zone 64000 with a marking of 1: <pre><code>cookie=0xdeff105, duration=15820.270s, table=0, n_packets=5442, n_bytes=4579489, priority=50,ip,in_port=eth0 actions=ct(table=1,zone=64000)\ncookie=0xdeff105, duration=15820.270s, table=1, n_packets=0, n_bytes=0, priority=100,ct_state=+est+trk,ct_mark=0x1,ip actions=output:\"patch-breth0_ov\"\ncookie=0xdeff105, duration=15820.270s, table=1, n_packets=0, n_bytes=0, priority=100,ct_state=+rel+trk,ct_mark=0x1,ip actions=output:\"patch-breth0_ov\"\n</code></pre></p> <p>OVN GR will then handle unSNAT, unDNAT and send the packet back towards breth0 where the packet will be handled the same way as it was in the previous example.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#host-service-host-endpoint-on-same-node","title":"Host -&gt; Service -&gt; Host Endpoint on Same Node","text":"<p>This is the most complicated use case. Unlike the previous examples, multiple sessions have to be established and masqueraded between the host and OVN in order to trick the host into thinking it has two external connections that are not the same. This avoids issues with RPF as well as short-circuiting where the host would skip sending traffic back to OVN because it knows the traffic destination is local to itself. In this example a host network pod is used as the backend endpoint on ovn-worker: <pre><code>[trozet@trozet contrib]$ kubectl get ep web-service\nNAME          ENDPOINTS                       AGE\nweb-service   172.18.0.3:80,172.18.0.3:9999   42m\n</code></pre></p> <p>There is another manifestation of this case. Sometimes, an endpoint contains a secondary interface on a node. This can happen when endpoints are manually managed (especially in the case of api-server trickery). Thus, the endpoints look like <pre><code>$ kubectl get ep kubernetes\nNAME          ENDPOINTS                       AGE\nkubernetes    172.20.0.2:4443                  42m\n</code></pre></p> <p>where <code>172.20.0.2</code> is an \"extra\" address, maybe even on a different interface (e.g. <code>lo</code>).</p> <p>Like the previous example, Steps 1-4 are the same. Continuing with Step 5   :</p> <ol> <li>OVN GR receives the packet, DNAT's to ovn-worker's endpoint IP. However, if the endpoint IP is the node's physical IP, then it is replaced in the OVN load-balancer backends with the host masquerade IP (169.254.169.2): CT Entry: <pre><code>tcp      6 117 TIME_WAIT src=169.254.169.2 dst=10.96.146.87 sport=33316 dport=80 src=169.254.169.2 dst=169.254.169.2 sport=80 dport=33316 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=16 use=1\n</code></pre></li> <li>OVN GR hairpins the packet back towards breth0 while SNAT'ing to the GR IP 172.18.0.3, and forwarding towards 169.254.169.2: CT Entry: <pre><code>tcp      6 117 TIME_WAIT src=169.254.169.2 dst=169.254.169.2 sport=33316 dport=80 src=169.254.169.2 dst=172.18.0.3 sport=80 dport=33316 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 use=1\n</code></pre></li> <li>The packet is received in br-eth0, where it hits one of two flows, depending on if the destination is the masquerade IP or a \"secondary\" IP: <pre><code>cookie=0xdeff105, duration=2877.527s, table=0, n_packets=11, n_bytes=810, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=172.18.0.3,nw_dst=169.254.169.2 actions=ct(commit,table=4,zone=64001,nat(dst=172.18.0.3)) #primary case\ncookie=0xdeff105, duration=2877.527s, table=0, n_packets=11, n_bytes=810, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=172.18.0.3,nw_dst=127.20.0.2 actions=ct(commit,table=4,zone=64001) # secondary case\n</code></pre> This flow detects the packet is destined for the masqueraded host IP, DNATs it if necessary back to the host, and sends to table 4. CT Entry: <pre><code>tcp      6 117 TIME_WAIT src=172.18.0.3 dst=169.254.169.2 sport=33316 dport=80 src=172.18.0.3 dst=172.18.0.3 sport=80 dport=33316 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=64001 use=1\n</code></pre></li> <li>In table 4, the packet hits the following flow: <pre><code>cookie=0xdeff105, duration=2877.527s, table=4, n_packets=11, n_bytes=810, ip actions=ct(commit,table=3,zone=64002,nat(src=169.254.169.1))\n</code></pre> Here, the packet is SNAT'ed to the special OVN masquerade IP (169.254.169.1) in order to obfuscate the node IP from the host as a source address. CT Entry: <pre><code>tcp      6 117 TIME_WAIT src=172.18.0.3 dst=172.18.0.3 sport=33316 dport=80 src=172.18.0.3 dst=169.254.169.1 sport=80 dport=33316 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=64002 use=1\n</code></pre></li> <li>The packet then enters table 3, where the MAC addresses are modified and sent to the LOCAL host.</li> </ol> <p>The trick here is that the host thinks it has two different connections, which are really part of the same single connection: 1. 172.18.0.3-&gt; 10.96.x.x 2. 169.254.169.1 -&gt; 172.18.0.3</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#reply_2","title":"Reply","text":"<p>Reply traffic comes back into breth0 where via seeing a response to 169.254.169.1, we know this is a reply packet and thus hits an OpenFlow rule: <pre><code>cookie=0xdeff105, duration=2877.527s, table=0, n_packets=12, n_bytes=1736, priority=500,ip,in_port=LOCAL,nw_dst=169.254.169.1 actions=ct(table=5,zone=64002,nat)\n</code></pre> This flow will unSNAT in zone 64002, and then send the packet to table 5. In table 5 it will hit this flow: <pre><code>cookie=0xdeff105, duration=2877.527s, table=5, n_packets=12, n_bytes=1736, ip actions=ct(commit,table=2,zone=64001,nat)\n</code></pre> Here the packet will be unDNAT'ed and sent towards table 2: <pre><code>cookie=0xdeff105, duration=5.520s, table=2, n_packets=47, n_bytes=4314, actions=set_field:02:42:ac:12:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n</code></pre> Table 2 will modify the MAC accordingly and sent it back into OVN GR. OVN GR will then unSNAT, unDNAT and send the packet back to breth0 in the same fashion as the previous example.</p>"},{"location":"design/live-migration/","title":"KubeVirt virtual machine bridge binding pod network seamless live migration","text":""},{"location":"design/live-migration/#introduction","title":"Introduction","text":"<p>At kubevirt every VM is executed inside a \"virt-launcher\" pod; how the \"virt-launcher\" pod net interface is \"bound\" to the VM is specified by the network binding at the VM spec, ovn-kubernetes support live migration of the KubeVirt network bridge binding. </p> <p>The KubeVirt bridge binding adds  a bridge and a tap device and connects the existing pod interface as a port of the aforementioned bridge. When IPAM is configured on the pod interface, a DHCP server is started to advertise the pod's IP to DHCP aware VMs.</p> <p>KubeVirt doesn't replace anything. It adds a bridge and a tap device, and connects the existing pod interface as a port of the aforementioned bridge.</p> <p>Benefit of the bridge binding is that is able to expose the pod IP to the VM as expected by most users also pod network bridge binding live migration is implemented by ovn-kubernetes by doing the following: - Do IP assignemnt with ovn-k but skip the CNI part that configure it at pod's netns veth. - Do not expect the IP to be on the pod netns. - Add the ability at ovn-k controllers to migrate pod's IP from node to node.</p>"},{"location":"design/live-migration/#implementation","title":"Implementation","text":"<p>To implement live migration ovn-kubernetes do the following: - Send DHCP replies advertising the allocated IP address to the guest VM (via OVN-Kubernetes DHCP options configured for the logical switch ports). - A point to point routing is used so one node's subnet IP can be routed from different node - The VM's gateway IP and MAC are independent of the node they are running on using proxy arp</p> <p>Point to point routing:</p> <p>When the VM is running at a node that does not \"own\" it's IP address - e.g. after a live migration -  after a live migration, do point to point routing with a policy for non  interconnect and a cluster wide static route for interconnected zones, it will route outbound traffic and a static route to route inboud. By doing this, the VM can live migrate to different node and keep previous  addresses (IP / MAC), thus preserving n/s and e/w communication, and ensuring traffic goes over the node where the VM is running. The latter reduces inter node communication.</p> <p>If the VM is going back to the node that \"owns\" the ip, those static routes and  policies should be reconciled (deleted).</p> <pre><code>       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502     ovn_cluster_router    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 static route (ingress all)\u2502\u2500\u2500\u2500\u2502 policy  (egress n/s)       \u2502\n\u2502 prefix: 10.244.0.8        \u2502   \u2502 match: ip4.src==10.244.0.8 \u2502\n\u2502 nexthop: 10.244.0.8       \u2502   \u2502 action: reroute            \u2502\n\u2502 output-port: rtos-node1   \u2502   \u2502 nexthop: 10.64.0.2         \u2502\n\u2502 policy: dst-ip            \u2502   \u2502                            \u2502  \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n- static route: ingress traffic to VM's IP via VM's node port\n- policy: egress n/s traffic from VM's IP via gateway router ip\n</code></pre> <p>When ovn-kubernetes is deployed with multiple ovn zones interconected  an extra static route is added to the local zone (zone where vm is running) to route egress n/s traffic to the gw router. Note that for interconnect the previous policy is not needed, at remote zones a static route is address to  enroute VM egress traffic over the transit switch port address.</p> <p>If the transit switch port address is 10.64.0.3 and gw router is 10.64.0.2,  those below are the static route added to the topology when running with  multiple zones:</p> <pre><code>Local zone:\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502     ovn_cluster_router    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 static route (egress n/s) \u2502\u2500\u2500\u2500\u2502 static route (ingress all)\u2502\n\u2502 prefix: 10.244.0.0/16     \u2502   \u2502 prefix: 10.244.0.8        \u2502\n\u2502 nexthop: 10.64.0.2        \u2502   \u2502 nexthop: 10.244.0.8       \u2502\n\u2502 policy: src-ip            \u2502   \u2502 output-port: rtos-node1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502 policy: dst-ip            \u2502\n                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRemote zone:\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502     ovn_cluster_router    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502 static route (ingress all)\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 prefix: 10.244.0.8        \u2502\n\u2502 nexthop: 10.64.0.3        \u2502\n\u2502 policy: dst-ip            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Nodes logical switch ports:</p> <p>To have a consistent gateway at VMs (keep ip and mac after live migration)  the \"arp_proxy\" feature is used and it need to be activated at  the logical switch port of type router connects node's logical switch to  ovn_cluster_router logical router.</p> <p>The \"arp_proxy\" LSP option will include the MAC to answer ARPs with, and the link local ipv4 and ipv6 to answer for and the cluster wide pod CIDR to answer to pod subnets when the node switch do not have the live migrated ip. The flows from arp_proxy has less priority than the ones from the node logical  switch so ARP flows are not overriden.</p> <pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502logical switch node1\u2502   \u2502logical switch node2\u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502     \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 lsp stor-node1     \u2502\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2502 lsp stor-node2      \u2502\n\u2502 options:           \u2502            \u2502 options:            \u2502\n\u2502  arp_proxy:        \u2502            \u2502   arp_proxy:        \u2502\n\u2502   0a:58:0a:f3:00:00\u2502            \u2502    0a:58:0a:f3:00:00\u2502\n\u2502   169.254.1.1      \u2502            \u2502    169.254.1.1      \u2502\n\u2502   10.244.0.0/16    \u2502            \u2502    10.244.0.0/16    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>VMs logical switch ports:</p> <p>The logical switch port for new VMs will use ovn-k ipam to reserve an IP address, and when live-migrating the VM, the LSP address will be re-used.</p> <p>CNI must avoid setting the IP address on the migration destination pod, but ovn-k controllers should  preserve the IP allocation, that is done when ovn-k detects that the pod is live migratable.</p> <p>Also the DHCP options will be configured to deliver the address to the VMs</p> <pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502logical switch node1  \u2502   \u2502logical switch node2\u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502     \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \n\u2502 lsp ns-virt-launcher1\u2502\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2502 lsp ns-virt-launcher2\u2502     \n\u2502 dhcpv4_options: 1234 \u2502            \u2502 dhcpv4_options: 1234 \u2502\n\u2502 address:             \u2502            \u2502 address:             \u2502    \n\u2502  0a:58:0a:f4:00:01   \u2502            \u2502  0a:58:0a:f4:00:01   \u2502\n\u2502  10.244.0.8          \u2502            \u2502  10.244.0.8          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dhcp-options 1234               \u2502\n\u2502   lease_time: 3500              \u2502\n\u2502   router: 169.254.1.1           \u2502\n\u2502   dns_server: [kubedns]         \u2502\n\u2502   server_id: 169.254.1.1        \u2502\n\u2502   server_mac: c0:ff:ee:00:00:01 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Virt-launcher pod address: The CNI will not set an address at virt-launcher pod netns, that address is assigned to the VM with the DHCP options from the LSP, this allows to use  kubevirt bridge binding with pod networking and still do live migration.</p>"},{"location":"design/live-migration/#ipam","title":"IPAM","text":"<p>The point to point routing feature allows an address to be running at a node different from the one \"owning\" the subnet the address is coming from. This will happen after VM live migration.</p> <p>One scenario for live migration is to shut down the node the VMs were migrated from, this means that the IPAM node subnet should go back to the pool but since the migrated VMs contains IPs from it those IPs should reserved in case the  subnet is assigned to a new node.</p> <p>On that case before assigning the subnet to the node the VMs ips need to be  reserved so they don't get assigned to new pods</p> <p>Another scenario is ovn-kubernetes pods restarting after live migration, on  that case ovnkube-master should discover to what IP pool the VM belongs.</p>"},{"location":"design/live-migration/#detecting-migratable-vms-and-changing-point-to-point-routes","title":"Detecting migratable VMs and changing point to point routes","text":"<p>To detect that a pod is migratable KubevirtVm the annotation <code>kubevirt.io/allow-pod-bridge-network-live-migration</code> has to be present and also the label <code>vm.kubevirt.io/name=vm1</code>, on that case the point to point routes will be created after live migration and also a  DHCPOptions will be configured to serve the ip configuration to the VM</p> <p>During live migration there are two virt-launcher pods with different names for the same VM (source and target). The ovn-k pod controller uses <code>CreationTimestamp</code> and <code>kubevirt.io/vm=vm1</code> to differentiate between them; it then watches the target pod <code>kubevirt.io/nodeName</code> and <code>kubevirt.io/migration-target-start-timestamp</code> annotation and label, with the following intent: - The <code>kubevirt.io/nodeName</code> is set after the VM finishes live migrating or when it becomes ready. - The <code>kubevirt.io/migration-target-start-timestamp</code> is set when live migration has not finished but migration-target pod is ready to receive traffic (this happens at post-copy live migration, where migration is taking too long).</p> <p>The point to point routing cleanup (remove of static routes and policies) will be done at this cases: - VM is deleted, all the routing related to the VM is removed at all the ovn zones. - VM is live migrated back to the node that owns its IP, all the routing related to the VM is removed at all the ovn zones. - ovn-kubernetes controllers are restarted, stale routing is removed.</p>"},{"location":"design/service-traffic-policy/","title":"Kubernetes Service Traffic Policy Implementation","text":""},{"location":"design/service-traffic-policy/#external-traffic-policy","title":"External Traffic Policy","text":"<p>For Kubernetes Services of type Nodeport or Loadbalancer a user can set the <code>service.spec.externalTrafficPolicy</code> field to either <code>cluster</code> or <code>local</code> to denote whether or not external traffic is routed to cluster-wide or node-local endpoints. The default value for the <code>externalTrafficPolicy</code> field is <code>cluster</code>. In this configuration in ingress traffic is equally disributed across all backends and the original client IP address is lost due to SNAT. If set to <code>local</code> then the client source IP is preserved throughout the service flow and if service traffic arrives at nodes without local endpoints it gets dropped. See sources for more information on ETP=local.</p> <p>Setting an <code>ExternalTrafficPolicy</code> to <code>Local</code> is only allowed for Services of type <code>NodePort</code> or <code>LoadBalancer</code>. The APIServer enforces this requirement.</p>"},{"location":"design/service-traffic-policy/#implementing-externaltrafficpolicy-in-ovn-kubernetes","title":"Implementing <code>externalTrafficPolicy</code> In OVN-Kubernetes","text":"<p>To properly implement this feature for all relevant traffic flows, required changing how OVN, Iptables rules, and Physical OVS flows are updated and managed in OVN-Kubernetes</p>"},{"location":"design/service-traffic-policy/#externaltrafficpolicylocal","title":"ExternalTrafficPolicy=Local","text":""},{"location":"design/service-traffic-policy/#ovn-load_balancer-configuration","title":"OVN Load_Balancer configuration","text":"<p>Normally, each service in Kubernetes has a corresponding single Load_Balancer row created in OVN. This LB is attached to all node switches and gateway routers (GWRs). ExternalTrafficPolicy creates multiple LBs, however.</p> <p>Specifically, different load balancers are attached to switches versus routers. The node switch LBs handle traffic from pods, whereas the gateway router LBs handle external traffic.</p> <p>Thus, an additional LB is created with the <code>skip_snat=\"true\"</code> option and is applied to the GatewayRouters and Worker switches. It is needed to override the <code>lb_force_snat_ip=router_ip</code> option that is on all the Gateway Routers, which allows ingress traffic to arrive at OVN managed endpoints with the original client IP.</p> <p>All externally-accessible vips (NodePort, ExternalIPs, LoadBalancer Status IPs) for services with <code>externalTrafficPolicy:local</code> will reside on this loadbalancer. The loadbalancer backends may be empty, depending on whether there are pods local to that node.</p>"},{"location":"design/service-traffic-policy/#handling-flows-between-the-overlay-and-underlay","title":"Handling Flows between the overlay and underlay","text":"<p>In this section we will look at some relevant traffic flows when a service's <code>externalTrafficPolicy</code> is <code>local</code>.  For these examples we will be using a Nodeport service, but the flow is generally the same for ExternalIP and Loadbalancer type services.</p>"},{"location":"design/service-traffic-policy/#ingress-traffic","title":"Ingress Traffic","text":"<p>This section will cover the networking entities hit when traffic ingresses a cluster via a service to either host networked pods or cluster networked pods. If its host networked pods, then the traffic flow is the same on both gateway modes. If its cluster networked pods, they will be different for each mode.</p>"},{"location":"design/service-traffic-policy/#external-source-service-ovn-pod","title":"External Source -&gt; Service -&gt; OVN pod","text":""},{"location":"design/service-traffic-policy/#shared-gateway-mode","title":"Shared Gateway Mode","text":"<p>This case is the same as normal shared gateway traffic ingress, meaning the externally sourced traffic is routed into OVN via flows on breth0, except in this case the new local load balancer is hit on the GR, which ensures the ip of the client is preserved  by the time it gets to the destination Pod.</p> <pre><code>          host (ovn-worker, 172.18.0.3) \n           |\neth0---&gt;|breth0| -----&gt; 172.18.0.3 OVN GR 100.64.0.4 --&gt; join switch --&gt; ovn_cluster_router --&gt; 10.244.1.3 pod\n</code></pre>"},{"location":"design/service-traffic-policy/#local-gateway-mode","title":"Local Gateway Mode","text":"<p>The implementation of this case differs for local gateway from that for shared gateway. In local gateway if the above path is used, response traffic would be assymmetric since the default route for pod egress traffic is via <code>ovn-k8s-mp0</code>.</p> <p>In local gateway mode, rather than sending the traffic from breth0 into OVN via gateway router, we use flows on breth0 to send it into the host.</p> <pre><code>          host (ovn-worker, 172.18.0.3) ---- 172.18.0.3 LOCAL(host) -- iptables -- ovn-k8s-mp0 -- node-local-switch -- 10.244.1.3 pod\n           ^\n           ^\n           |\neth0---&gt;|breth0|\n</code></pre> <ol> <li>Match on the incoming traffic via default flow on <code>table0</code>, send it to <code>table1</code>:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=99979, n_bytes=298029215, priority=50,ip,in_port=eth0 actions=ct(table=1,zone=64000)\n</code></pre> <ol> <li>Send it out to LOCAL ovs port on breth0 and traffic is delivered to the host:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.787s, table=1, n_packets=108, n_bytes=23004, priority=0 actions=NORMAL\n</code></pre> <ol> <li>In the host, we have an IPtable rule in the PREROUTING chain that DNATs this packet matched on nodePort to a masqueradeIP (169.254.169.3) used specially for this traffic flow.</li> </ol> <pre><code>[3:180] -A OVN-KUBE-ETP -p tcp -m addrtype --dst-type LOCAL -m tcp --dport 31746 -j DNAT --to-destination 169.254.169.3:31746\n</code></pre> <ol> <li>The special masquerade route in the host sends this packet into OVN via the management port.</li> </ol> <pre><code>169.254.169.3 via 10.244.0.1 dev ovn-k8s-mp0 \n</code></pre> <ol> <li>Since by default, all traffic into <code>ovn-k8s-mp0</code> gets SNAT-ed, we add an IPtable rule to <code>OVN-KUBE-SNAT-MGMTPORT</code> chain to ensure it doesn't get SNAT-ed to preserve its source-ip.</li> </ol> <pre><code>[3:180] -A OVN-KUBE-SNAT-MGMTPORT -p tcp -m tcp --dport 31746 -j RETURN\n</code></pre> <ol> <li>Traffic enters the node local switch on the worker node and hits the load-balancer where we add a new vip for this masqueradeIP to DNAT it correctly to the local backends. Note that this vip will translate only to the backends that are local to that worker node and hence traffic will be rejected if there is no local endpoint thus respecting ETP=local type traffic rules.</li> </ol> <p>The switch load-balancer on a node with local endpoints will look like this:</p> <pre><code>_uuid               : b3201caf-3089-4462-b96e-1406fd7c4256\nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"default/example-service-1\"}\nhealth_check        : []\nip_port_mappings    : {}\nname                : \"Service_default/example-service-1_TCP_node_switch_ovn-worker2\"\noptions             : {event=\"false\", reject=\"true\", skip_snat=\"false\"}\nprotocol            : tcp\nselection_fields    : []\nvips                : {\"169.254.169.3:31746\"=\"10.244.1.3:8080\", \"172.18.0.3:31746\"=\"10.244.1.3:8080,10.244.2.3:8080\"}\n</code></pre> <p>The switch load-balancer on a node without local endpoints will look like this: <pre><code>_uuid               : 42d75e10-5598-4197-a6f2-1a37094bee13\nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"default/example-service-1\"}\nhealth_check        : []\nip_port_mappings    : {}\nname                : \"Service_default/example-service-1_TCP_node_switch_ovn-worker\"\noptions             : {event=\"false\", reject=\"true\", skip_snat=\"false\"}\nprotocol            : tcp\nselection_fields    : []\nvips                : {\"169.254.169.3:31746\"=\"\", \"172.18.0.4:31746\"=\"10.244.1.3:8080,10.244.2.3:8080\"}\n</code></pre></p> <p>Response traffic will follow the same path (backend-&gt;node switch-&gt;mp0-&gt;host-&gt;breth0-&gt;eth0).</p> <ol> <li>Return traffic gets matched on default flow in <code>table0</code> and it sent out via default interface back to the external source.</li> </ol> <pre><code>cookie=0xdeff105, duration=12994.192s, table=0, n_packets=47706, n_bytes=3199460, idle_age=0, priority=100,ip,in_port=LOCAL actions=ct(commit,zone=64000,exec(load:0x2-&gt;NXM_NX_CT_MARK[])),output:1\n</code></pre> <p>The conntrack state looks like this: <pre><code>    [NEW] tcp      6 120 SYN_SENT src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 [UNREPLIED] src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366\n    [NEW] tcp      6 120 SYN_SENT src=172.18.0.1 dst=169.254.169.3 sport=36366 dport=31746 [UNREPLIED] src=10.244.1.3 dst=172.18.0.1 sport=8080 dport=36366 zone=9\n    [NEW] tcp      6 120 SYN_SENT src=172.18.0.1 dst=10.244.1.3 sport=36366 dport=8080 [UNREPLIED] src=10.244.1.3 dst=172.18.0.1 sport=8080 dport=36366 zone=11\n [UPDATE] tcp      6 60 SYN_RECV src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366\n [UPDATE] tcp      6 432000 ESTABLISHED src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366 [ASSURED]\n    [NEW] tcp      6 300 ESTABLISHED src=172.18.0.3 dst=172.18.0.1 sport=31746 dport=36366 [UNREPLIED] src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 mark=2 zone=64000\n [UPDATE] tcp      6 120 FIN_WAIT src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366 [ASSURED]\n [UPDATE] tcp      6 30 LAST_ACK src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366 [ASSURED]\n [UPDATE] tcp      6 120 TIME_WAIT src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366 [ASSURED]\n</code></pre></p>"},{"location":"design/service-traffic-policy/#allocateloadbalancernodeportsfalse","title":"AllocateLoadBalancerNodePorts=False","text":"<p>If AllocateLoadBalancerNodePorts=False then we cannot follow the same path as above since there won't be any node ports to DNAT to. Thus, for this special case which is only applicable to services of type LoadBalancer, we directly DNAT to the endpoints. Steps 1 &amp; 2 are same as above i.e packet arrives into the host via openflows on <code>breth0</code>.</p> <ol> <li>In the host, we introduce IPtable rules in the PREROUTING chain that DNATs this packet matched on externalIP or load balancer ingress VIP directly to the pod backend using random probability mode algorithm</li> </ol> <pre><code>[0:0] -A OVN-KUBE-ETP -d 172.18.0.10/32 -p tcp -m tcp --dport 80 -m statistic --mode random --probability 0.50000000000 -j DNAT --to-destination 10.244.0.3:8080\n[3:180] -A OVN-KUBE-ETP -d 172.18.0.10/32 -p tcp -m tcp --dport 80 -m statistic --mode random --probability 1.00000000000 -j DNAT --to-destination 10.244.0.4:8080\n</code></pre> <ol> <li>The pod subnet route in the host sends this packet into OVN via the management port.</li> </ol> <pre><code>10.244.0.0/24 dev ovn-k8s-mp0 proto kernel scope link src 10.244.0.2 \n</code></pre> <ol> <li>Since by default, all traffic into <code>ovn-k8s-mp0</code> gets SNAT-ed, we add an IPtable rule to <code>OVN-KUBE-SNAT-MGMTPORT</code> chain to ensure it doesn't get SNAT-ed to preserve its source-ip.</li> </ol> <pre><code>[0:0] -A OVN-KUBE-SNAT-MGMTPORT -d 10.244.0.3/32 -p tcp -m tcp --dport 8080 -j RETURN\n[3:180] -A OVN-KUBE-SNAT-MGMTPORT -d 10.244.0.4/32 -p tcp -m tcp --dport 8080 -j RETURN\n</code></pre> <ol> <li>Traffic hits the switch and enters the destination pod. OVN load balancers play no role in this traffic flow.</li> </ol> <p>Here are conntrack entries for this traffic: <pre><code>tcp,orig=(src=172.18.0.5,dst=10.244.0.4,sport=50134,dport=8080),reply=(src=10.244.0.4,dst=172.18.0.5,sport=8080,dport=50134),zone=11,protoinfo=(state=TIME_WAIT)\ntcp,orig=(src=172.18.0.5,dst=172.18.0.10,sport=50134,dport=80),reply=(src=10.244.0.4,dst=172.18.0.5,sport=8080,dport=50134),protoinfo=(state=TIME_WAIT)\ntcp,orig=(src=172.18.0.10,dst=172.18.0.5,sport=80,dport=50134),reply=(src=172.18.0.5,dst=172.18.0.10,sport=50134,dport=80),zone=64000,mark=2,protoinfo=(state=TIME_WAIT)\ntcp,orig=(src=172.18.0.5,dst=10.244.0.4,sport=50134,dport=8080),reply=(src=10.244.0.4,dst=172.18.0.5,sport=8080,dport=50134),zone=10,protoinfo=(state=TIME_WAIT)\n</code></pre></p>"},{"location":"design/service-traffic-policy/#external-source-service-host-networked-pod","title":"External Source -&gt; Service -&gt; Host Networked pod","text":"<p>This Scenario is a bit different, specifically traffic now needs to be directed from an external source to service and then to the host itself (a host networked pod)</p> <p>In this flow, rather than going from breth0 into OVN we shortcircuit the path with physical flows on breth0. This is the same for both the gateway modes.</p> <pre><code>          host (ovn-worker, 172.18.0.3) \n           ^\n           ^\n           |\neth0---&gt;|breth0| ---- 172.18.0.3 OVN GR 100.64.0.4 -- join switch -- ovn_cluster_router -- 10.244.1.3 pod\n</code></pre> <ol> <li>Match on the incoming traffic via it's nodePort, DNAT directly to the host networked endpoint, and send to <code>table=6</code></li> </ol> <pre><code>cookie=0x790ba3355d0c209b, duration=153.288s, table=0, n_packets=18, n_bytes=1468, idle_age=100, priority=100,tcp,in_port=1,tp_dst=&lt;nodePort&gt; actions=ct(commit,table=6,zone=64003,nat(dst=&lt;nodeIP&gt;:&lt;targetIP&gt;))\n</code></pre> <ol> <li>Send out the LOCAL ovs port on breth0, and traffic is delivered to host netwoked pod</li> </ol> <pre><code>cookie=0x790ba3355d0c209b, duration=113.033s, table=6, n_packets=18, n_bytes=1468, priority=100 actions=LOCAL\n</code></pre> <ol> <li>Return traffic from the host networked pod to external source is matched in <code>table=7</code> based on the src_ip of the return    traffic being equal to <code>&lt;targetIP&gt;</code>, and un-Nat back to <code>&lt;nodeIP&gt;:&lt;NodePort&gt;</code></li> </ol> <pre><code> cookie=0x790ba3355d0c209b, duration=501.037s, table=0, n_packets=12, n_bytes=1259, idle_age=448, priority=100,tcp,in_port=LOCAL,tp_src=&lt;targetIP&gt; actions=ct(commit,table=7,zone=64003,nat)\n</code></pre> <ol> <li>Send the traffic back out breth0 back to the external source in <code>table=7</code></li> </ol> <pre><code>cookie=0x790ba3355d0c209b, duration=501.037s, table=7, n_packets=12, n_bytes=1259, idle_age=448, priority=100 actions=output:1\n</code></pre>"},{"location":"design/service-traffic-policy/#host-traffic","title":"Host Traffic","text":"<p>NOTE: Host-&gt; svc (NP/EIP/LB) is neither \"internal\" nor \"external\" traffic, hence it defaults to special case \"Cluster\" even if ETP=local. Only Host-&gt;differentNP traffic flow obeys ETP=local.</p>"},{"location":"design/service-traffic-policy/#externaltrafficpolicycluster","title":"ExternalTrafficPolicy=Cluster","text":""},{"location":"design/service-traffic-policy/#local-gateway-mode_1","title":"Local Gateway Mode","text":""},{"location":"design/service-traffic-policy/#external-source-service-host-networked-pod-non-hairpin-case","title":"External Source -&gt; Service -&gt; Host Networked pod (non-hairpin case)","text":"<p>NOTE: Same steps happen for <code>Host -&gt; Service -&gt; Host Networked Pods (non-hairpin case)</code>.</p> <p>The implementation of this case differs for local gateway from that for shared gateway. In local gateway all service traffic is sent straight to host (instead of sending it to OVN) to allow users to apply custom routes according to their use cases.</p> <p>In local gateway mode, rather than sending the traffic from breth0 into OVN via gateway router, we use flows on breth0 to send it into the host. Similarly rather than sending the DNAT-ed traffic from OVN to wire, we send it to host first.</p> <pre><code>          host (ovn-worker2, 172.19.0.3) ---- 172.19.0.3 LOCAL(host) -- iptables -- breth0 -- GR -- breth0 -- host -- breth0 -- eth0 (backend ovn-worker 172.19.0.4)\n           ^\n           ^\n           |\neth0---&gt;|breth0|\n</code></pre> <p>SYN flow:</p> <ol> <li>Match on the incoming traffic via default flow on <code>table0</code>, send it to <code>table1</code>:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=99979, n_bytes=298029215, priority=50,ip,in_port=eth0 actions=ct(table=1,zone=64000,nat)\n</code></pre> <ol> <li>Send it out to LOCAL ovs port on breth0 and traffic is delivered to the host:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.787s, table=1, n_packets=108, n_bytes=23004, priority=0 actions=NORMAL\n</code></pre> <ol> <li>In the host, we have an IPtable rule in the PREROUTING chain that DNATs this packet matched on nodePort to its clusterIP:targetPort</li> </ol> <pre><code>[8:480] -A OVN-KUBE-NODEPORT -p tcp -m addrtype --dst-type LOCAL -m tcp --dport 31339 -j DNAT --to-destination 10.96.115.103:80\n</code></pre> <ol> <li>The service route in the host sends this packet back to breth0.</li> </ol> <pre><code>10.96.0.0/16 via 169.254.169.4 dev breth0 mtu 1400 \n</code></pre> <ol> <li>On breth0, we have priority 500 flows meant to handle hairpining, that will SNAT the srcIP to the special <code>169.254.169.2</code> masqueradeIP and send it to <code>table2</code></li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=11, n_bytes=814, priority=500,ip,in_port=LOCAL,nw_dst=10.96.0.0/16 actions=ct(commit,table=2,zone=64001,nat(src=169.254.169.2))\n</code></pre> <ol> <li>In <code>table2</code> we have a flow that forwards this to patch port that takes the traffic in OVN:</li> </ol> <pre><code>cookie=0xdeff105, duration=6.308s, table=2, n_packets=11, n_bytes=814, actions=set_field:02:42:ac:12:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n</code></pre> <ol> <li>Traffic enters the GR on the worker node and hits the load-balancer where we DNAT it correctly to the local backends.</li> </ol> <p>The GR load-balancer on a node with endpoints for the clusterIP will look like this:</p> <pre><code>_uuid               : 4e7ff1e3-a211-45d7-8243-54e087ca3965                                                                                                                   \nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"default/example-service-hello-world\"}                                                                \nhealth_check        : []                                                                                                                                                     \nip_port_mappings    : {}                                                                                                                                                     \nname                : \"Service_default/example-service-hello-world_TCP_node_router+switch_ovn-control-plane\"                                                                 \noptions             : {event=\"false\", hairpin_snat_ip=\"169.254.169.5 fd69::5\", reject=\"true\", skip_snat=\"false\"}                                                             \nprotocol            : tcp                                                                                                                                                    \nselection_fields    : []                                                                                                                                                     \nvips                : {\"10.96.115.103:80\"=\"172.19.0.3:8080\", \"172.19.0.3:31339\"=\"172.19.0.4:8080\"} \n</code></pre> <ol> <li>Traffic from OVN is sent back to host:</li> </ol> <pre><code>  cookie=0xdeff105, duration=839.789s, table=0, n_packets=6, n_bytes=484, priority=175,tcp,in_port=\"patch-breth0_ov\",nw_src=172.19.0.3 actions=ct(table=4,zone=64001)\n  cookie=0xdeff105, duration=2334.510s, table=4, n_packets=18, n_bytes=1452, ip actions=ct(commit,table=3,zone=64002,nat(src=169.254.169.1))\n  cookie=0xdeff105, duration=1.612s, table=3, n_packets=10, n_bytes=892, actions=move:NXM_OF_ETH_DST[]-&gt;NXM_OF_ETH_SRC[],set_field:02:42:ac:13:00:03-&gt;eth_dst,LOCAL\n</code></pre> <ol> <li>The routes in the host send this back to breth0:</li> </ol> <pre><code>169.254.169.1 dev breth0 src 172.19.0.4 mtu 1400 \n</code></pre> <ol> <li>Traffic leaves to primary interface from breth0:</li> </ol> <pre><code> cookie=0xdeff105, duration=2334.510s, table=0, n_packets=7611, n_bytes=754388, priority=100,ip,in_port=LOCAL actions=ct(commit,zone=64000,exec(load:0x2-&gt;NXM_NX_CT_MARK[])),output:eth0\n</code></pre> <p>Packet goes to other host via underlay.</p> <p>SYNACK flow:</p> <ol> <li>Match on the incoming traffic via default flow on <code>table0</code>, send it to <code>table1</code>:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=99979, n_bytes=298029215, priority=50,ip,in_port=eth0 actions=ct(table=1,zone=64000,nat)\n</code></pre> <ol> <li>Send it out to LOCAL ovs port on breth0 and traffic is delivered to the host:</li> </ol> <pre><code> cookie=0xdeff105, duration=2334.510s, table=1, n_packets=9466, n_bytes=4512265, priority=100,ct_state=+est+trk,ct_mark=0x2,ip actions=LOCAL\n</code></pre> <ol> <li>Before coming to host in breth0 using above flow it will get unSNATed back to .1 masqueradeIP in 64000 zone, then unDNATed back to clusterIP using iptables and sent to OVN:</li> </ol> <pre><code> cookie=0xdeff105, duration=2334.510s, table=0, n_packets=14, n_bytes=1356, priority=500,ip,in_port=LOCAL,nw_dst=169.254.169.1 actions=ct(table=5,zone=64002,nat)\n cookie=0xdeff105, duration=2334.510s, table=5, n_packets=14, n_bytes=1356, ip actions=ct(commit,table=2,zone=64001,nat)\n cookie=0xdeff105, duration=0.365s, table=2, n_packets=33, n_bytes=2882, actions=set_field:02:42:ac:13:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n</code></pre> <ol> <li>From OVN it gets sent back to host and then back from host into breth0 and into the wire:</li> </ol> <pre><code>  cookie=0xdeff105, duration=2334.510s, table=0, n_packets=18, n_bytes=1452, priority=175,ip,in_port=\"patch-breth0_ov\",nw_src=172.19.0.4 actions=ct(table=4,zone=64001,nat)\n  cookie=0xdeff105, duration=2334.510s, table=4, n_packets=18, n_bytes=1452, ip actions=ct(commit,table=3,zone=64002,nat(src=169.254.169.1))\n  cookie=0xdeff105, duration=0.365s, table=3, n_packets=32, n_bytes=2808, actions=move:NXM_OF_ETH_DST[]-&gt;NXM_OF_ETH_SRC[],set_field:02:42:ac:13:00:03-&gt;eth_dst,LOCAL\n  cookie=0xdeff105, duration=2334.510s, table=0, n_packets=7611, n_bytes=754388, priority=100,ip,in_port=LOCAL actions=ct(commit,zone=64000,exec(load:0x2-&gt;NXM_NX_CT_MARK[])),output:eth0\n</code></pre> <p>NOTE: We have added a masquerade rule to iptable rules to SNAT towards the netIP of the interface via which the packet leaves.</p> <pre><code>[12:720] -A POSTROUTING -s 169.254.169.0/29 -j MASQUERADE\n</code></pre> <p>tcpdump: <pre><code>SYN:\n13:38:52.988279 eth0  In  ifindex 19 02:42:df:4d:b6:d2 ethertype IPv4 (0x0800), length 80: 172.19.0.1.36363 &gt; 172.19.0.4.30950: Flags [S], seq 3548868802, win 64240, options [mss 1460,sackOK,TS val 1854443570 ecr 0,nop,wscale 7], length 0\n13:38:52.988315 breth0 In  ifindex 6 02:42:df:4d:b6:d2 ethertype IPv4 (0x0800), length 80: 172.19.0.1.36363 &gt; 172.19.0.4.30950: Flags [S], seq 3548868802, win 64240, options [mss 1460,sackOK,TS val 1854443570 ecr 0,nop,wscale 7], length 0\n13:38:52.988357 breth0 Out ifindex 6 02:42:ac:13:00:04 ethertype IPv4 (0x0800), length 80: 172.19.0.1.36363 &gt; 10.96.211.228.80: Flags [S], seq 3548868802, win 64240, options [mss 1460,sackOK,TS val 1854443570 ecr 0,nop,wscale 7], length 0\n13:38:52.989240 breth0 In  ifindex 6 02:42:ac:13:00:03 ethertype IPv4 (0x0800), length 80: 169.254.169.1.36363 &gt; 172.19.0.3.8080: Flags [S], seq 3548868802, win 64240, options [mss 1460,sackOK,TS val 1854443570 ecr 0,nop,wscale 7], length 0\n13:38:52.989240 breth0 In  ifindex 6 02:42:ac:13:00:03 ethertype IPv4 (0x0800), length 80: 172.19.0.4.31991 &gt; 172.19.0.3.8080: Flags [S], seq 3548868802, win 64240, options [mss 1460,sackOK,TS val 1854443570 ecr 0,nop,wscale 7], length 0\nSYNACK:\n13:38:52.989515 breth0 Out ifindex 6 02:42:ac:13:00:04 ethertype IPv4 (0x0800), length 80: 172.19.0.3.8080 &gt; 172.19.0.4.31991: Flags [S.], seq 3406651567, ack 3548868803, win 65160, options [mss 1460,sackOK,TS val 2294391439 ecr 1854443570,nop,wscale 7], length 0\n13:38:52.989515 breth0 Out ifindex 6 02:42:ac:13:00:04 ethertype IPv4 (0x0800), length 80: 172.19.0.3.8080 &gt; 169.254.169.1.36363: Flags [S.], seq 3406651567, ack 3548868803, win 65160, options [mss 1460,sackOK,TS val 2294391439 ecr 1854443570,nop,wscale 7], length 0\n13:38:52.989562 breth0 In  ifindex 6 0a:58:a9:fe:a9:04 ethertype IPv4 (0x0800), length 80: 10.96.211.228.80 &gt; 172.19.0.1.36363: Flags [S.], seq 3406651567, ack 3548868803, win 65160, options [mss 1460,sackOK,TS val 2294391439 ecr 1854443570,nop,wscale 7], length 0\n13:38:52.989571 breth0 Out ifindex 6 02:42:ac:13:00:04 ethertype IPv4 (0x0800), length 80: 172.19.0.4.30950 &gt; 172.19.0.1.36363: Flags [S.], seq 3406651567, ack 3548868803, win 65160, options [mss 1460,sackOK,TS val 2294391439 ecr 1854443570,nop,wscale 7], length 0\n13:38:52.989581 eth0  Out ifindex 19 02:42:ac:13:00:04 ethertype IPv4 (0x0800), length 80: 172.19.0.4.30950 &gt; 172.19.0.1.36363: Flags [S.], seq 3406651567, ack 3548868803, win 65160, options [mss 1460,sackOK,TS val 2294391439 ecr 1854443570,nop,wscale 7], length 0\n</code></pre></p>"},{"location":"design/service-traffic-policy/#external-source-service-ovn-pod_1","title":"External Source -&gt; Service -&gt; OVN pod","text":"<p>The implementation of this case differs for local gateway from that for shared gateway. In local gateway all service traffic is sent straight to host (instead of sending it to OVN) to allow users to apply custom routes according to their use cases.</p> <p>In local gateway mode, rather than sending the traffic from breth0 into OVN via gateway router, we use flows on breth0 to send it into the host.</p> <pre><code>          host (ovn-worker, 172.18.0.3) ---- 172.18.0.3 LOCAL(host) -- iptables -- breth0 -- GR -- 10.244.1.3 pod\n           ^\n           ^\n           |\neth0---&gt;|breth0|\n</code></pre> <ol> <li>Match on the incoming traffic via default flow on <code>table0</code>, send it to <code>table1</code>:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=99979, n_bytes=298029215, priority=50,ip,in_port=eth0 actions=ct(table=1,zone=64000)\n</code></pre> <ol> <li>Send it out to LOCAL ovs port on breth0 and traffic is delivered to the host:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.787s, table=1, n_packets=108, n_bytes=23004, priority=0 actions=NORMAL\n</code></pre> <ol> <li>In the host, we have an IPtable rule in the PREROUTING chain that DNATs this packet matched on nodePort to its clusterIP:targetPort</li> </ol> <pre><code>[8:480] -A OVN-KUBE-NODEPORT -p tcp -m addrtype --dst-type LOCAL -m tcp --dport 31842 -j DNAT --to-destination 10.96.67.170:80\n</code></pre> <ol> <li>The service route in the host sends this packet back to breth0.</li> </ol> <pre><code>10.96.0.0/16 via 172.18.0.1 dev breth0 mtu 1400\n</code></pre> <ol> <li>On breth0, we have priority 500 flows meant to handle hairpining, that will SNAT the srcIP to the special <code>169.254.169.2</code> masqueradeIP and send it to <code>table2</code></li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=11, n_bytes=814, priority=500,ip,in_port=LOCAL,nw_dst=10.96.0.0/16 actions=ct(commit,table=2,zone=64001,nat(src=169.254.169.2))\n</code></pre> <ol> <li>In <code>table2</code> we have a flow that forwards this to patch port that takes the traffic in OVN:</li> </ol> <pre><code>cookie=0xdeff105, duration=6.308s, table=2, n_packets=11, n_bytes=814, actions=set_field:02:42:ac:12:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n</code></pre> <ol> <li>Traffic enters the GR on the worker node and hits the load-balancer where we DNAT it correctly to the local backends.</li> </ol> <p>The GR load-balancer on a node with endpoints for the clusterIP will look like this:</p> <pre><code>_uuid               : b3201caf-3089-4462-b96e-1406fd7c4256\nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"default/example-service-1\"}\nhealth_check        : []\nip_port_mappings    : {}\nname                : \"Service_default/example-service-1_TCP_cluster\"\noptions             : {event=\"false\", reject=\"true\", skip_snat=\"false\"}\nprotocol            : tcp\nselection_fields    : []\nvips                : {\"10.96.67.170:80\"=\"10.244.1.3:8080,10.244.2.3:8080\"}\n</code></pre> <p>Response traffic will follow the same path (backend-&gt;GR-&gt;breth0-&gt;host-&gt;breth0-&gt;eth0).</p> <ol> <li>Return traffic gets matched on the priority 500 flow in <code>table0</code> which sends it to <code>table3</code>.</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=10, n_bytes=540, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=10.96.0.0/16,nw_dst=169.254.169.2 actions=ct(table=3,zone=64001,nat)\n</code></pre> <ol> <li>In <code>table3</code>, we send it to host:</li> </ol> <pre><code>cookie=0xdeff105, duration=6.308s, table=3, n_packets=10, n_bytes=540, actions=move:NXM_OF_ETH_DST[]-&gt;NXM_OF_ETH_SRC[],set_field:02:42:ac:12:00:03-&gt;eth_dst,LOCAL\n</code></pre> <ol> <li>From host we send it back to breth0 using:</li> </ol> <pre><code>cookie=0xdeff105, duration=5992.878s, table=0, n_packets=89312, n_bytes=6154654, idle_age=0, priority=100,ip,in_port=LOCAL actions=ct(commit,zone=64000,exec(load:0x2-&gt;NXM_NX_CT_MARK[])),output:eth0\n</code></pre> <p>where packet leaves the node and goes back to the external entity that initiated the connection.</p>"},{"location":"design/service-traffic-policy/#sources","title":"Sources","text":"<ul> <li>https://www.asykim.com/blog/deep-dive-into-kubernetes-external-traffic-policies</li> </ul>"},{"location":"design/service-traffic-policy/#internal-traffic-policy","title":"Internal Traffic Policy","text":"<p>Service Internal Traffic Policy (ITP) is a feature that can be enabled on a kubernetes service type object. This feature imposes restrictions on traffic that originates internally by routing it only to endpoints that are local to the node from where the traffic originated from. Here \"internal\" traffic means traffic originating from pods and nodes within the cluster. See https://kubernetes.io/docs/concepts/services-networking/service-traffic-policy/ for more details.</p> <pre><code>NOTE1: ITP is applicable only to service of type \"ClusterIP\" meaning it has no effect on services of type NodePorts, ExternalIPs or LoadBalancers. So if\nInternalTrafficPolicy=Local for services of types NodePorts, ExternalIPs or LoadBalancers, then the restirction of traffic policy will apply only to the\nclusterIP serviceVIP of these service types.\n\nNOTE2: Unlike ETP, it is not necessary that the srcIP be preserved in case of ITP.\n</code></pre> <p>By default ITP is of type <code>Cluster</code> on all services; meaning internal traffic will be routed to any of the endpoints for that service. When it is set to <code>Local</code>, only local endpoints are considered. The way this is implemented in OVN-K is by filtering out endpoints from the load balancer on the node switches for <code>ClusterIP</code> services .</p>"},{"location":"design/service-traffic-policy/#internaltrafficpolicylocal","title":"InternalTrafficPolicy=Local","text":"<p>This feature is implemented exactly in the same way for both the gateway modes.</p>"},{"location":"design/service-traffic-policy/#pod-traffic","title":"Pod Traffic","text":"<p>This section will cover the networking entities hit when traffic travels from an OVN pod via a service backed by either host networked pods or cluster networked pods.</p> <p>We have a service of type <code>NodePort</code> where <code>internalTrafficPolicy: Local</code>:</p> <pre><code>$ oc get svc\nNAMESPACE        NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE\ndefault          hello-world-2   NodePort    10.96.61.132   172.18.0.9    80:31358/TCP             108m\n$ oc get ep\nNAME            ENDPOINTS                         AGE\nhello-world-2   10.244.0.6:8080,10.244.1.3:8080   111m\n</code></pre> <p>This service is backed by two ovn pods: <pre><code>$ oc get pods -owide -n surya\nNAME                            READY   STATUS    RESTARTS   AGE    IP           NODE          NOMINATED NODE   READINESS GATES\nhello-world-2-5c87676b7-h6rm5   1/1     Running   0          111m   10.244.0.6   ovn-worker    &lt;none&gt;           &lt;none&gt;\nhello-world-2-5c87676b7-l82w8   1/1     Running   0          111m   10.244.1.3   ovn-worker2   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>Traffic from pod hits the load balancer on the switch where the non-local endpoints for clusterIPs are filtered out. So traffic will be DNAT-ed to local endpoints if any. Here is how the load balancers will look like on the three KIND worker nodes for the above service.</p> <pre><code>a7865d9f-43d2-4cf9-a316-fc35e8c357a8    Service_surya/he    tcp        10.96.61.132:80        \n                                                            tcp        172.18.0.4:31358       10.244.0.6:8080,10.244.1.3:8080\n                                                            tcp        172.18.0.9:80          10.244.0.6:8080,10.244.1.3:8080\nd58c012a-7b1f-45ad-a817-a86845fde164    Service_surya/he    tcp        10.96.61.132:80        10.244.0.6:8080\n                                                            tcp        172.18.0.2:31358       10.244.0.6:8080,10.244.1.3:8080\n                                                            tcp        172.18.0.9:80          10.244.0.6:8080,10.244.1.3:8080\na993003a-a177-4673-8f7c-825e2c9bf205    Service_surya/he    tcp        10.96.61.132:80        10.244.1.3:8080\n                                                            tcp        172.18.0.3:31358       10.244.0.6:8080,10.244.1.3:8080\n                                                            tcp        172.18.0.9:80          10.244.0.6:8080,10.244.1.3:8080\n</code></pre> <p>Once the packet is DNAT-ed it is then redirected to the backend pod.</p> <p>NOTE: This traffic flow behaves exactly the same if the backend pods were host-networked.</p>"},{"location":"design/service-traffic-policy/#host-traffic_1","title":"Host Traffic","text":"<p>This section will cover the networking entities hit when traffic travels from a cluster host via a clusterIP service to either host networked pods or cluster networked pods. Note that host to clusterIP is considered \"internal\" traffic.</p>"},{"location":"design/service-traffic-policy/#host-service-clusterip-ovn-pod","title":"Host -&gt; Service (ClusterIP) -&gt; OVN Pod","text":"<ol> <li>Packet generated from the host towards clusterIP service <code>10.96.61.132:80</code> is marked for forwarding in the <code>mangle</code> table by an IP table rule called from the <code>OUTPUT</code> chain:</li> </ol> <pre><code>-A OUTPUT -j OVN-KUBE-ITP\n[1:60] -A OVN-KUBE-ITP -d 10.96.61.132/32 -p tcp -m tcp --dport 80 -j MARK --set-xmark 0x1745ec/0xffffffff\n</code></pre> <ol> <li>A routing policy (priority 30) is setup in the database to match on this mark and send it to custom routing table <code>number 7</code>:</li> </ol> <pre><code>root@ovn-worker:/# ip rule\n30: from all fwmark 0x1745ec lookup 7\n</code></pre> <ol> <li>In routing table <code>7</code> we have a route that steers this traffic into management port <code>ovn-k8s-mp0</code> instead of sending it via default service route towards <code>breth0</code>:</li> </ol> <pre><code>root@ovn-worker:/# ip r show table 7\n10.96.0.0/16 via 10.244.0.1 dev ovn-k8s-mp0 \n</code></pre> <ol> <li>Packet enters ovn via <code>k8s-nodename</code> port and hits the load balancer on the switch where the packet is DNAT-ed into the local endpoint if any, else rejected.</li> </ol> <p>Here is a sample load balancer on <code>ovn-worker</code> node which has an endpoint:</p> <pre><code>_uuid               : d58c012a-7b1f-45ad-a817-a86845fde164\nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"surya/hello-world-2\"}\nhealth_check        : []\nip_port_mappings    : {}\nname                : \"Service_surya/hello-world-2_TCP_node_switch_ovn-worker\"\noptions             : {event=\"false\", reject=\"true\", skip_snat=\"false\"}\nprotocol            : tcp\nselection_fields    : []\nvips                : {\"10.96.61.132:80\"=\"10.244.0.6:8080\", \"172.18.0.2:31358\"=\"10.244.0.6:8080,10.244.1.3:8080\", \"172.18.0.9:80\"=\"10.244.0.6:8080,10.244.1.3:8080\"}\n</code></pre> <p>Note that only endpoints for <code>ClusterIP</code> are filtered, externalIPs/nodePorts are not.</p> <p>Here is a sample load balancer on <code>ovn-control-plane</code> node which does not have an endpoint:</p> <pre><code>_uuid               : a7865d9f-43d2-4cf9-a316-fc35e8c357a8\nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"surya/hello-world-2\"}\nhealth_check        : []\nip_port_mappings    : {}\nname                : \"Service_surya/hello-world-2_TCP_node_switch_ovn-control-plane\"\noptions             : {event=\"false\", reject=\"true\", skip_snat=\"false\"}\nprotocol            : tcp\nselection_fields    : []\nvips                : {\"10.96.61.132:80\"=\"\", \"172.18.0.4:31358\"=\"10.244.0.6:8080,10.244.1.3:8080\", \"172.18.0.9:80\"=\"10.244.0.6:8080,10.244.1.3:8080\"}\n</code></pre> <ol> <li>Packet is then delivered to backend pod.</li> </ol>"},{"location":"design/service-traffic-policy/#host-service-host-networked-pod","title":"Host -&gt; Service -&gt; Host Networked Pod","text":"<p>When the backend is a host networked pod we shortcircuit OVN to counter reverse path filtering issues and use iptables rules on the host to DNAT directly to the correct host endpoint.</p> <pre><code>[1:60] -A OVN-KUBE-ITP -d 10.96.48.132/32 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8080\n</code></pre> <p>NOTE: If a service with ITP=local has both host-networked pods and ovn pods as local endpoints, traffic will always be delivered to the host-networked pod. This is acceptable since traffic policy claims unfair load balancing as a side effect of the feature.</p>"},{"location":"design/topology/","title":"OVN Kubernetes Network Topology","text":"<p>Like we saw earlier in the architecture section there are two modes of deployment in OVN Kubernetes:</p> <ul> <li>default mode (centralized control plane architecture)</li> <li>interconnect mode (distributed control plane architecture)</li> </ul> <p>Based on the mode, there are subtle differences in network topology running on each node in the cluster</p>"},{"location":"design/topology/#ovn-kubernetes-network-topology-default-mode","title":"OVN-Kubernetes Network Topology - Default Mode","text":"<p>The centralized architecture in OVN-K looks like this today:</p> <p> </p> <p>On each node we have:</p> <ul> <li>node-local-switch: all the logical switch ports for the pods created on a node are bound to this switch and it also hosts load balancers that take care of DNAT-ing the service traffic</li> <li>distributed-ovn-cluster-router: it's responsible for tunnelling overlay traffic between the nodes and also routing traffic between the node switches and gateway router's</li> <li>distributed-join-switch: connects the ovn-cluster-router to the gateway routers</li> <li>node-local-gateway-router: it's responsible for north-south traffic routing and connects the join switch to the external switch and it also hosts load balancers that take care of DNAT-ing the service traffic</li> <li>node-local-external-switch: connects the gateway router to the external bridge</li> </ul>"},{"location":"design/topology/#ovn-kubernetes-network-topology-distributed-interconnect","title":"OVN-Kubernetes Network Topology - Distributed (Interconnect)","text":"<p>The interconnect architecture in OVN-K looks like this today (we assume each node is in a zone of their own):</p> <p> </p> <p>On each node we have:</p> <ul> <li>node-local-switch: all the logical switch ports for the pods created on a node are bound to this switch and it also hosts load balancers that take care of DNAT-ing the service traffic</li> <li>distributed-ovn-cluster-router: it's responsible for tunnelling overlay traffic between the nodes and also routing traffic between the node switches and gateway router's (note that if its one node per zone this behaves like a local router since there is no need for a distributed setup; if there are multiple nodes in the same zone, then it uses GENEVE tunnel for overlay traffic)</li> <li>distributed-join-switch: connects the ovn-cluster-router to the gateway routers (note that if its one node per zone this behaves like local switch since there is no need for a distributed setup; if there are multiple nodes in the same zone, then its distributed and connects cross more than one gateway router)</li> <li>node-local-gateway-router: it's responsible for north-south traffic routing and connects the join switch to the external switch and it also hosts load balancers that take care of DNAT-ing the service traffic</li> <li>node-local-external-switch: connects the gateway router to the external bridge</li> <li>transit-switch: This is the shiny new component coming in for IC. It is distributed across the nodes in the cluster and is responsible for routing traffic between the different zones.</li> </ul> <p>FIXME: This page is lazily written, there is so much more to do here.</p>"},{"location":"design/topology/#references","title":"References","text":"<ul> <li>https://docs.google.com/presentation/d/1BtkYAO30gI3v6ah2hS6XTGtt6JBHNRHh64vhGEtfLEM/edit#slide=id.gfb215b3717_0_3299</li> </ul>"},{"location":"developer-guide/developer/","title":"Developer Documentation","text":"<p>This file aims to have information that is useful to the people contributing to this repo.</p>"},{"location":"developer-guide/developer/#generating-ovsdb-bindings-using-modelgen","title":"Generating ovsdb bindings using modelgen","text":"<p>In order to generate the latest NBDB and SBDB bindings, we have a tool called <code>modelgen</code> which lives in the libovsdb repo: https://github.com/ovn-org/libovsdb#modelgen. It is a code generator that uses <code>pkg/nbdb/gen.go</code> and <code>pkg/sbdb/gen.go</code> files to auto-generate the models and additional code like deep-copy methods.</p> <p>In order to use this tool do the following: <pre><code>$ cd go-controller/\n$ make modelgen\ncurl -sSL https://raw.githubusercontent.com/ovn-org/ovn/${OVN_SCHEMA_VERSION}/ovn-nb.ovsschema -o pkg/nbdb/ovn-nb.ovsschema\ncurl -sSL https://raw.githubusercontent.com/ovn-org/ovn/${OVN_SCHEMA_VERSION}/ovn-sb.ovsschema -o pkg/sbdb/ovn-sb.ovsschema\nhack/update-modelgen.sh\n</code></pre></p> <p>If there are new bindings then you should see the changes being generated in the <code>pkg/nbdb</code> and <code>pkg/sbdb</code> parts of the repo. Include them and push a commit!</p> <p>NOTE1: You have to pay attention to the version of the commit hash used to download the modelgen client. While the client doesn't change too often it can also become outdated causing wrong generations. So keep in mind to re-install modelgen with latest commits and change the hash value in the <code>hack/update-modelgen.sh</code> file if you find it outdated.</p> <p>NOTE2: From time to time we always bump our fedora version of OVN used by KIND. But we oftentimes forget to update the <code>OVN_SCHEMA_VERSION</code> in our <code>Makefile</code> which is used to download the ovsdb schema. If that version seems to be outdated, probably best to update that as well and re-generate the schema bindings.</p>"},{"location":"developer-guide/developer/#generating-crd-yamls-using-codegen","title":"Generating CRD yamls using codegen","text":"<p>In order to generate the latest yaml files for a given CRD or to add a new CRD, once the <code>types.go</code> has been created according to sig-apimachinery docs, the developer can run <code>make codegen</code> to be able to generate all the clientgen, listers and informers for the new CRD along with the deep-copy methods and actual yaml files which get created in <code>_output/crd</code> folder and are copied over to <code>dist/templates</code> to then be used when creating a KIND cluster.</p>"},{"location":"developer-guide/mocks-ut-faq/","title":"Mocks ut faq","text":""},{"location":"developer-guide/mocks-ut-faq/#how-are-the-mock-files-for-unit-tests-organized","title":"How are the mock files for unit tests organized?","text":"<ul> <li> <p>The name of the mock file generated will be the same as the name of the <code>interface</code> definition.</p> </li> <li> <p>Mock files for <code>interfaces</code> defined in the <code>go-controller/vendor</code> directories are located in the  <code>go-controller/pkg/testing/mocks</code> directory. The directory structure in the <code>go-controller/pkg/testing/mocks/</code> closely  mimic the directory structure of  <code>go-controller/vendor/</code>.</p> <p>e.g; a) The <code>Cmd</code> interface defined in the <code>go-controller/vendor/k8s.io/utils/exec.go</code> file has its mock generated  in <code>go-controller/pkg/testing/mocks/k8s.io/utils/exec/Cmd.go</code> file</p> <p>e.g; b) The <code>Link</code> interface defined in the <code>go-controller/vendor/github.com/vishvananda/netlink/link.go</code> file has  its mock generated in <code>go-controller/pkg/testing/mocks/vishvananda/netlink/Link.go</code> file</p> </li> <li> <p>Mock files for <code>interfaces</code> defined in the non vendor directories of the project are located in the <code>mocks</code> directory  of the same package as where the interface is defined.</p> <p>e.g; a) The <code>ExecRunner</code> interface defined in <code>go-controller/pkg/util/ovs.go</code> file has the its mock generated in  <code>go-controller/pkg/util/mocks/ExecRunner.go</code> file.</p> <p>e.g; b) The <code>SriovNetLibOps</code> interface defined in <code>go-controller/pkg/cni/helper_linux.go</code> file has its mock  generated in <code>go-controller/pkg/cni/mocks/SriovNetLibOps.go</code> file.</p> </li> </ul>"},{"location":"developer-guide/mocks-ut-faq/#how-are-the-mocks-for-interfaces-to-be-consumed-by-unit-tests-currently-generated","title":"How are the mocks for interfaces to be consumed by unit tests currently generated?","text":"<ul> <li> <p>The vektra/mockery package from https://github.com/vektra/mockery is leveraged to auto generate mocks for defined interfaces.</p> </li> <li> <p>Mocks for interfaces can be generated using vektra/mockery in one of two ways:</p> <ul> <li> <p>Using the binaries at https://github.com/vektra/mockery/releases</p> </li> <li> <p>Using the docker image</p> </li> </ul> </li> <li> <p>Sample commands to generate mocks when using the <code>binary</code> installed on a linux host.</p> <ul> <li> <p>Mock for interface <code>SriovNetLibOps</code> defined in the <code>go-controller/pkg/cni/helper_linux.go</code> file when executing the <code>mockery</code> command from dir <code>go-controller/</code></p> <p><code>mockery --name SriovnetLibOps --dir pkg/cni/ --output pkg/cni/mocks</code></p> </li> <li> <p>Mock for all interfaces defined in the vendor folder <code>go-controller/vendor/k8s.io/utils/exec</code> when executing the <code>mockery</code> command from dir <code>go-controller/</code></p> <p><code>mockery --all --dir vendor/k8s.io/utils/exec --output pkg/testing/mocks/k8s.io/utils/exec</code></p> </li> </ul> </li> <li> <p>Sample command to generate mocks when using the <code>docker</code> image</p> <ul> <li> <p>Mock for interface <code>SriovNetLibOps</code> defined in the <code>go-controller/pkg/cni/helper_linux.go</code> file when running the <code>docker</code> container from dir <code>go-controller/</code></p> <p><code>docker run -v $PWD:/src -w /src vektra/mockery --name SriovNetLibOps --dir pkg/cni/ --output pkg/cni/mocks</code></p> </li> <li> <p>Mock for all interfaces defined in the vendor folder <code>go-controller/vendor/k8s.io/utils/exec</code> when running the <code>docker</code> container from dir <code>go-controller/</code></p> <p><code>docker run -v $PWD:/src -w /src vektra/mockery --all --dir vendor/k8s.io/utils/exec --output pkg/testing/mocks/k8s.io/utils/exec</code></p> </li> </ul> </li> </ul>"},{"location":"developer-guide/mocks-ut-faq/#how-to-regenerate-all-existing-mocks-when-interfaces-locally-defined-or-in-vendor-libraries-are-updated","title":"How to regenerate all existing mocks when interfaces (locally defined or in vendor libraries) are updated?","text":"<pre><code>- Execute the ```go-controller/hack/regenerate_vendor_mocks.sh``` in situations where all existing mocks have to be regenerated.\nNOTE: It would take a while(approx 20+ minutes) for all mocks to be regenerated.\n</code></pre>"},{"location":"developer-guide/mocks-ut-faq/#reference-links-that-explain-how-to-use-mocks-with-testify","title":"Reference links that explain how to use mocks with testify","text":"<ul> <li> <p>https://tutorialedge.net/golang/improving-your-tests-with-testify-go/ </p> </li> <li> <p>https://techblog.fexcofts.com/2019/09/23/go-and-test-mocking/ </p> </li> <li> <p>https://gowalker.org/github.com/stretchr/testify/mock </p> </li> <li> <p>https://ncona.com/2020/02/using-testify-for-golang-tests/ </p> </li> </ul>"},{"location":"features/hybrid-overlay/","title":"Hybrid Overlay","text":""},{"location":"features/hybrid-overlay/#introduction","title":"Introduction","text":"<p>The hybrid overlay feature creates VXLAN tunnels to nodes in the cluster that have been excluded from the ovn-kubernetes overlay using the <code>no-hostsubnet-nodes</code> config option.</p> <p>These tunnels allow pods on ovn-kubernetes nodes to communicate directly with other pods on nodes that do not run ovn-kubernetes.</p>"},{"location":"features/hybrid-overlay/#requirements","title":"Requirements","text":"<p>The feature can be enabled at runtime, but requires that the VXLAN UDP port (4789) be accessible on every node in the cluster. The cluster administrator is responsible for ensuring this port is open on all nodes.</p> <p>Hybrid overlay uses the third IP address on every node's logical switch as the gateway for traffic to hybrid overlay nodes. If the feature is enabled after the cluster has been installed, and a pod on the node is using the address, that pod will no longer work correctly until it has been killed and restarted. This is not handled automatically.</p> <p>It is recommended the hybrid overlay feature be enabled at cluster install time.</p>"},{"location":"features/live-migration/","title":"Live Migration","text":""},{"location":"features/live-migration/#introduction","title":"Introduction","text":"<p>The Live Migration feature allows kubevirt virtual machines to be live migrated while keeping the established TCP connections alive, and preserving the VM IP configuration. These two requirements provide seamless live-migration of a KubeVirt VM using OVN-Kubernetes cluster default network.</p>"},{"location":"features/live-migration/#requirements","title":"Requirements","text":"<ul> <li>KubeVirt &gt;= v1.0.0</li> <li>DHCP aware guest image (fedora family is well tested, https://quay.io/organization/containerdisks)</li> </ul>"},{"location":"features/live-migration/#limitations","title":"Limitations","text":"<ul> <li>Only KubeVirt VMs with bridge binding pod network are supported</li> <li>Single stack IPv6 is not supported</li> <li>DualSack does not configure routes for IPv6 over DHCP/autoconf</li> <li>SRIOV is not supported</li> </ul>"},{"location":"features/live-migration/#example-live-migrating-a-fedora-guest-image","title":"Example: live migrating a fedora guest image","text":"<p>Install KubeVirt following the guide here</p> <p>Create a fedora virtual machine with the annotations <code>kubevirt.io/allow-pod-bridge-network-live-migration</code>: <pre><code>cat &lt;&lt;'EOF' | kubectl create -f -\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: fedora\nspec:\n  runStrategy: Always\n  template:\n    metadata:\n      annotations:\n        # Allow KubeVirt VMs with bridge binding to be migratable\n        # also ovn-k will not configure network at pod, delegate it to DHCP\n        kubevirt.io/allow-pod-bridge-network-live-migration: \"\"\n    spec:\n      domain:\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: containerdisk\n          - disk:\n              bus: virtio\n            name: cloudinit\n          rng: {}\n        features:\n          acpi: {}\n          smm:\n            enabled: true\n        firmware:\n          bootloader:\n            efi:\n              secureBoot: true\n        resources:\n          requests:\n            memory: 1Gi\n      terminationGracePeriodSeconds: 180\n      volumes:\n      - containerDisk:\n          image: quay.io/containerdisks/fedora:38\n        name: containerdisk\n      - cloudInitNoCloud:\n          networkData: |\n            version: 2\n            ethernets:\n              eth0:\n                dhcp4: true\n          userData: |-\n            #cloud-config\n            # The default username is: fedora\n            password: fedora\n            chpasswd: { expire: False }\n        name: cloudinit\nEOF\n</code></pre></p> <p>After waiting for the VM to be ready - <code>kubectl wait vmi fedora --for=condition=Ready --timeout=5m</code> - the VM status should be as shown below - i.e. <code>kubectl get vmi fedora</code>:</p> <pre><code>kubectl get vmi fedora\nNAME     AGE     PHASE     IP            NODENAME      READY\nfedora   9m42s   Running   10.244.2.26   ovn-worker3   True\n</code></pre> <p>Login and check that the VM has receive a proper address <code>virtctl console fedora</code> <pre><code>[fedora@fedora ~]ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc fq_codel state UP group default qlen 1000\n    link/ether 0a:58:0a:f4:02:1a brd ff:ff:ff:ff:ff:ff\n    altname enp1s0\n    inet 10.244.2.26/24 brd 10.244.2.255 scope global dynamic noprefixroute eth0\n       valid_lft 3412sec preferred_lft 3412sec\n    inet6 fe80::32d2:10d4:f5ed:3064/64 scope link noprefixroute\n       valid_lft forever preferred_lft forever\n</code></pre></p> <p>Also we can check the neighbours cache to verify it later on <pre><code>[fedora@fedora ~]arp -a\n_gateway (169.254.1.1) at 0a:58:a9:fe:01:01 [ether] on eth0\n</code></pre></p> <p>Keep in mind the default gw is a link local address; that is because  the live migration feature is implemented using ARP proxy.</p> <p>The last route is needed since the link local address subnet is not bound to any interface, that route is automatically created by dhcp client.</p> <pre><code>[fedora@fedora ~]ip route\ndefault via 169.254.1.1 dev eth0 proto dhcp src 10.244.2.26 metric 100\n10.244.2.0/24 dev eth0 proto kernel scope link src 10.244.2.26 metric 100\n169.254.1.1 dev eth0 proto dhcp scope link src 10.244.2.26 metric 100\n</code></pre> <p>Then a live migration can be initialized with <code>virtctl migrate fedora</code> and wait at the vmim resource <pre><code>virtctl migrate fedora\nVM fedora was scheduled to migrate\nkubectl get vmim -A -o yaml\n  status:\n    migrationState:\n      completed: true\n</code></pre></p> <p>After migration, the network configuration is the same - including the GW neighbor cache. <pre><code>oc get vmi -A\nNAMESPACE   NAME     AGE   PHASE     IP            NODENAME     READY\ndefault     fedora   16m   Running   10.244.2.26   ovn-worker   True\n[fedora@fedora ~]ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc fq_codel state UP group default qlen 1000\n    link/ether 0a:58:0a:f4:02:1a brd ff:ff:ff:ff:ff:ff\n    altname enp1s0\n    inet 10.244.2.26/24 brd 10.244.2.255 scope global dynamic noprefixroute eth0\n       valid_lft 2397sec preferred_lft 2397sec\n    inet6 fe80::32d2:10d4:f5ed:3064/64 scope link noprefixroute\n       valid_lft forever preferred_lft forever\n[fedora@fedora ~]arp -a\n_gateway (169.254.1.1) at 0a:58:a9:fe:01:01 [ether] on eth0\n</code></pre></p>"},{"location":"features/live-migration/#configuring-dns-server","title":"Configuring dns server","text":"<p>By default the DHCP server at ovn-kuberntes will configure the kubernetes default dns service <code>kube-system/kube-dns</code> as the name server. This can be overriden with the following command line options: - dns-service-namespace - dns-service-name</p>"},{"location":"features/live-migration/#configuring-dual-stack-guest-images","title":"Configuring dual stack guest images","text":"<p>For dual stack, ovn-kubernetes is configuring the IPv6 address to guest VMs using DHCPv6, but the IPv6 default gateway has to be configured manually. Since this address is the same - <code>fe80::1</code> - the virtual machine configuration is stable. </p> <p>Both the ipv4 and ipv6 configurations have to be activated.</p> <p>NOTE: The IPv6 autoconf/SLAAC is not supported at ovn-k live-migration</p> <p>For fedora cloud-init can be used to activate dual stack: <pre><code> - cloudInitNoCloud:\n     networkData: |\n       version: 2\n       ethernets:\n         eth0:\n           dhcp4: true\n           dhcp6: true\n     userData: |-\n       #cloud-config\n       # The default username is: fedora\n       password: fedora\n       chpasswd: { expire: False }\n       runcmd:\n         - nmcli c m \"cloud-init eth0\" ipv6.method dhcp\n         - nmcli c m \"cloud-init eth0\" +ipv6.routes \"fe80::1\"\n         - nmcli c m \"cloud-init eth0\" +ipv6.routes \"::/0 fe80::1\"\n         - nmcli c reload \"cloud-init eth0\"\n</code></pre></p> <p>For fedora coreos this can be configured with using the following ignition yaml:</p> <pre><code>variant: fcos\nversion: 1.4.0\nstorage:\n  files:\n    - path: /etc/nmstate/001-dual-stack-dhcp.yml\n      contents:\n        inline: |\n          interfaces:\n          - name: enp1s0\n            type: ethernet\n            state: up\n            ipv4:\n              enabled: true\n              dhcp: true\n            ipv6:\n              enabled: true\n              dhcp: true\n              autoconf: false\n    - path: /etc/nmstate/002-dual-sack-ipv6-gw.yml\n      contents:\n        inline: |\n          routes:\n            config:\n            - destination: ::/0\n              next-hop-interface: enp1s0\n              next-hop-address: fe80::1\n</code></pre>"},{"location":"features/multi-homing/","title":"Multi-homing","text":"<p>A K8s pod with more than one network interface is said to be multi-homed. The Network Plumbing Working Group has put forward a standard describing how to specify the configurations for additional network interfaces.</p> <p>There are several delegating plugins or meta-plugins (Multus, Genie) implementing this standard.</p> <p>After a pod is scheduled on a particular Kubernetes node, kubelet will invoke the delegating plugin to prepare the pod for networking. This meta-plugin will then invoke the CNI responsible for setting up the pod's default cluster network, and afterwards it iterates the list of additional attachments on the pod, invoking the corresponding delegate CNI implementing the logic to attach the pod to that particular network.</p>"},{"location":"features/multi-homing/#configuring-secondary-networks","title":"Configuring secondary networks","text":"<p>To allow pods to have multiple network interfaces, the user must provide the configurations specifying how to connect to these networks; these configurations are defined in a CRD named <code>NetworkAttachmentDefinition</code>.</p> <p>Below you will find example attachment configurations for each of the current topologies OVN-K allows for secondary networks.</p> <p>NOTE: - networks are not namespaced - i.e. creating multiple   <code>network-attachment-definition</code>s with different configurations pointing at the   same network (same <code>NetConf.Name</code> attribute) is not supported.</p>"},{"location":"features/multi-homing/#routed-layer-3-topology","title":"Routed - layer 3 - topology","text":"<p>This topology is a simplification of the topology for the cluster default network - but without egress.</p> <p>There is a logical switch per node - each with a different subnet - and a router interconnecting all the logical switches.</p> <p>The following net-attach-def configures the attachment to a routed secondary network.</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: l3-network\n  namespace: ns1\nspec:\n  config: |2\n    {\n            \"cniVersion\": \"0.3.1\",\n            \"name\": \"l3-network\",\n            \"type\": \"ovn-k8s-cni-overlay\",\n            \"topology\":\"layer3\",\n            \"subnets\": \"10.128.0.0/16/24\",\n            \"mtu\": 1300,\n            \"netAttachDefName\": \"ns1/l3-network\"\n    }\n</code></pre>"},{"location":"features/multi-homing/#network-configuration-reference","title":"Network Configuration reference","text":"<ul> <li><code>name</code> (string, required): the name of the network. This attribute is not namespaced.</li> <li><code>type</code> (string, required): \"ovn-k8s-cni-overlay\".</li> <li><code>topology</code> (string, required): \"layer3\".</li> <li><code>subnets</code> (string, required): a comma separated list of subnets. When multiple subnets   are provided, the user will get an IP from each subnet.</li> <li><code>mtu</code> (integer, optional): explicitly set MTU to the specified value. Defaults to the value chosen by the kernel.</li> <li><code>netAttachDefName</code> (string, required): must match <code>&lt;namespace&gt;/&lt;net-attach-def name&gt;</code>   of the surrounding object.</li> </ul> <p>NOTE - the <code>subnets</code> attribute indicates both the subnet across the cluster, and per node.   The example above means you have a /16 subnet for the network, but each node has   a /24 subnet. - routed - layer3 - topology networks only allow for east/west traffic.</p>"},{"location":"features/multi-homing/#switched-layer-2-topology","title":"Switched - layer 2 - topology","text":"<p>This topology interconnects the workloads via a cluster-wide logical switch.</p> <p>The following net-attach-def configures the attachment to a layer 2 secondary network.</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: l2-network\n  namespace: ns1\nspec:\n  config: |2\n    {\n            \"cniVersion\": \"0.3.1\",\n            \"name\": \"l2-network\",\n            \"type\": \"ovn-k8s-cni-overlay\",\n            \"topology\":\"layer2\",\n            \"subnets\": \"10.100.200.0/24\",\n            \"mtu\": 1300,\n            \"netAttachDefName\": \"ns1/l2-network\",\n            \"excludeSubnets\": \"10.100.200.0/29\"\n    }\n</code></pre>"},{"location":"features/multi-homing/#network-configuration-reference_1","title":"Network Configuration reference","text":"<ul> <li><code>name</code> (string, required): the name of the network. This attribute is not namespaced.</li> <li><code>type</code> (string, required): \"ovn-k8s-cni-overlay\".</li> <li><code>topology</code> (string, required): \"layer2\".</li> <li><code>subnets</code> (string, optional): a comma separated list of subnets. When multiple subnets   are provided, the user will get an IP from each subnet.</li> <li><code>mtu</code> (integer, optional): explicitly set MTU to the specified value. Defaults to the value chosen by the kernel.</li> <li><code>netAttachDefName</code> (string, required): must match <code>&lt;namespace&gt;/&lt;net-attach-def name&gt;</code>   of the surrounding object.</li> <li><code>excludeSubnets</code> (string, optional): a comma separated list of CIDRs / IPs.   These IPs will be removed from the assignable IP pool, and never handed over   to the pods.</li> </ul> <p>NOTE - when the subnets attribute is omitted, the logical switch implementing the   network will only provide layer 2 communication, and the users must configure   IPs for the pods. Port security will only prevent MAC spoofing. - switched - layer2 - secondary networks only allow for east/west traffic.</p>"},{"location":"features/multi-homing/#switched-localnet-topology","title":"Switched - localnet - topology","text":"<p>This topology interconnects the workloads via a cluster-wide logical switch to a physical network.</p> <p>The following net-attach-def configures the attachment to a localnet secondary network.</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: localnet-network\n  namespace: ns1\nspec:\n  config: |2\n    {\n            \"cniVersion\": \"0.3.1\",\n            \"name\": \"localnet-network\",\n            \"type\": \"ovn-k8s-cni-overlay\",\n            \"topology\":\"localnet\",\n            \"subnets\": \"202.10.130.112/28\",\n            \"vlanID\": 33,\n            \"mtu\": 1500,\n            \"netAttachDefName\": \"ns1/localnet-network\"\n    }\n</code></pre> <p>Note that in order to connect to the physical network, it is expected that ovn-bridge-mappings is configured appropriately on the chassis for this localnet network.</p>"},{"location":"features/multi-homing/#network-configuration-reference_2","title":"Network Configuration reference","text":"<ul> <li><code>name</code> (string, required): the name of the network.</li> <li><code>type</code> (string, required): \"ovn-k8s-cni-overlay\".</li> <li><code>topology</code> (string, required): \"localnet\".</li> <li><code>subnets</code> (string, optional): a comma separated list of subnets. When multiple subnets   are provided, the user will get an IP from each subnet.</li> <li><code>mtu</code> (integer, optional): explicitly set MTU to the specified value. Defaults to the value chosen by the kernel.</li> <li><code>netAttachDefName</code> (string, required): must match <code>&lt;namespace&gt;/&lt;net-attach-def name&gt;</code>   of the surrounding object.</li> <li><code>excludeSubnets</code> (string, optional): a comma separated list of CIDRs / IPs.   These IPs will be removed from the assignable IP pool, and never handed over   to the pods.</li> <li><code>vlanID</code> (integer, optional): assign VLAN tag. Defaults to none.</li> </ul> <p>NOTE - when the subnets attribute is omitted, the logical switch implementing the   network will only provide layer 2 communication, and the users must configure   IPs for the pods. Port security will only prevent MAC spoofing.</p>"},{"location":"features/multi-homing/#pod-configuration","title":"Pod configuration","text":"<p>The user must specify the secondary network attachments via the <code>k8s.v1.cni.cncf.io/networks</code> annotation.</p> <p>The following example provisions a pod with two secondary attachments, one for each of the attachment configurations presented in Configuring secondary networks.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/networks: l3-network,l2-network\n  name: tinypod\n  namespace: ns1\nspec:\n  containers:\n  - args:\n    - pause\n    image: registry.k8s.io/e2e-test-images/agnhost:2.36\n    imagePullPolicy: IfNotPresent\n    name: agnhost-container\n</code></pre>"},{"location":"features/multi-homing/#setting-static-ip-addresses-on-a-pod","title":"Setting static IP addresses on a pod","text":"<p>The user can specify attachment parameters via network-selection-elements , namely IP, MAC, and interface name.</p> <p>Refer to the following yaml for an example on how to request a static IP for a pod, a MAC address, and specify the pod interface name.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/networks: '[\n      {\n        \"name\": \"l2-network\",\n        \"mac\": \"02:03:04:05:06:07\",\n        \"interface\": \"myiface1\",\n        \"ips\": [\n          \"192.0.2.20/24\"\n        ]\n      }\n    ]'\n  name: tinypod\n  namespace: ns1\nspec:\n  containers:\n  - args:\n    - pause\n    image: registry.k8s.io/e2e-test-images/agnhost:2.36\n    imagePullPolicy: IfNotPresent\n    name: agnhost-container\n</code></pre> <p>NOTE: - the user can specify the IP address for a pod's secondary attachment   only for an L2 or localnet attachment. - specifying a static IP address for the pod is only possible when the   attachment configuration does not feature subnets.</p>"},{"location":"features/multi-homing/#multi-network-policies","title":"Multi-network Policies","text":"<p>OVN-Kubernetes implements native support for multi-networkpolicy, an API providing network policy features for secondary networks.</p> <p>To configure pod isolation, the user must: - provision a <code>network-attachment-definition</code>. - provision a <code>MultiNetworkPolicy</code> indicating to which secondary networks it   applies via the   policy-for   annotation.</p> <p>NOTE: the <code>OVN_MULTI_NETWORK_ENABLE</code> config flag must be enabled.</p> <p>Please refer to the following example: <pre><code>---\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: tenant-blue\nspec:\n    config: '{\n        \"cniVersion\": \"0.4.0\",\n        \"name\": \"tenant-blue\",\n        \"netAttachDefName\": \"default/tenant-blue\",\n        \"topology\": \"layer2\",\n        \"type\": \"ovn-k8s-cni-overlay\",\n        \"subnets\": \"192.168.100.0/24\"\n    }'\n---\napiVersion: k8s.cni.cncf.io/v1beta1\nkind: MultiNetworkPolicy\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/policy-for: default/tenant-blue # indicates the net-attach-defs this policy applies to\n  name: allow-ports-same-ns\nspec:\n  podSelector:\n    matchLabels:\n      app: stuff-doer # the policy will **apply** to all pods with this label\n  ingress:\n  - ports:\n    - port: 9000\n      protocol: TCP\n    from:\n    - namespaceSelector:\n        matchLabels:\n          role: trusted # only pods on namespaces with this label will be allowed on port 9000\n  policyTypes:\n    - Ingress\n</code></pre></p> <p>Please note the <code>MultiNetworkPolicy</code> has the exact same API of the native <code>networking.k8s.io/v1</code> <code>NetworkPolicy</code>object; check its documentation for more information.</p> <p>Note: <code>net-attach-def</code>s referred to by the <code>k8s.v1.cni.cncf.io/policy-for</code> annotation without the subnet attribute defined are possible if the policy only features <code>ipBlock</code> peers. If the <code>net-attach-def</code> features the <code>subnet</code> attribute, it can also feature <code>namespaceSelectors</code> and <code>podSelectors</code>.</p>"},{"location":"features/multi-homing/#limitations","title":"Limitations","text":"<p>OVN-K currently does not support: - the same attachment configured multiple times in the same pod - i.e.   <code>k8s.v1.cni.cncf.io/networks: l3-network,l3-network</code> is invalid. - updates to the network selection elements lists - i.e. <code>k8s.v1.cni.cncf.io/networks</code> annotation</p>"},{"location":"features/multicast/","title":"Multicast","text":""},{"location":"features/multicast/#introduction","title":"Introduction","text":"<p>IP multicast enables data to be delivered to multiple IP addresses simultaneously. Multicast can distribute data one-to-many or many-to-many. For this to happen, the 'receivers' join a multicast group, and the sender(s) send data to it. In other words, multicast filtering is achieved by dynamic group control management.</p> <p>The multicast group membership is implemented with IGMP. For details, check RFCs 1112 and 2236.</p>"},{"location":"features/multicast/#configuring-multicast-on-the-cluster","title":"Configuring multicast on the cluster","text":"<p>The feature is gated by config flag. In order to create a KIND cluster with multicast feature enabled, use the <code>--multicast-enabled</code> option with KIND.</p>"},{"location":"features/multicast/#enabling-multicast-per-namespace","title":"Enabling multicast per namespace","text":"<p>The multicast traffic between pods in the cluster is blocked by default; it can be enabled per namespace - but it cannot be enabled cluster wide.</p> <p>To enable multicast support on a given namespace, you need to annotate the namespace:</p> <pre><code>$ kubectl annotate namespace &lt;namespace name&gt; \\\n    k8s.ovn.org/multicast-enabled=true\n</code></pre>"},{"location":"features/multicast/#changes-in-ovn-northbound-database","title":"Changes in OVN northbound database","text":"<p>In this section we will be seeing plenty of OVN north entities; all of it consists of an example with a single pod:</p> <pre><code># only list the pod name + IPs of the pod (in the default namespace)\n$ kubectl get pods -o=custom-columns=Name:.metadata.name,IP:.status.podIPs\nName                                 IP\nvirt-launcher-vmi-masquerade-thr9j   [map[ip:10.244.1.8]]\n</code></pre> <p>The implementation of IPv4 multicast for ovn-kubernetes relies on: - 2 ACLs (ingress/egress) dropping all multicast traffic - on all switches (via clusterPortGroup) - 2 ACLs (ingress/egress) allowing all multicast traffic - on clusterRouterPortGroup  (that allows multicast between pods that reside on different nodes, see  https://github.com/ovn-org/ovn-kubernetes/commit/3864f2b6463392ae2d80c18d06bd46ec44e639f9 for more details)</p> <p>These ACLs Matches look like:</p> <pre><code># deny all multicast match\n\"(ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\n\n# allow clusterPortGroup match ingress\n\"outport == @clusterRtrPortGroup &amp;&amp; (ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\n# allow clusterPortGroup match egress\n\"inport == @clusterRtrPortGroup &amp;&amp; (ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\n</code></pre> <p>Then, for each annotated(<code>k8s.ovn.org/multicast-enabled=true</code>) namespace, two ACLs with higher priority are provisioned; in the following example we show ACLs that apply to the <code>default</code> namespace.</p> <pre><code># egress direction\nmatch               : \"inport == @a16982411286042166782 &amp;&amp; ip4.mcast\"\n\n# ingress direction\nmatch               : \"outport == @a16982411286042166782 &amp;&amp; (igmp || (ip4.src == $a5154718082306775057 &amp;&amp; ip4.mcast))\"\n</code></pre> <p>As can be seen in the match condition of the ACLs above, the former ACL allows egress traffic for all multicast traffic whose originating ports belong to the namespace, whereas the latter allows ingress multicast traffic for ports belonging to the namespace. This last match also assures that traffic originating by pods in the same namespace are allowed.</p> <p>Both these ACLs require a port group to keep track of all ports within the namespace - <code>@a16982411286042166782</code> - while the <code>ingress</code> ACL also requires the namespace's <code>address set</code> to be up to date. Both these tables can be seen below:</p> <pre><code># port group encoding the `default` namespace\n_uuid               : dde5bbb0-0b1d-4dab-b100-0b710f46fc28\nacls                : [b930b6ea-5b16-4eb1-b962-6b3e9273d0a0, f086c9b7-fa61-4a91-b545-f228f6cf954b]\nexternal_ids        : {name=default}\nname                : a16982411286042166782\nports               : [5fefc0c6-e651-48a9-aa0d-f86197b93267]\n\n# address set encoding the `default` namespace\n_uuid               : 1349957f-68e5-4f5a-af26-01ec58d96f6b\naddresses           : [\"10.244.1.8\"]\nexternal_ids        : {name=default_v4}\nname                : a5154718082306775057\n</code></pre> <p>Note: notice the IP address on the address set encoding the default namespace matches the IP address of the pod listed on our example.</p> <p>For completeness, let's also take a look at the port referenced in the port group:</p> <p><pre><code>_uuid               : 5fefc0c6-e651-48a9-aa0d-f86197b93267\naddresses           : [\"0a:58:0a:f4:01:08 10.244.1.8\"]\ndhcpv4_options      : []\ndhcpv6_options      : []\ndynamic_addresses   : []\nenabled             : []\nexternal_ids        : {namespace=default, pod=\"true\"}\nha_chassis_group    : []\nname                : default_virt-launcher-vmi-masquerade-thr9j\noptions             : {requested-chassis=ovn-worker2}\nparent_name         : []\nport_security       : [\"0a:58:0a:f4:01:08 10.244.1.8\"]\ntag                 : []\ntag_request         : []\ntype                : \"\"\nup                  : true\n</code></pre> As you can see, this is the OVN logical switch port assigned to the pod running on the namespace we have annotated.</p>"},{"location":"features/multicast/#multicast-group-membership","title":"Multicast group membership","text":"<p>As indicated in the introduction secion, multicast filtering is achieved by dynamic group control management. The manager of these filtering lists is the switch that is configured to act as an IGMP querier - on OVN-K's case, the node's logical switches.</p> <p>For these membership requests to be allowed in the network, each node's logical switch's must declare themselves \"multicast queriers\", by using the <code>other_config:mcast_querier=true</code> option.</p> <p>The <code>other_config:mcast_snoop=true</code> is also used so the logical switches can passively snoop IGMP query / report / leave packets transferred between the multicast hosts and switches to compute group membership.</p> <p>Without these two options, multicast traffic would be treated as broadcast traffic, which forwards packets to all ports on the network.</p> <p>Please refer to the following snippet featuring the node's logical swithes of a cluster with one control plane node, and two workers, to see these options in use: <pre><code># control plane node\n_uuid               : 09d7a498-8885-4b66-9c50-da2286579382\nacls                : [ef11235e-54f7-4f7b-a19a-d6d935c836a2]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [78e40b60-458c-43b7-b1d8-344ca85c8b08, 84b6c4a7-3ec4-46d4-8611-23267a2e1d83, 9ab56c1a-00f1-4d3d-bea0-e3a2679bce11, bafdde0a-\nf303-41e2-98eb-4e149bc75c91, ce697ebf-2059-48fa-a47c-20b901a18395, ebbb52b8-ba28-4550-b03f-0865e6c7162b]\nname                : ovn-control-plane\nother_config        : {mcast_eth_src=\"0a:58:0a:f4:00:01\", mcast_ip4_src=\"10.244.0.1\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.0.0/24\"}\nports               : [08273d87-50f9-461f-9421-4df0360a624b, 3ab00f31-a2dc-4257-8882-5e73913c55d3, 3d4ce3c5-407f-4fa2-bbb6-fe9107d9ddc2]\nqos_rules           : []\n\n# ovn-worker\n_uuid               : cb85ea2b-309b-43fa-8fc8-db97353e872c\nacls                : [8d9f3900-1cc2-433b-8a1b-c9536eac8575]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [17cb0b5e-840c-4aad-abc5-d0dd9d67a1e7, 2a84133f-0675-4f40-bc47-78608886b848, 38455c9c-5c03-4777-8f35-3d71b3af0487, 84b6c4a7-3ec4-46d4-8611-23267a2e1d83, 9ab56c1a-00f1-4d3d-bea0-e3a2679bce11, ebbb52b8-ba28-4550-b03f-0865e6c7162b]\nname                : ovn-worker\nother_config        : {mcast_eth_src=\"0a:58:0a:f4:02:01\", mcast_ip4_src=\"10.244.2.1\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.2.0/24\"}\nports               : [199d3b11-3eba-41b6-947b-22db7f9ee529, 4202a225-83b4-46ff-ba87-624af78d2b16, 84cf40d2-7dc2-477c-aff8-bdf2e0fc60d7, b7415c11-2840-4abb-8965-3abf073aa26b, c716aa8a-7178-4d52-9b7f-174c22084e4e, c7d9ef31-4ec6-450e-8299-cc4d715ce9ea, dfa10e32-5613-4297-b635-75f4db26f4e7, ef13e922-a097-460f-94a2-e1f08b16fbeb, fd8e6b71-ea0d-4a44-a18b-824aa5ff6bc0]\nqos_rules           : []\n\n# ovn-worker 2\n_uuid               : b9255452-3e7a-4b84-ab64-11154432eb08\nacls                : [208907e0-55b1-4b6d-a1d8-985148ba6b29]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [32091c8e-394d-46a8-a65e-10b0f1d4aecc, 36f5827e-e027-4740-a908-d6e277b5ad55, 535c0e56-9762-4890-931b-50c4062aa673, 84b6c4a7-3ec4-46d4-8611-23267a2e1d83, 9ab56c1a-00f1-4d3d-bea0-e3a2679bce11, ebbb52b8-ba28-4550-b03f-0865e6c7162b]\nname                : ovn-worker2\nother_config        : {mcast_eth_src=\"0a:58:0a:f4:01:01\", mcast_ip4_src=\"10.244.1.1\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.1.0/24\"}\nports               : [20e826fb-0298-4f45-8621-5af6fe7c3bd1, 49938955-f3b8-4363-ab40-acd826cd977c, 5fefc0c6-e651-48a9-aa0d-f86197b93267, 7323dbf3-f8a8-4858-8a00-5b7987e97648, 79463ad7-a50d-4a05-8e15-d695713f6bb3, dddf772b-6925-4fa7-9ea5-1e4fd1267cb3, f584c3b2-5370-45c6-912b-56be00168c98]\nqos_rules           : []\n</code></pre></p> <p>Note: it is important to note that the source IP / MAC address of the multicast queries sent by the logical switch are the addresses of the logical router ports connecting each node's switch to the cluster's router, as can be seen below.</p> <pre><code># ovn-control-plane logical router port\n_uuid               : e3e21af2-0ec5-4993-bb0c-e052b3f3eeb7\nenabled             : []\nexternal_ids        : {}\ngateway_chassis     : []\nha_chassis_group    : []\nipv6_prefix         : []\nipv6_ra_configs     : {}\nmac                 : \"0a:58:0a:f4:00:01\"\nname                : rtos-ovn-control-plane\nnetworks            : [\"10.244.0.1/24\"]\noptions             : {}\npeer                : []\n\n# ovn-worker logical router port\n_uuid               : 28be35a4-26cf-4daf-b922-c6aa5cecf58b\nenabled             : []\nexternal_ids        : {}\ngateway_chassis     : []\nha_chassis_group    : []\nipv6_prefix         : []\nipv6_ra_configs     : {}\nmac                 : \"0a:58:0a:f4:02:01\"\nname                : rtos-ovn-worker\nnetworks            : [\"10.244.2.1/24\"]\noptions             : {}\npeer                : []\n\n# ovn-worker2 logical router port\n_uuid               : 6bcbca4e-572f-4109-a71e-862292f463b2\nenabled             : []\nexternal_ids        : {}\ngateway_chassis     : []\nha_chassis_group    : []\nipv6_prefix         : []\nipv6_ra_configs     : {}\nmac                 : \"0a:58:0a:f4:01:01\"\nname                : rtos-ovn-worker2\nnetworks            : [\"10.244.1.1/24\"]\noptions             : {}\npeer                : []\n</code></pre> <p>Finally, to enable the multicast group to span across multiple OVN nodes, we need to enable multicast relay on the cluster router. That is done via the <code>option:mcast_relay=true</code> option. Please check below for a real OVN-K deployment example:</p> <pre><code>_uuid               : acb0f1ab-2b40-463b-b5ce-fbdd183f0f44\nenabled             : []\nexternal_ids        : {k8s-cluster-router=yes, k8s-ovn-topo-version=\"4\"}\nload_balancer       : []\nname                : ovn_cluster_router\nnat                 : [8002667a-f4a1-4cbb-8830-9286f3636791, a74f337f-6a8d-4cd6-a32d-df49560aa224, c38d2424-0e13-40a8-a67f-3317b9bdbbdd]\noptions             : {mcast_relay=\"true\"}\npolicies            : [1ace3e52-8be3-49fa-86ae-e910ffbe4dd3, 1afd7fa9-c32d-4240-84dd-da48115e95c8, 2274b1f0-e8c3-495d-9c9d-a5f14f517920, 24f00c8a-7df5-4041-986b-5be79a605807, 2d7df894-ea13-4819-9614-04c814f34c94, 3a20641e-29f3-4577-aaa5-12a5ae8a56fa, 521edd9a-bc5d-4ca7-bf13-7435c5f1fbce, 8d5ee2c5-65b3-478a-a034-a6624231b6ec, b024d223-56eb-462e-9e27-1b8b4ffcec08, b4999288-7af0-4b1f-bcd7-94974562e5b0, b4ac21ed-530f-45ac-bff4-fc09fcfedd25, d4c434df-9ddd-47ab-8016-062f42d3102b, ec342a8c-a39a-4ea5-ae13-b39d17760a4e]\nports               : [28be35a4-26cf-4daf-b922-c6aa5cecf58b, 3f5b669e-6c6c-46b0-a029-c6198d47706d, 6bcbca4e-572f-4109-a71e-862292f463b2, e3e21af2-0ec5-4993-bb0c-e052b3f3eeb7, f39f5210-8ec6-4d0b-89ef-8397599cc8cf]\nstatic_routes       : [3d9a8a37-368a-43ca-9c62-80cdae843b77, 53cfa8f0-a10e-45aa-9a9f-8e9b4910315b, 6f992b50-5c52-4caf-a146-ba5ca45d7d6a, ae5f8b78-3253-47b1-818d-13f07f42dd48, b65fcc82-1015-40dd-99f4-5b98e7514fe0, de705ce6-3a28-42ac-b3bb-fdba55b020a5]\n</code></pre>"},{"location":"features/multicast/#ipv6-considerations","title":"IPv6 considerations","text":"<p>There are some changes when the cluster is configured to also assign IPv6 addresses to the pods, starting with the <code>allow</code> ACLs, which now also account for the IPv6 addresses:</p> <pre><code>_uuid               : 67ed1d4d-c81e-4553-a232-f0448798462e\naction              : allow\ndirection           : to-lport\nexternal_ids        : {default-deny-policy-type=Ingress}\nlog                 : false\nmatch               : \"outport == @a16982411286042166782 &amp;&amp; ((igmp || (ip4.src == $a5154718082306775057 &amp;&amp; ip4.mcast)) || (mldv1 || mldv2 || (ip6.src == $a5154715883283518635 &amp;&amp; (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))))\"\nmeter               : acl-logging\nname                : default_MulticastAllowIngress\npriority            : 1012\nseverity            : info\n\n_uuid               : bda7c475-613e-4dcf-8e88-024ef70b030d\naction              : allow\ndirection           : from-lport\nexternal_ids        : {default-deny-policy-type=Egress}\nlog                 : false\nmatch               : \"inport == @a16982411286042166782 &amp;&amp; (ip4.mcast || (mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1)))\"\nmeter               : acl-logging\nname                : default_MulticastAllowEgress\npriority            : 1012\nseverity            : info\n</code></pre> <p>Please note that in fact there is an address set per IP family per namespace ; this means the <code>default</code>  namespaces has two distinct address sets: one for IPv4, another for IPv6:</p> <pre><code>_uuid               : 2b48e408-5058-470b-8c0f-59d839d6a80f\naddresses           : [\"10.244.2.9\"]\nexternal_ids        : {name=default_v4}\nname                : a5154718082306775057\n\n_uuid               : 37076cff-9560-47c2-bbaa-312ff5e1a114\naddresses           : [\"fd00:10:244:3::9\"]\nexternal_ids        : {name=default_v6}\nname                : a5154715883283518635\n</code></pre> <p>Finally, it is also important to refer we need to specify the <code>mcast_ip6_src</code> option on each node's logical switch, also using the IP address of each node's logical router port:</p> <pre><code>_uuid               : e1fd9e60-831a-4ca9-ad4c-6a5bdfc018f8\nacls                : [0403fbb5-6633-46f1-8481-b329c1ccd916, f49c2de7-5f9d-42bf-a2dc-5f10b53a8347]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [142ed4bf-1422-4e9a-a69f-9d07aa67abb8, 6df77979-f829-465f-9617-abc479945261, a9395376-9e96-4b1e-ac2d-8b03947db6cc, bd8a8f3a-4872-49d2-8d14-cd83c48ca265, f05994f7-3f21-457e-aa2b-896a86da319e, fa55f0a8-e3af-4467-8444-a54acfffef6d]\nname                : ovn-control-plane\nother_config        : {ipv6_prefix=\"fd00:10:244:1::\", mcast_eth_src=\"0a:58:0a:f4:00:01\", mcast_ip4_src=\"10.244.0.1\", mcast_ip6_src=\"fe80::858:aff:fef4:1\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.0.0/24\"}\nports               : [abea774f-d989-402a-a2af-07c066b130db, c6c6468f-6459-4072-8c98-e02bf08fb377, ecceb049-8afd-40ea-bd6d-77eab32c3302]\nqos_rules           : []\n\n_uuid               : 6b7de40a-618a-4d5a-b8b8-58e1f964ee71\nacls                : [42018b5b-31e2-4733-bcde-c6ea470836f2, d980fb54-3178-468a-981b-b43e5efa1d48]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [109b7d66-2a59-4945-b067-1e51548a6f30, 6727e44b-6256-47b7-8146-0dc9561ce270, 6df77979-f829-465f-9617-abc479945261, 7e59e9b1-a2db-4005-97f3-c1b99131126f, a9395376-9e96-4b1e-ac2d-8b03947db6cc, bd8a8f3a-4872-49d2-8d14-cd83c48ca265]\nname                : ovn-worker\nother_config        : {ipv6_prefix=\"fd00:10:244:2::\", mcast_eth_src=\"0a:58:0a:f4:01:01\", mcast_ip4_src=\"10.244.1.1\", mcast_ip6_src=\"fe80::858:aff:fef4:101\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.1.0/24\"}\nports               : [0a9f14e7-94f0-49f2-b5df-acb123ace867, 53251da6-1c21-4e81-97f6-a9b41c64d71a, 6b5deecb-7536-4eeb-a865-2b8ca17be1fc, 8d56ad80-e45c-43b7-9f20-91d15bfc8836, a3252b54-7c2d-48e0-b2b2-36d121ca7292, cc0c791d-c6ee-4381-b8a4-91462039bfd1, f261f485-b974-4448-9751-62e883d677f2, fe77f44c-51ee-45f7-a7a2-4e03862803de]\nqos_rules           : []\n\n_uuid               : 3b4e3a69-8439-4124-a028-cb2851d80da6\nacls                : [607abd66-a12a-4387-8b3f-66c4ba4cb9a8, 9963f94c-cde0-41a1-b0cc-c3e6f5800e67]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [26a7f6a7-9a5c-4023-ac55-10a860237a6d, 2fa35fce-a63a-4185-ae49-b9c09709fdea, 6df77979-f829-465f-9617-abc479945261, a9395376-9e96-4b1e-ac2d-8b03947db6cc, bd8a8f3a-4872-49d2-8d14-cd83c48ca265, c3de2c11-8cd3-4d6f-9373-7a1b54e1366a]\nname                : ovn-worker2\nother_config        : {ipv6_prefix=\"fd00:10:244:3::\", mcast_eth_src=\"0a:58:0a:f4:02:01\", mcast_ip4_src=\"10.244.2.1\", mcast_ip6_src=\"fe80::858:aff:fef4:201\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.2.0/24\"}\nports               : [1b40d04a-26d2-4881-ac55-6c2a5fa679a9, 35316f83-79c3-4a23-9fdd-04cefd963f54, 39c240ef-3303-4fa4-8b5c-b860d69d7c11, 44ddb1ab-56c3-4665-8f1e-02505553a950, 728221da-fe1f-41cc-90ff-9e2a8eb8fb6d, ac22f113-7e40-4657-ad7e-4444a39bfd45, bd115b86-bdc1-4427-9cc7-0ece2d9269c6, f2a89d6a-b6c8-491c-b2b1-398d37c5aa4c]\nqos_rules           : []\n</code></pre>"},{"location":"features/multicast/#sources","title":"Sources","text":"<ul> <li>PR introducing multicast into OVN-K</li> <li>PR introducing IPv6 multicast support into OVN-K</li> <li>Dumitru Ceara's presentation about IGMP snooping / relay</li> </ul>"},{"location":"features/cluster-egress-controls/egress-ip/","title":"EgressIP","text":""},{"location":"features/cluster-egress-controls/egress-ip/#introduction","title":"Introduction","text":"<p>The Egress IP feature enables a cluster administrator to ensure that the traffic from one or more pods in one or more namespaces has a consistent source IP address for services outside the cluster network. East-West traffic (including pod -&gt; node IP) is excluded from Egress IP.  </p> <p>For more info, consider looking at the following links: - Egress IP CRD - Assigning an egress IP address - Managing Egress IP in OpenShift 4 with OVN Kubernetes</p>"},{"location":"features/cluster-egress-controls/egress-ip/#example","title":"Example","text":"<p>An example of EgressIP might look like this:</p> <p><pre><code>apiVersion: k8s.ovn.org/v1\nkind: EgressIP\nmetadata:\n  name: egressip-prod\nspec:\n  egressIPs:\n    - 172.18.0.33\n    - 172.18.0.44\n  namespaceSelector:\n    matchExpressions:\n      - key: environment\n        operator: NotIn\n        values:\n          - development\n  podSelector:\n    matchLabels:\n      app: web\n</code></pre> It specifies to use <code>172.18.0.33</code> or <code>172.18.0.44</code> egressIP for pods that are labeled with <code>app: web</code> that run in a namespace without <code>environment: development</code> label. Both selectors use the generic kubernetes label selectors.</p>"},{"location":"features/cluster-egress-controls/egress-ip/#traffic-flows","title":"Traffic flows","text":"<p>If the Egress IP(s) are hosted on the OVN primary network then the implementation is redirecting the POD traffic to an egress node where it is SNATed and sent out.  </p> <p>Using the example EgressIP and a matching pod with <code>10.244.1.3</code> IP, the following logical router policies are configured in <code>ovn_cluster_router</code>: <pre><code>Routing Policies\n  1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip4.dst == 172.18.0.4 /* ovn-control-plane */                            reroute  10.244.0.2\n  1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip4.dst == 172.18.0.2 /* ovn-worker */                                          reroute  10.244.1.2\n  1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip4.dst == 172.18.0.3 /* ovn-worker2 */                                        reroute  10.244.2.2\n\n   102 (ip4.src == $a12749576804119081385 || ip4.src == $a16335301576733828072) &amp;&amp; ip4.dst == $a11079093880111560446  allow    pkt_mark=1008\n   102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 10.244.0.0/16                                                           allow\n   102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 100.64.0.0/16                                                           allow\n\n   100 ip4.src == 10.244.1.3                                                                                          reroute  100.64.0.3, 100.64.0.4\n</code></pre> - Rules with <code>1004</code> priority are responsible for redirecting <code>pod -&gt; local host IP</code> traffic. - Rules with <code>102</code> priority are added by OVN-Kubernetes when EgressIP feature is enabled, they ensure that east-west traffic is not using egress IPs. - The rule with <code>100</code> priority is added for the pod matching <code>egressip-prod</code> EgressIP, and it redirects the traffic to one of the egress nodes (ECMP is used to balance the traffic between next hops).</p> <p>Once the redirected traffic reaches one of the egress nodes it gets SNATed in the gateway router: <pre><code>ovn-nbctl lr-nat-list GR_ovn-worker\nTYPE             GATEWAY_PORT          EXTERNAL_IP        EXTERNAL_PORT    LOGICAL_IP          EXTERNAL_MAC         LOGICAL_PORT\n...\nsnat                                   172.18.0.33                         10.244.1.3\novn-nbctl lr-nat-list  GR_ovn-worker2\nTYPE             GATEWAY_PORT          EXTERNAL_IP        EXTERNAL_PORT    LOGICAL_IP          EXTERNAL_MAC         LOGICAL_PORT\n...\nsnat                                   172.18.0.44                         10.244.1.3\n</code></pre></p> <p>Lets now imagine the Egress IP(s) mentioned previously, are not hosted by the OVN primary network and is hosted by a secondary host network which is assigned to a standard linux interface, a redirect to the egress-able node management port IP address: <pre><code>Routing Policies\n  1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip4.dst == 172.18.0.4 /* ovn-control-plane */                            reroute  10.244.0.2\n  1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip4.dst == 172.18.0.2 /* ovn-worker */                                          reroute  10.244.1.2\n  1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip4.dst == 172.18.0.3 /* ovn-worker2 */                                        reroute  10.244.2.2\n\n   102 (ip4.src == $a12749576804119081385 || ip4.src == $a16335301576733828072) &amp;&amp; ip4.dst == $a11079093880111560446  allow    pkt_mark=1008\n   102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 10.244.0.0/16                                                           allow\n   102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 100.64.0.0/16                                                           allow\n\n   100 ip4.src == 10.244.1.3                                                                                          reroute  10.244.1.2, 10.244.2.2\n</code></pre></p> <p>IPTables will have the following chain in NAT table and also rules within that chain to source NAT to the correct IP address: <pre><code>sh-5.2# iptables-save \n# Generated by iptables-save v1.8.7 on Tue Jul 25 13:09:39 2023\n*mangle\n:PREROUTING ACCEPT [14087:9430205]\n:INPUT ACCEPT [13923:9397241]\n:FORWARD ACCEPT [0:0]\n:OUTPUT ACCEPT [11270:1030982]\n...\n:KUBE-POSTROUTING - [0:0]\n:OVN-KUBE-EGRESS-IP-Multi-NIC - [0:0]\n:OVN-KUBE-EGRESS-SVC - [0:0]\n...\n-A POSTROUTING -j OVN-KUBE-EGRESS-IP-MULTI-NIC\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n-A POSTROUTING -o ovn-k8s-mp0 -j OVN-KUBE-SNAT-MGMTPORT\n-A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING\n-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000\n-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -j MASQUERADE --random-fully\n...\n-A OVN-KUBE-EGRESS-IP-MULTI-NIC -s 10.244.2.3/32 -o dummy -j SNAT --to-source 10.10.10.100\n...\n-A OVN-KUBE-EGRESS-SVC -m mark --mark 0x3f0 -m comment --comment DoNotSNAT -j RETURN\n-A OVN-KUBE-SNAT-MGMTPORT -o ovn-k8s-mp0 -m comment --comment \"OVN SNAT to Management Port\" -j SNAT --to-source 10.244.2.2\nCOMMIT\n...\n</code></pre></p> <p>IPRoute2 rules will look like the following - note rule with priority <code>6000</code> and also the table <code>1111</code>: <pre><code>sh-5.2# ip rule\n0:  from all lookup local\n30: from all fwmark 0x1745ec lookup 7\n6000:   from 10.244.2.3 lookup 1111\n32766:  from all lookup main\n32767:  from all lookup default\n</code></pre></p> <p>And the default route in the correct table <code>1111</code>: <pre><code>sh-5.2# ip route show table 1111\ndefault dev dummy\n</code></pre></p> <p>No NAT is required on the OVN primary network gateway router. OVN-Kubernetes (ovnkube-node) takes care of adding a rule to the rule table with src IP of the pod and routed towards a new routing table specifically created to route the traffic out the correct interface. IPTables rules are also altered and an entry is created within the chain <code>OVN-KUBE-EGRESS-IP-Multi-NIC</code> for each selected pod to allow SNAT to occur when egress-ing a particular interface. The routing table number <code>1111</code> is generated from the interface name. Routes within the main routing table who's output interface share the same interface used for Egress IP are also cloned into the VRF 1111.</p>"},{"location":"features/cluster-egress-controls/egress-ip/#pod-to-node-ip-traffic","title":"Pod to node IP traffic","text":"<p>When a cluster networked pod matched by an egress IP tries to connect to a non-local node IP it hits the following logical router policy in <code>ovn_cluster_router</code>: <pre><code># $&lt;all_eip_pod_ips&gt; - address-set of all pod IPs matched by any EgressIP\n# $&lt;all_esvc_pod_ips&gt; - address-set of all pod IPs matched by any EgressService\n# $&lt;all_node_ips&gt; - address-set of all node IPs in the cluster\n102 (ip4.src == $&lt;all_eip_pod_ips&gt; || ip4.src == $&lt;all_esvc_pod_ips&gt;) &amp;&amp; ip4.dst == $&lt;all_node_ips&gt;  allow    pkt_mark=1008\n</code></pre> In addition to simply allowing the <code>pod -&gt; node IP</code> traffic so that EgressIP reroute policies  are not matched upon, it is also marked with the 1008 mark. If a pod is hosted on an egressNode the traffic will first get SNATed to the egress IP, and then it will hit  following flow on breth0 that will SNAT the traffic to local node IP: <pre><code># output truncated, 0x3f0 == 1008\npriority=105,pkt_mark=0x3f0,ip,in_port=2 actions=ct(commit,zone=64000,nat(src=&lt;NodeIP&gt;),exec(load:0x1-&gt;NXM_NX_CT_MARK[])),output:1\n</code></pre> This is required to make <code>pod -&gt; node IP</code> traffic behave the same regardless of where the pod is hosted. Implementation details: https://github.com/ovn-org/ovn-kubernetes/commit/e2c981a42a28e6213d9daf3b4489c18dc2b84b19.</p> <p>For local gateway mode, in which an Egress IP is assigned to a non-primary interface, an IP rule is added to send packets to the main routing table at a priority higher than that of EgressIP IP rules, which are set to priority <code>6000</code>: <pre><code>5999:   from all fwmark 0x3f0 lookup main\n</code></pre> Note: <code>0x3f0</code> is <code>1008</code> in hexadecimal. Lower IP rule priority number indicates higher precedence versus higher IP rule priority number.</p> <p>This ensures all traffic to node IPs will not be selected by EgressIP IP rules. However, reply traffic will not have the mark <code>1008</code> and would be dropped by reverse path filtering, therefore we add an IPTable rule to the mangle table to save and restore the <code>1008</code> mark: <pre><code>sh-5.2# iptables -t mangle -L  PREROUTING\nChain PREROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nCONNMARK   all  --  anywhere             anywhere             mark match 0x3f0 CONNMARK save\nCONNMARK   all  --  anywhere             anywhere             mark match 0x0 CONNMARK restore\n</code></pre></p>"},{"location":"features/cluster-egress-controls/egress-ip/#dealing-with-non-snated-traffic","title":"Dealing with non SNATed traffic","text":"<p>Egress IP is often configured on a node different from the one hosting the affected pods. Due to the fact that ovn-controllers on different nodes apply the changes independently, there is a chance that some pod traffic will reach the egress node before it configures the SNAT rules. The following flows are added on breth0 to address this scenario: <pre><code># Commit connections from local pods so they are not affected by the drop rule below, this is required for ICNIv2\npriority=109,ip,in_port=2,nw_src=&lt;nodeSubnet&gt; actions=ct(commit,zone=64000,exec(set_field:0x1-&gt;ct_mark)),output:1\n\n# Drop non SNATed egress traffic coming from non-local pods\npriority=104,ip,in_port=2,nw_src=&lt;clusterSubnet&gt; actions=drop\n\n# Commit connections coming from IPs not in cluster network\npriority=100,ip,in_port=2 actions=ct(commit,zone=64000,exec(set_field:0x1-&gt;ct_mark)),output:1\n</code></pre></p>"},{"location":"features/cluster-egress-controls/egress-ip/#special-considerations-for-egress-ips-hosted-by-standard-linux-interfaces","title":"Special considerations for Egress IPs hosted by standard linux interfaces","text":"<p>If you wish to assign an Egress IP to a standard linux interface (non OVS type), then the following is required: * Link is up * IP address must have scope universe / global * Links and their addresses must not be removed during runtime after an egress IP is assigned to it. If you wish to remove the link, first remove the Egress IP and then remove the address / link. * IP forwarding must be enabled for the link</p>"},{"location":"features/cluster-egress-controls/egress-ip/#egress-nodes","title":"Egress Nodes","text":"<p>In order to select which node(s) may be used as egress, the following label must be added to the <code>node</code> resource:</p> <pre><code>kubectl label nodes &lt;node_name&gt; k8s.ovn.org/egress-assignable=\"\"\n</code></pre>"},{"location":"features/cluster-egress-controls/egress-ip/#egress-ip-reachability","title":"Egress IP reachability","text":"<p>Once a node has been labeled with <code>k8s.ovn.org/egress-assignable</code>, the EgressIP operator in the leader ovnkube-master pod will periodically check if that node is usable. EgressIPs assigned to a node that is no longer reachable will get revalidated and moved to another useable node.</p> <p>Egress nodes normally have multiple IP addresses. For sake of Egress IP reachability, the management (aka internal SDN) addresses of the node are the ones used. In deployments of ovn-kubernetes this is known to be the <code>ovn-k8s-mp0</code> interface of a node.</p> <p>Even though the periodic checking of egress nodes is hard coded to trigger every 5 seconds, there are attributes that the user can set:</p> <ul> <li>egressIPTotalTimeout</li> <li>gRPC vs. DISCARD port</li> </ul>"},{"location":"features/cluster-egress-controls/egress-ip/#egressiptotaltimeout","title":"egressIPTotalTimeout","text":"<p>This attribute specifies the maximum amount of time, in seconds, that the egressIP operator will wait until it declares the node unreachable. The default value is 1 second.</p> <p>This value can be set in the following ways: - ovnkube binary flag: <code>--egressip-reachability-total-timeout=&lt;TIMEOUT&gt;</code> - inside config specified by <code>--config-file</code> flag: <pre><code>[ovnkubernetesfeature]\negressip-reachability-total-timeout=123\n</code></pre></p> <p>Note: Using value <code>0</code> will skip reachability. Use this to assume that egress nodes are available.</p>"},{"location":"features/cluster-egress-controls/egress-ip/#grpc-vs-discard-port","title":"gRPC vs. DISCARD port","text":"<p>Up until recently, the only method available for determining if an egress node was reachable relied on the <code>TCP port unreachable</code> icmp response from the probed node. The TCP port 9 (aka DISCARD) is the port used for that.</p> <p>Later implementation of ovn-kubernetes is capable of leveraging secure gRPC sessions in order to probe nodes. That requires the <code>ovnkube node</code> pods to be listening on a pre-specified TCP port, in addition to configuring the <code>ovnkube master</code> pod(s).</p> <p>This value can be set in the following ways: - ovnkube binary flag: <code>--egressip-node-healthcheck-port=&lt;TCP_PORT&gt;</code> - inside config specified by <code>--config-file</code> flag: <pre><code>[ovnkubernetesfeature]\negressip-node-healthcheck-port=9107\n</code></pre></p> <p>Note: If not specifying a value, or using <code>0</code> as the <code>egressip-node-healthcheck-port</code> will make Egress IP reachability probe the egress nodes using the DISCARD port method. Unlike egressip-reachability-total-timeout, it is important that both node and master pods of ovnkube get configured with the same value!</p>"},{"location":"features/cluster-egress-controls/egress-ip/#additional-details-on-the-implementation-of-the-grpc-probing","title":"Additional details on the implementation of the gRPC probing:","text":"<ul> <li>If available, the session uses the same TLS certs used by ovnkube to connect to the northbound OVSDB server. Conversely, an insecure gRPC session is used when no certs are specified.</li> <li>The message used for probing is the standard service health specified in gRPC.</li> <li>Special care was taken into consideration to handle cases when the gRPC session bounced for normal reasons. EgressIP implementation will not declare a node unreachable under these circumstances.</li> </ul>"},{"location":"features/cluster-egress-controls/egress-qos/","title":"EgressQoS","text":""},{"location":"features/cluster-egress-controls/egress-qos/#introduction","title":"Introduction","text":"<p>The EgressQoS feature enables marking pods egress traffic with a valid QoS Differentiated Services Code Point (DSCP) value. The QoS markings will be consumed and acted upon by network appliances outside of the Kubernetes cluster to optimize traffic flow throughout their networks.</p> <p>The EgressQoS resource is namespaced-scoped and allows specifying a set of QoS rules - each has a DSCP value, an optional destination CIDR (dstCIDR) and an optional PodSelector (podSelector). A rule applies its DSCP marking to traffic coming from pods whose labels match the podSelector heading to the dstCIDR. A namespace supports having only one EgressQoS resource named <code>default</code> (other EgressQoSes will be ignored).</p>"},{"location":"features/cluster-egress-controls/egress-qos/#example","title":"Example","text":"<pre><code>kind: EgressQoS\napiVersion: k8s.ovn.org/v1\nmetadata:\n  name: default\n  namespace: default\nspec:\n  egress:\n  - dscp: 30\n    dstCIDR: 1.2.3.0/24\n  - dscp: 42\n    podSelector:\n      matchLabels:\n        app: example\n  - dscp: 28\n</code></pre> <p>This example marks the packets originating from pods in the <code>default</code> namespace in the following way: * All traffic heading to an address that belongs to 1.2.3.0/24 is marked with DSCP 30. * Egress traffic from pods labeled <code>app: example</code> is marked with DSCP 42. * All egress traffic is marked with DSCP 28.</p> <p>The priority of a rule is determined by its placement in the egress array. An earlier rule is processed before a later rule. In this example, if the rules are reversed all traffic originating from pods in the <code>default</code> namespace is marked with DSCP 28 - regardless of its destination or pods labels. Because of that specific rules should always come before general ones in that array.</p>"},{"location":"features/cluster-egress-controls/egress-qos/#changes-in-ovn-northbound-database","title":"Changes in OVN northbound database","text":"<p>EgressQoS is implemented by reacting to events from <code>EgressQoSes</code>, <code>Pods</code> and <code>Nodes</code> changes - updating OVN's northbound database <code>QoS</code>, <code>Address_Set</code> and <code>Logical_Switch</code> objects. The code is implemented under <code>pkg/ovn/egressqos.go</code> and most of the logic for the events sits under the <code>syncEgressQoS</code>, <code>syncEgressQoSPod</code>, <code>syncEgressQoSNode</code> functions.</p> <p>We'll use the example YAML above and see how the related objects are changed in OVN's northbound database in a Dual-Stack kind cluster.</p> <pre><code>$ kubectl get nodes\nNAME                STATUS   ROLES               \novn-control-plane   Ready    control-plane,master\novn-worker          Ready    &lt;none&gt;              \novn-worker2         Ready    &lt;none&gt;              \n</code></pre> <pre><code>$ kubectl get pods -o=custom-columns=Name:.metadata.name,IPs:.status.podIPs,LABELS:.metadata.labels\nName           IPs                                             LABELS\nno-labels      [map[ip:10.244.1.3] map[ip:fd00:10:244:2::3]]   &lt;none&gt;\nwith-labels1   [map[ip:10.244.2.3] map[ip:fd00:10:244:3::3]]   map[app:example]\n</code></pre> <pre><code>$ kubectl get egressqoses\nNo resources found in default namespace.\n</code></pre> <p>At this point there are no QoS objects in the northbound database. We create the example EgressQoS: <pre><code>$ kubectl get egressqoses\nNAME      AGE\ndefault   6s\n</code></pre></p> <p>And the following is created in the northbound database (<code>SyncEgressQoS</code>): <pre><code># QoS\n\n_uuid               : 14b923a1-d7b0-42b8-a3d7-6a5028b09ae2\naction              : {dscp=30}\nbandwidth           : {}\ndirection           : to-lport\nexternal_ids        : {EgressQoS=default}\nmatch               : \"(ip4.dst == 1.2.3.0/24) &amp;&amp; (ip4.src == $a5154718082306775057 || ip6.src == $a5154715883283518635)\"\npriority            : 1000\n\n_uuid               : 820a011d-0eda-43b7-994d-46a55620c4bf\naction              : {dscp=42}\nbandwidth           : {}\ndirection           : to-lport\nexternal_ids        : {EgressQoS=default}\nmatch               : \"(ip4.dst == 0.0.0.0/0 || ip6.dst == ::/0) &amp;&amp; (ip4.src == $a10759091379580272948 || ip6.src == $a10759093578603529370)\"\npriority            : 999\n\n_uuid               : 1e35ea19-3353-4cbc-a1f5-7ea5bf831d67\naction              : {dscp=28}\nbandwidth           : {}\ndirection           : to-lport\nexternal_ids        : {EgressQoS=default}\nmatch               : \"(ip4.dst == 0.0.0.0/0 || ip6.dst == ::/0) &amp;&amp; (ip4.src == $a5154718082306775057 || ip6.src == $a5154715883283518635)\"\npriority            : 998\n</code></pre></p> <p>A QoS object is created for each rule specified in the EgressQoS, all attached to all of the nodes logical switches: <pre><code># Logical_Switch\n\nname                : ovn-worker\nqos_rules           : [14b923a1-d7b0-42b8-a3d7-6a5028b09ae2, 1e35ea19-3353-4cbc-a1f5-7ea5bf831d67, 820a011d-0eda-43b7-994d-46a55620c4bf]\n\nname                : ovn-worker2\nqos_rules           : [14b923a1-d7b0-42b8-a3d7-6a5028b09ae2, 1e35ea19-3353-4cbc-a1f5-7ea5bf831d67, 820a011d-0eda-43b7-994d-46a55620c4bf]\n\nname                : ovn-control-plane\nqos_rules           : [14b923a1-d7b0-42b8-a3d7-6a5028b09ae2, 1e35ea19-3353-4cbc-a1f5-7ea5bf831d67, 820a011d-0eda-43b7-994d-46a55620c4bf]\n</code></pre></p> <p>When a new node is added to the cluster we attach all of the <code>QoS</code> objects that belong to EgressQoSes to its logical switch as well (<code>SyncEgressQoSNode</code>).</p> <p>Notice that QoS objects that belong to rules without a <code>podSelector</code> reference the same address sets for their src match - which are the namespace's address sets: <pre><code># Address_Set\n\n_uuid               : 7cb04096-7107-4a35-ae34-be4b0e0c584e\naddresses           : [\"10.244.1.3\", \"10.244.2.3\"]\nexternal_ids        : {name=default_v4}\nname                : a5154718082306775057\n\n_uuid               : 467a93c9-9c76-40bf-8318-e91ee7f6f010\naddresses           : [\"fd00:10:244:2::3\", \"fd00:10:244:3::3\"]\nexternal_ids        : {name=default_v6}\nname                : a5154715883283518635\n</code></pre> We only use these address sets without editing them.</p> <p>For each rule that does have a <code>podSelector</code> an address set is created, adding the pods that match to it. Its name (external_ids) follows the pattern <code>egress-qos-pods-&lt;rule-namespace&gt;-&lt;rule-priority&gt;</code>, rule-priority is <code>1000 - rule's index in the array</code> which matches the QoS object's priority that relates to that rule. <pre><code># Address_Set\n\n_uuid               : 74b8df1e-5a2d-4fa4-a5e5-57bdea61d0c5\naddresses           : [\"fd00:10:244:3::3\"]\nexternal_ids        : {name=egress-qos-pods-default-999_v6}\nname                : a10759093578603529370\n\n_uuid               : a718588e-0a9b-480e-9adb-2738e524b82d\naddresses           : [\"10.244.2.3\"]\nexternal_ids        : {name=egress-qos-pods-default-999_v4}\nname                : a10759091379580272948\n</code></pre></p> <p>When a new pod that matches a rule's <code>podSelector</code> is created in the namespace we add its IPs to the relevant address set (<code>SyncEgressQoSPod</code>): <pre><code>$ kubectl get pods -o=custom-columns=Name:.metadata.name,IPs:.status.podIPs,LABELS:.metadata.labels\nName           IPs                                             LABELS\nno-labels      [map[ip:10.244.1.3] map[ip:fd00:10:244:2::3]]   &lt;none&gt;\nwith-labels1   [map[ip:10.244.2.3] map[ip:fd00:10:244:3::3]]   map[app:example]\nwith-labels2   [map[ip:10.244.2.4] map[ip:fd00:10:244:3::4]]   map[app:example]\n</code></pre></p> <pre><code># Address_Set\n\n_uuid               : 74b8df1e-5a2d-4fa4-a5e5-57bdea61d0c5\naddresses           : [\"fd00:10:244:3::3\", \"fd00:10:244:3::4\"]\nexternal_ids        : {name=egress-qos-pods-default-999_v6}\nname                : a10759093578603529370\n--\n_uuid               : a718588e-0a9b-480e-9adb-2738e524b82d\naddresses           : [\"10.244.2.3\", \"10.244.2.4\"]\nexternal_ids        : {name=egress-qos-pods-default-999_v4}\nname                : a10759091379580272948\n</code></pre> <p>When a pod is deleted or its labels change and do not longer match the rule's <code>podSelector</code> its IPs are deleted from the relevant address set: <pre><code>$ kubectl get pods -o=custom-columns=Name:.metadata.name,IPs:.status.podIPs,LABELS:.metadata.labels\nName           IPs                                             LABELS\nno-labels      [map[ip:10.244.1.3] map[ip:fd00:10:244:2::3]]   &lt;none&gt;\nwith-labels2   [map[ip:10.244.2.4] map[ip:fd00:10:244:3::4]]   map[app:not-the-example]\n</code></pre></p> <pre><code># Address_Set\n\n_uuid               : 74b8df1e-5a2d-4fa4-a5e5-57bdea61d0c5\naddresses           : []\nexternal_ids        : {name=egress-qos-pods-default-999_v6}\nname                : a10759093578603529370\n--\n_uuid               : a718588e-0a9b-480e-9adb-2738e524b82d\naddresses           : []\nexternal_ids        : {name=egress-qos-pods-default-999_v4}\nname                : a10759091379580272948\n</code></pre> <p>When an EgressQoS is updated - we recreate the QoS and Address Set objects, detach the old QoSes from the logical switches and attach the new ones:</p> <pre><code>kind: EgressQoS\napiVersion: k8s.ovn.org/v1\nmetadata:\n  name: default\n  namespace: default\nspec:\n  egress:\n  - dscp: 48\n    podSelector:\n      matchLabels:\n        app: updated-example\n  - dscp: 28\n</code></pre> <pre><code>$ kubectl get pods -o=custom-columns=Name:.metadata.name,IPs:.status.podIPs,LABELS:.metadata.labels\nName                  IPs                                             LABELS\nno-labels             [map[ip:10.244.1.3] map[ip:fd00:10:244:2::3]]   &lt;none&gt;\nwith-labels2          [map[ip:10.244.2.4] map[ip:fd00:10:244:3::4]]   map[app:not-the-example]\nwith-updated-labels   [map[ip:10.244.1.4] map[ip:fd00:10:244:2::4]]   map[app:updated-example]\n</code></pre> <pre><code># QoS\n\n_uuid               : cf84322a-0b9e-4aef-97bb-4dcd13f0e73d\naction              : {dscp=48}\nbandwidth           : {}\ndirection           : to-lport\nexternal_ids        : {EgressQoS=default}\nmatch               : \"(ip4.dst == 0.0.0.0/0 || ip6.dst == ::/0) &amp;&amp; (ip4.src == $a17475935309050627288 || ip6.src == $a17475937508073883710)\"\npriority            : 1000\n\n_uuid               : 8bf07e6b-ca0a-4d76-a206-b7ccb5e948d8\naction              : {dscp=28}\nbandwidth           : {}\ndirection           : to-lport\nexternal_ids        : {EgressQoS=default}\nmatch               : \"(ip4.dst == 0.0.0.0/0 || ip6.dst == ::/0) &amp;&amp; (ip4.src == $a5154718082306775057 || ip6.src == $a5154715883283518635)\"\npriority            : 999\n</code></pre> <pre><code># Logical_Switch\n\nname                : ovn-worker\nqos_rules           : [8bf07e6b-ca0a-4d76-a206-b7ccb5e948d8, cf84322a-0b9e-4aef-97bb-4dcd13f0e73d]\n\nname                : ovn-worker2\nqos_rules           : [8bf07e6b-ca0a-4d76-a206-b7ccb5e948d8, cf84322a-0b9e-4aef-97bb-4dcd13f0e73d]\n\nname                : ovn-control-plane\nqos_rules           : [8bf07e6b-ca0a-4d76-a206-b7ccb5e948d8, cf84322a-0b9e-4aef-97bb-4dcd13f0e73d]\n</code></pre> <pre><code># Address_Set\n\n_uuid               : a27cf559-0981-487f-a0ca-5a355da89cba\naddresses           : [\"10.244.1.4\"]\nexternal_ids        : {name=egress-qos-pods-default-1000_v4}\nname                : a17475935309050627288\n\n_uuid               : 93cdc1fd-995c-498b-88a1-4992af93c630\naddresses           : [\"fd00:10:244:2::4\"]\nexternal_ids        : {name=egress-qos-pods-default-1000_v6}\nname                : a17475937508073883710\n\n_uuid               : 7cb04096-7107-4a35-ae34-be4b0e0c584e\naddresses           : [\"10.244.1.3\", \"10.244.1.4\", \"10.244.2.4\"]\nexternal_ids        : {name=default_v4}\nname                : a5154718082306775057\n\n_uuid               : 467a93c9-9c76-40bf-8318-e91ee7f6f010\naddresses           : [\"fd00:10:244:2::3\", \"fd00:10:244:2::4\", \"fd00:10:244:3::4\"]\nexternal_ids        : {name=default_v6}\nname                : a5154715883283518635\n</code></pre>"},{"location":"features/cluster-egress-controls/egress-service/","title":"Egress Service","text":""},{"location":"features/cluster-egress-controls/egress-service/#introduction","title":"Introduction","text":"<p>The Egress Service feature enables the egress traffic of pods backing a LoadBalancer service to use a different network than the main one and/or their source IP to be the Service's ingress IP. This is useful for external systems that communicate with applications running on the Kubernetes cluster through a LoadBalancer service and expect that the source IP of egress traffic originating from the pods backing the service is identical to the destination IP they use to reach them - i.e the LoadBalancer's ingress IP. In addition, this allows to separate the egress traffic of specified LoadBalancer services by different networks (VRFs).</p> <p>By introducing a new CRD <code>EgressService</code>, users could request that egress packets originating from all of the pods that are endpoints of a LoadBalancer service would use a different network than the main one and/or their source IP will be the Service's ingress IP. The CRD is namespace scoped. The name of the EgressService corresponds to the name of a LoadBalancer Service that should be affected by this functionality. Note the mapping of EgressService to Kubernetes Service is 1to1. The feature is supported fully by \"Local\" gateway mode and almost entirely by \"Shared\" gateway mode (it does not support Network without LoadBalancer SNAT). In any case the affected traffic will be that which is coming from a pod to a destination outside of the cluster - meaning pod-pod / pod-service / pod-node traffic will not be affected.</p> <p>Announcing the service externally (for ingress traffic) is handled by a LoadBalancer provider (like MetalLB) and not by OVN-Kubernetes as explained later.</p>"},{"location":"features/cluster-egress-controls/egress-service/#details-modifying-the-source-ip-of-the-egress-packets","title":"Details - Modifying the source IP of the egress packets","text":"<p>Only SNATing a pod's IP to the LoadBalancer service ingress IP that it is backing is problematic, as usually the ingress IP is exposed via multiple nodes by the LoadBalancer provider. This means we can't just add an SNAT to the regular traffic flow of a pod before it exits its node because we don't have a guarantee that the reply will come back to the pod's node (where the traffic originated). An external client usually has multiple paths to reach the LoadBalancer ingress IP and could reply to a node that is not the pod's node - in that case the other node does not have the proper CONNTRACK entries to send the reply back to the pod and the traffic is lost. For that reason, we need to make sure that all traffic for the service's pods (ingress/egress) is handled by a single node so the right CONNTRACK entries are always matched and the traffic is not lost.</p> <p>The egress part is handled by OVN-Kubernetes, which chooses a node that acts as the point of ingress/egress, and steers the relevant pods' egress traffic to its mgmt port, by using logical router policies on the <code>ovn_cluster_router</code>. When that traffic reaches the node's mgmt port it will use its routing table and iptables before heading out. Because of that, it takes care of adding the necessary iptables rules on the selected node to SNAT traffic exiting from these pods to the service's ingress IP.</p> <p>These goals are achieved by introducing a new resource <code>EgressService</code> for users to create alongside LoadBalancer services with the following fields: - <code>sourceIPBy</code>: Determines the source IP of egress traffic originating from the pods backing the Service. When \"LoadBalancerIP\" the source IP is set to the Service's LoadBalancer ingress IP. When \"Network\" the source IP is set according to the interface of the Network, leveraging the masquerade rules that are already in place. Typically these rules specify SNAT to the IP of the outgoing interface, which means the packet will typically leave with the IP of the node.</p> <p><code>nodeSelector</code>: Allows limiting the nodes that can be selected to handle the service's traffic when sourceIPBy: \"LoadBalancerIP\". When present only a node whose labels match the specified selectors can be selected for handling the service's traffic as explained earlier. When the field is not specified any node in the cluster can be chosen to manage the service's traffic. In addition, if the service's <code>ExternalTrafficPolicy</code> is set to <code>Local</code> an additional constraint is added that only a node that has an endpoint can be selected - this is important as otherwise new ingress traffic will not work properly if there are no local endpoints on the host to forward to. This also means that when \"ETP=Local\" only endpoints local to the selected host will be used for ingress traffic and other endpoints will not be used.</p> <ul> <li><code>network</code>: The network which this service should send egress and corresponding ingress replies to. This is typically implemented as VRF mapping, representing a numeric id or string name of a routing table which by omission uses the default host routing.</li> </ul> <p>When a node is selected to handle the service's traffic both the status of the relevant <code>EgressService</code> is updated with <code>host: &lt;node_name&gt;</code> (which is consumed by <code>ovnkube-node</code>) and the node is labeled with <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code>, which can be consumed by a LoadBalancer provider to handle the ingress part.</p> <p>Similarly to the EgressIP feature, once a node is selected it is checked for readiness (TCP/gRPC) to serve traffic every x seconds. If a node fails the health check, its allocated services move to another node by removing the <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code> label from it, removing the logical router policies from the cluster router, resetting the status of the relevant <code>EgressServices</code> and requeuing them - causing a new node to be selected for the services. If the node becomes not ready or its labels no longer match the service's selectors the same re-election process happens.</p> <p>The ingress part is handled by a LoadBalancer provider, such as MetalLB, that needs to select the right node (and only it) for announcing the LoadBalancer service (ingress traffic) according to the <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code> label set by OVN-Kubernetes. A full example with MetalLB is detailed in Usage Example.</p> <p>Just to be clear, OVN-Kubernetes does not care which component advertises the LoadBalancer service or checks if it does it correctly - it is the user's responsibility to make sure ingress traffic arrives only to the node with the <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code> label.</p> <p>Assuming an Egress Service has <code>172.19.0.100</code> as its ingress IP and <code>ovn-worker</code> selected to handle all of its traffic, the egress traffic flow of an endpoint pod with the ip <code>10.244.1.6</code> on <code>ovn-worker2</code> towards an external destination (172.19.0.5) will look like: <pre><code>                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502                    \u2502\n                     \u2502external destination\u2502\n                     \u2502    172.19.0.5      \u2502\n                     \u2502                    \u2502\n                     \u2514\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n     5. packet reaches   \u2502                      2. router policy rereoutes it\n        the external     \u2502                         to ovn-worker's mgmt port\n        destination with \u2502                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        src ip:          \u2502                  \u250c\u2500\u2500\u2500\u2524ovn cluster router\u2502\n        172.19.0.100     \u2502                  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502                  \u2502               \u2502\n                         \u2502                  \u2502               \u25021. packet to 172.19.0.5\n                      \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2510              \u2502   heads to the cluster router\n                   \u250c\u2500\u2500\u2518 eth1 \u2514\u2500\u2500\u2510  \u250c\u2500\u2500\u2518 mgmt \u2514\u2500\u2500\u2510           \u2502   as usual\n                   \u2502 172.19.0.2 \u2502  \u2502 10.244.0.2 \u2502           \u2502\n                   \u251c\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2524           \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n4. an iptables rule\u2502     \u2502   ovn-worker  \u25023.    \u2502           \u2502   \u2502  ovn-worker2   \u2502\n   that SNATs to   \u2502     \u2502               \u2502      \u2502           \u2502   \u2502                \u2502\n   the service's ip\u2502     \u2502               \u2502      \u2502           \u2502   \u2502                \u2502\n   is hit          \u2502     \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502      \u2502           \u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n                   \u2502     \u25024.\u2502routes +\u2502   \u2502      \u2502           \u2514\u2500\u2500\u2500\u253c\u2500\u2524    pod     \u2502 \u2502\n                   \u2502     \u2514\u2500\u2500\u2524iptables\u25c4\u2500\u2500\u2500\u2518      \u2502               \u2502 \u2502 10.244.1.6 \u2502 \u2502\n                   \u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502               \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n                   \u2502                            \u2502               \u2502                \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                3. from the mgmt port it hits ovn-worker's\n                   routes and iptables rules\n</code></pre> Notice how the packet exits <code>ovn-worker</code>'s eth1 and not breth0, as the packet goes through the host's routing table regardless of the gateway mode.</p> <p>When <code>sourceIPBy: \"Network\"</code> is set, <code>ovnkube-master</code> does not need to create any logical router policies because the egress packets of each pod would exit through the pod's node but will set the status field of the resource with <code>host: ALL</code> as decribed later.</p>"},{"location":"features/cluster-egress-controls/egress-service/#network","title":"Network","text":"<p>The <code>EgressService</code> supports a <code>network</code> field to specify to which network the egress traffic of the service should be steered to. When it is specified the relevant <code>ovnkube-nodes</code> take care of creating ip rules on their host - either the node which matches <code>Status.Host</code> or all of the nodes when <code>Status.Host</code> is \"ALL\". Assuming an <code>EgressService</code> has <code>Network: blue</code>, a ClusterIP of 10.96.135.5 and its endpoints are 10.244.0.3 and 10.244.1.6 the following will be created on the host:</p> <pre><code>$ ip rule list\n5000:   from 10.96.135.5 lookup blue\n5000:   from 10.244.0.3 lookup blue\n5000:   from 10.244.1.6 lookup blue\n</code></pre> <p>This makes the egress traffic of endpoints of an EgressService to be routed via the \"blue\" routing table. An ip rule is also created for the ClusterIP of the service which is needed in order for the ingress reply traffic (reply to an external client calling the service) to use the correct table - this is because the packet flow of contacting a LoadBalancer service goes: <code>lb ip -&gt; node -&gt; enter ovn with ClusterIP -&gt; exit ovn with ClusterIP -&gt; exit node with lb ip</code> so we need to make sure that packets from ClusterIPs are marked before being routed in order for them to hit the relevant ip rule in time.</p>"},{"location":"features/cluster-egress-controls/egress-service/#network-without-loadbalancer-snat","title":"Network without LoadBalancer SNAT","text":"<p>As mentioned earlier, it is possible to use the \"Network\" capability without SNATing the traffic to the service's ingress IP. This is done by creating an EgressService with the <code>Network</code> field specified and <code>sourceIPBy: \"Network\"</code>.</p> <p>An EgressService with <code>sourceIPBy: \"Network\"</code> does not need to have a host selected, as the traffic will exit each node with the IP of the interface corresponding to the \"Network\" by leveraging the masquerade rules that are already in place.</p> <p>This works only on clusters running on \"Local\" gateway mode, because on \"Shared\" gateway mode the ip rules created by the controller are ignored (like all of the node's routing stack).</p> <p>When <code>sourceIPBy: \"Network\"</code>, <code>ovnkube-master</code> does not need to create any logical router policies as the egress packets of each pod would exit through the pod's node. However, <code>ovnkube-master</code> will set the status field of the resource with <code>host: ALL</code> to designate that no reroute logical router policies exist for the service, \"instructing\" all of the <code>ovnkube-nodes</code> to handle the resource's <code>Network</code> field without creating SNAT iptables rules.</p> <p>When <code>ovnkube-node</code> detects that the host of an EgressService is <code>ALL</code>, only the endpoints local to the node will have an ip rule created, and no SNAT iptables rules will be created.</p> <p>It is the user's responsibility to make sure that the pods backing an EgressService without SNAT run only on nodes that have the required \"Network\", as no additional steering (lrps) will take place by OVN and pods running on nodes without a correct \"Network\" will misbehave.</p>"},{"location":"features/cluster-egress-controls/egress-service/#changes-in-ovn-northbound-database-and-iptables","title":"Changes in OVN northbound database and iptables","text":"<p>The feature is implemented by reacting to events from <code>EgressServices</code>, <code>Services</code>, <code>EndpointSlices</code> and <code>Nodes</code> changes - updating OVN's northbound database <code>Logical_Router_Policy</code> objects to steer the traffic to the selected node and creating iptables SNAT rules in its <code>OVN-KUBE-EGRESS-SVC</code> chain, which is called by the POSTROUTING chain of its nat table.</p> <p>We'll see how the related objects are changed once a LoadBalancer is requested to act as an \"Egress Service\" by creating a corresponding <code>EgressService</code> named after it in a Dual-Stack kind cluster.</p> <p>We start with a clean cluster: <pre><code>$ kubectl get nodes\nNAME                STATUS   ROLES\novn-control-plane   Ready    control-plane\novn-worker          Ready    worker\novn-worker2         Ready    worker\n</code></pre></p> <pre><code>$ kubectl describe svc demo-svc\nName:                     demo-svc\nNamespace:                default\nType:                     LoadBalancer\nLoadBalancer Ingress:     5.5.5.5, 5555:5555:5555:5555:5555:5555:5555:5555\nEndpoints:                10.244.0.5:8080,10.244.2.7:8080\n                          fd00:10:244:1::5,fd00:10:244:3::7\n</code></pre> <pre><code>$ ovn-nbctl lr-policy-list ovn_cluster_router\nRouting Policies\n      1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip4.dst == 172.18.0.3 /* ovn-control-plane */         reroute                10.244.2.2\n      1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::3 /* ovn-control-plane */         reroute          fd00:10:244:3::2\n      1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip4.dst == 172.18.0.4 /* ovn-worker */         reroute                10.244.0.2\n      1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::4 /* ovn-worker */         reroute          fd00:10:244:1::2\n      1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip4.dst == 172.18.0.2 /* ovn-worker2 */         reroute                10.244.1.2\n      1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::2 /* ovn-worker2 */         reroute          fd00:10:244:2::2\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 10.244.0.0/16           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 100.64.0.0/16           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.2/32           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.3/32           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.4/32           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::2/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::3/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::4/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fd00:10:244::/48           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fd98::/64           allow\n</code></pre> <p>At this point nothing related to Egress Services is in place. It is worth noting that the \"allow\" policies (102's) that make sure east-west traffic is not affected for EgressIPs are present here as well - if the EgressIP feature is enabled it takes care of creating them, otherwise the \"Egress Service\" feature does (sharing the same logic), as we do not want Egress Services to change the behavior of east-west traffic. Also, the policies created (seen later) for an Egress Service use a higher priority than the EgressIP ones, which means that if a pod belongs to both an EgressIP and an Egress Service the service's ingress IP will be used for the SNAT.</p> <p>We now request that our service \"demo-svc\" will act as an \"Egress Service\" by creating a corresponding <code>EgressService</code>, with the constraint that only a node with the <code>\"node-role.kubernetes.io/worker\": \"\"</code> label can be selected to handle its traffic: <pre><code>$ cat egress-service.yaml\napiVersion: k8s.ovn.org/v1\nkind: EgressService\nmetadata:\n  name: demo-svc\n  namespace: default\nspec:\n  sourceIPBy: \"LoadBalancerIP\"\n  nodeSelector:\n    matchLabels:\n      node-role.kubernetes.io/worker: \"\"\n\n$ kubectl apply -f egress-service.yaml\negressservice.k8s.ovn.org/demo-svc created\n</code></pre></p> <p>Once the <code>EgressService</code> is created a node is selected to handle all of its traffic (ingress/egress) as described earlier. The <code>EgressService</code> status is updated with its name, logical router policies are created on ovn_cluster_router to steer the endpoints' traffic to its mgmt port, SNAT rules are created in its iptables and it is labeled as the node in charge of the service's traffic:</p> <p>The status points to <code>ovn-worker2</code>, meaning it was selected to handle the service's traffic: <pre><code>$ kubectl describe egressservice demo-svc\nName:         demo-svc\nNamespace:    default\nSpec:\n    Source IP By:                        LoadBalancerIP\n    Node Selector:\n      Match Labels:\n        node-role.kubernetes.io/worker:  \"\"\nStatus:\n  Host:  ovn-worker2\n</code></pre></p> <p>A logical router policy is created for each endpoint to steer its egress traffic towards <code>ovn-worker2</code>'s mgmt port: <pre><code>$ ovn-nbctl lr-policy-list ovn_cluster_router\nRouting Policies\n       &lt;truncated 1004's and 102's&gt;\n       101                              ip4.src == 10.244.0.5         reroute                10.244.1.2\n       101                              ip4.src == 10.244.2.7         reroute                10.244.1.2\n       101                        ip6.src == fd00:10:244:1::5         reroute          fd00:10:244:2::2\n       101                        ip6.src == fd00:10:244:3::7         reroute          fd00:10:244:2::2\n</code></pre></p> <p>An SNAT rule to the service's ingress IP is created for each endpoint: <pre><code>$ hostname\novn-worker2\n\n$ iptables-save\n*nat\n...\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n-A OVN-KUBE-EGRESS-SVC -s 10.244.0.5/32 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5.5.5.5\n-A OVN-KUBE-EGRESS-SVC -s 10.244.2.7/32 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5.5.5.5\n...\n\n$ ip6tables-save\n...\n*nat\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n-A OVN-KUBE-EGRESS-SVC -s fd00:10:244:3::7/128 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5555:5555:5555:5555:5555:5555:5555:5555\n-A OVN-KUBE-EGRESS-SVC -s fd00:10:244:1::5/128 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5555:5555:5555:5555:5555:5555:5555:5555\n...\n</code></pre></p> <p><code>ovn-worker2</code> is the only node holding the <code>egress-service.k8s.ovn.org/default-demo-svc=\"\"</code> label: <pre><code>$ kubectl get nodes -l egress-service.k8s.ovn.org/default-demo-svc=\"\"\nNAME          STATUS   ROLES\novn-worker2   Ready    worker\n</code></pre></p> <p>When the endpoints of the service change, the logical router policies and iptables rules are changed accordingly.</p> <p>We will now simulate a failover of the service when a node fails its health check. By stopping <code>ovn-worker2</code>'s container we see that all of the resources \"jump\" to <code>ovn-worker</code>, as it is the only node left matching the <code>nodeSelector</code>:</p> <pre><code>$ docker stop ovn-worker2\novn-worker2\n</code></pre> <p>The status now points to <code>ovn-worker</code>: <pre><code>$ kubectl describe egressservice demo-svc\nName:         demo-svc\nNamespace:    default\nSpec:\n    Source IP By:                        LoadBalancerIP\n    Node Selector:\n      Match Labels:\n        node-role.kubernetes.io/worker:  \"\"\nStatus:\n  Host:  ovn-worker\n</code></pre></p> <p>The reroute destination changed to <code>ovn-worker</code>'s mgmt port (10.244.1.2 -&gt; 10.244.0.2, fd00:10:244:2::2 -&gt; fd00:10:244:1::2): <pre><code>$ ovn-nbctl lr-policy-list ovn_cluster_router\nRouting Policies\n       &lt;truncated 1004's and 102's&gt;\n       101                              ip4.src == 10.244.0.5         reroute                10.244.0.2\n       101                              ip4.src == 10.244.2.7         reroute                10.244.0.2\n       101                        ip6.src == fd00:10:244:1::5         reroute          fd00:10:244:1::2\n       101                        ip6.src == fd00:10:244:3::7         reroute          fd00:10:244:1::2\n</code></pre></p> <p>The iptables rules were created on <code>ovn-worker</code>: <pre><code>$ hostname\novn-worker\n\n$ iptables-save\n*nat\n...\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n-A OVN-KUBE-EGRESS-SVC -s 10.244.0.5/32 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5.5.5.5\n-A OVN-KUBE-EGRESS-SVC -s 10.244.2.7/32 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5.5.5.5\n...\n\n$ ip6tables-save\n...\n*nat\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n-A OVN-KUBE-EGRESS-SVC -s fd00:10:244:3::7/128 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5555:5555:5555:5555:5555:5555:5555:5555\n-A OVN-KUBE-EGRESS-SVC -s fd00:10:244:1::5/128 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5555:5555:5555:5555:5555:5555:5555:5555\n...\n</code></pre></p> <p>The label moved to <code>ovn-worker</code>: <pre><code>$ kubectl get nodes -l egress-service.k8s.ovn.org/default-demo-svc=\"\"\nNAME         STATUS   ROLES\novn-worker   Ready    worker\n</code></pre></p> <p>Finally, deleting the <code>EgressService</code> resource resets the cluster to the point we started from: <pre><code>$ kubectl delete egressservice demo-svc\negressservice.k8s.ovn.org \"demo-svc\" deleted\n</code></pre></p> <pre><code>$ ovn-nbctl lr-policy-list ovn_cluster_router\nRouting Policies\n      1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip4.dst == 172.18.0.3 /* ovn-control-plane */         reroute                10.244.2.2\n      1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::3 /* ovn-control-plane */         reroute          fd00:10:244:3::2\n      1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip4.dst == 172.18.0.4 /* ovn-worker */         reroute                10.244.0.2\n      1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::4 /* ovn-worker */         reroute          fd00:10:244:1::2\n      1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip4.dst == 172.18.0.2 /* ovn-worker2 */         reroute                10.244.1.2\n      1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::2 /* ovn-worker2 */         reroute          fd00:10:244:2::2\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 10.244.0.0/16           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 100.64.0.0/16           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.2/32           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.3/32           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.4/32           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::2/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::3/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::4/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fd00:10:244::/48           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fd98::/64           allow\n</code></pre> <pre><code>$ hostname\novn-worker\n\n$ iptables-save | grep EGRESS\n:OVN-KUBE-EGRESS-SVC - [0:0]\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n\n$ ip6tables-save | grep EGRESS\n:OVN-KUBE-EGRESS-SVC - [0:0]\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n</code></pre> <pre><code>$ kubectl get nodes -l egress-service.k8s.ovn.org/default-demo-svc=\"\"\nNo resources found\n</code></pre>"},{"location":"features/cluster-egress-controls/egress-service/#tbd-dealing-with-non-snated-traffic","title":"TBD: Dealing with non SNATed traffic","text":"<p>The host of an Egress Service is often in charge of pods (endpoints) that run in different nodes. Due to the fact that ovn-controllers on different nodes apply the changes independently, there is a chance that some pod traffic will reach the host before it configures the relevant SNAT iptables rules. In that timeframe, the egress traffic from these pods will exit the host with their ip instead of the LB's ingress ip, and the it will not be able to return properly because an external client is not aware of a pod's inner ip.</p> <p>This is currently a known issue for EgressService because we can't leverage the same as EgressIP currently does by setting a flow on breth0 - the flow won't be hit because the traffic \"exits\" OVN when using EgressService (= doesn't hit the host's breth0) as opposed to how EgressIP \"keeps everything\" inside OVN.</p>"},{"location":"features/cluster-egress-controls/egress-service/#usage-example","title":"Usage Example","text":"<p>While the user does not need to know all of the details of how \"Egress Services\" work, they need to know that in order for a service to work properly the access to it from outside the cluster (ingress traffic) has to go only through the node labeled with the <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code> label - i.e the node designated by OVN-Kubernetes to handle all of the service's traffic. As mentioned earlier, OVN-Kubernetes does not care which component advertises the LoadBalancer service or checks if it does it correctly.</p> <p>Here we look at an example of \"Egress Services\" using MetalLB to advertise the LoadBalancer service externally. A user of MetalLB can follow these steps to create a LoadBalancer service whose endpoints exit the cluster with its ingress IP. We already assume MetalLB's <code>BGPPeers</code> are configured and the sessions are established.</p> <ol> <li> <p>Create the IPAddressPool with the desired IP for the service. It makes sense to set <code>autoAssign: false</code> so it is not taken by another service by mistake - our service will request that pool explicitly.  <pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: example-pool\n  namespace: metallb-system\nspec:\n  addresses:\n  - 172.19.0.100/32\n  autoAssign: false\n</code></pre></p> </li> <li> <p>Create the LoadBalancer service and the corresponding EgressService. We create the service with the <code>metallb.universe.tf/address-pool</code> annotation to explicitly request its IP to be from the <code>example-pool</code> and the EgressService with a <code>nodeSelector</code> so that the traffic exits from a node that matches these selectors. <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: some-namespace\n  annotations:\n    metallb.universe.tf/address-pool: example-pool\nspec:\n  selector:\n    app: example\n  ports:\n    - name: http\n      protocol: TCP\n      port: 8080\n      targetPort: 8080\n  type: LoadBalancer\n---\napiVersion: k8s.ovn.org/v1\nkind: EgressService\nmetadata:\n  name: example-service\n  namespace: some-namespace\nspec:\n  sourceIPBy: \"LoadBalancerIP\"\n  nodeSelector:\n    matchLabels:\n      node-role.kubernetes.io/worker: \"\"\n</code></pre></p> </li> <li> <p>Advertise the service from the node in charge of the service's traffic. So far the service is \"broken\" - it is not reachable from outside the cluster and if the pods try to send traffic outside it would probably not come back as it is SNATed to an IP which is unknown. We create the advertisements targeting only the node that is in charge of the service's traffic using the <code>nodeSelector</code> field, relying on ovn-k to label the node properly. <pre><code>apiVersion: metallb.io/v1beta1\nkind: BGPAdvertisement\nmetadata:\n  name: example-bgp-adv\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n  - example-pool\n  nodeSelector:\n  - matchLabels:\n      egress-service.k8s.ovn.org/some-namespace-example-service: \"\"\n</code></pre> While possible to create more advertisements resources for the <code>example-pool</code>, it is the user's responsibility to make sure that the pool is advertised only by advertisements targeting the node holding the <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code> label - otherwise the traffic of the service will be broken.</p> </li> </ol>"},{"location":"features/hardware-offload/dpu-support/","title":"Dpu support","text":""},{"location":"features/hardware-offload/dpu-support/#dpu-support","title":"DPU support","text":"<p>With the emergence of Data Processing Units (DPUs),  NIC vendors can now offer greater hardware acceleration capability, flexibility and security. </p> <p>It is desirable to leverage DPU in OVN-kubernetes to accelerate networking and secure the network control plane.</p> <p>A DPU consists of: - Industry-standard, high-performance, software-programmable multi-core CPU - High-performance network interface - Flexible and programmable acceleration engines</p> <p>Similarly to Smart-NICs, a DPU follows the kernel switchdev model. In this model, every VF/PF net-device on the host has a corresponding representor net-device existing on the embedded CPU.</p> <p>Any vendor that manufactures a DPU which supports the above model should work with current design.</p> <p>Design document can be found here.</p>"},{"location":"features/hardware-offload/ovs_offload/","title":"OVS Hardware Offload","text":"<p>The OVS software based solution is CPU intensive, affecting system performance and preventing fully utilizing available bandwidth. OVS 2.8 and above support new feature called OVS Hardware Offload which improves performance significantly.  This feature allows to offload the OVS data-plane to the NIC while maintaining  OVS control-plane unmodified. It is using SR-IOV technology with VF representor host net-device. The VF representor plays the same role as TAP devices in Para-Virtual (PV) setup. A packet sent through the VF representor on the host arrives to the VF, and a packet sent through the VF is received by its representor.</p>"},{"location":"features/hardware-offload/ovs_offload/#supported-ethernet-controllers","title":"Supported Ethernet controllers","text":"<p>The following manufacturers are known to work:</p> <ul> <li>Mellanox ConnectX-5 NIC</li> <li>Mellanox ConnectX-6DX NIC</li> </ul>"},{"location":"features/hardware-offload/ovs_offload/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux Kernel 5.7.0 or above</li> <li>Open vSwitch 2.13 or above</li> <li>iproute &gt;= 4.12</li> <li>sriov-device-plugin</li> <li>multus-cni</li> </ul>"},{"location":"features/hardware-offload/ovs_offload/#worker-node-sr-iov-configuration","title":"Worker Node SR-IOV Configuration","text":"<p>In order to enable Open vSwitch hardware offloading, the following steps are required. Please make sure you have root privileges to run the commands below.</p> <p>Check the Number of VF Supported on the NIC</p> <pre><code>cat /sys/class/net/enp3s0f0/device/sriov_totalvfs\n8\n</code></pre> <p>Create the VFs</p> <pre><code>echo '4' &gt; /sys/class/net/enp3s0f0/device/sriov_numvfs\n</code></pre> <p>Verfiy the VFs are created</p> <pre><code>ip link show enp3s0f0\n8: enp3s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT qlen 1000\n   link/ether a0:36:9f:8f:3f:b8 brd ff:ff:ff:ff:ff:ff\n   vf 0 MAC 00:00:00:00:00:00, spoof checking on, link-state auto\n   vf 1 MAC 00:00:00:00:00:00, spoof checking on, link-state auto\n   vf 2 MAC 00:00:00:00:00:00, spoof checking on, link-state auto\n   vf 3 MAC 00:00:00:00:00:00, spoof checking on, link-state auto\n</code></pre> <p>Setup the PF to be up</p> <pre><code>ip link set enp3s0f0 up\n</code></pre> <p>Unbind the VFs from the driver</p> <pre><code>echo 0000:03:00.2 &gt; /sys/bus/pci/drivers/mlx5_core/unbind\necho 0000:03:00.3 &gt; /sys/bus/pci/drivers/mlx5_core/unbind\necho 0000:03:00.4 &gt; /sys/bus/pci/drivers/mlx5_core/unbind\necho 0000:03:00.5 &gt; /sys/bus/pci/drivers/mlx5_core/unbind\n</code></pre> <p>Configure SR-IOV VFs to switchdev mode</p> <pre><code>devlink dev eswitch set pci/0000:03:00.0 mode switchdev\nethtool -K enp3s0f0 hw-tc-offload on\n</code></pre> <p>Bind the VFs to the driver</p> <pre><code>echo 0000:03:00.2 &gt; /sys/bus/pci/drivers/mlx5_core/bind\necho 0000:03:00.3 &gt; /sys/bus/pci/drivers/mlx5_core/bind\necho 0000:03:00.4 &gt; /sys/bus/pci/drivers/mlx5_core/bind\necho 0000:03:00.5 &gt; /sys/bus/pci/drivers/mlx5_core/bind\n</code></pre> <p>Set hw-offload=true restart Open vSwitch</p> <pre><code>systemctl enable openvswitch.service\novs-vsctl set Open_vSwitch . other_config:hw-offload=true\nsystemctl restart openvswitch.service\n</code></pre>"},{"location":"features/hardware-offload/ovs_offload/#worker-node-sr-iov-network-device-plugin-configuration","title":"Worker Node SR-IOV network device plugin configuration","text":"<p>This plugin creates device plugin endpoints based on the configurations given in file <code>/etc/pcidp/config.json</code>. This configuration file is in json format as shown below:</p> <pre><code>{\n    \"resourceList\": [\n         {\n            \"resourceName\": \"cx5_sriov_switchdev\",\n            \"selectors\": {\n                \"vendors\": [\"15b3\"],\n                \"devices\": [\"1018\"]\n            }\n        }\n    ]\n}\n</code></pre> <p>Deploy SR-IOV network device plugin as daemonset see https://github.com/intel/sriov-network-device-plugin</p>"},{"location":"features/hardware-offload/ovs_offload/#worker-node-multus-cni-configuration","title":"Worker Node Multus CNI configuration","text":"<p>Multus Config <pre><code>{\n  \"name\": \"multus-cni-network\",\n  \"type\": \"multus\",\n  \"clusterNetwork\": \"default\",\n  \"defaultNetworks\":[],\n  \"kubeconfig\": \"/etc/kubernetes/node-kubeconfig.yaml\"\n}\n</code></pre></p> <p>Deploy multus CNI as daemonset see https://github.com/intel/multus-cni</p> <p>Create NetworkAttachementDefinition CRD with OVN CNI config</p> <pre><code>Kubernetes Network CRD Spec:\napiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: default\n  annotations:\n    k8s.v1.cni.cncf.io/resourceName: mellanox.com/cx5_sriov_switchdev\nspec:\n  Config: '{\"cniVersion\":\"0.3.1\",\"name\":\"ovn-kubernetes\",\"type\":\"ovn-k8s-cni-overlay\",\"ipam\":{},\"dns\":{}}'\n</code></pre>"},{"location":"features/hardware-offload/ovs_offload/#deploy-pod-with-ovs-hardware-offload","title":"Deploy POD with OVS hardware-offload","text":"<p>Create POD spec and</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ovs-offload-pod1\n  annotations:\n    v1.multus-cni.io/default-network: default\nspec:\n  containers:\n  - name: appcntr1\n    image: centos/tools\n    resources:\n      requests:\n        mellanox.com/cx5_sriov_switchdev: '1'\n      limits:\n        mellanox.com/cx5_sriov_switchdev: '1'\n</code></pre>"},{"location":"features/hardware-offload/ovs_offload/#verify-hardware-offload-is-working","title":"Verify Hardware-Offload is working","text":"<p>Lookup VF representor, in this example it is e5a1c8fcef0f327</p> <pre><code>$ ip link show enp3s0f0\n6: enp3s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000\n   link/ether ec:0d:9a:46:9e:84 brd ff:ff:ff:ff:ff:ff\n   vf 0 MAC 00:00:00:00:00:00, spoof checking off, link-state enable, trust off, query_rss off\n   vf 1 MAC 00:00:00:00:00:00, spoof checking off, link-state enable, trust off, query_rss off\n   vf 2 MAC 00:00:00:00:00:00, spoof checking off, link-state enable, trust off, query_rss off\n   vf 3 MAC fa:16:3e:b9:b8:ce, vlan 57, spoof checking on, link-state enable, trust off, query_rss off\n\ncompute_node2# ls -l /sys/class/net/\nlrwxrwxrwx 1 root root 0 Sep 11 10:54 eth0 -&gt; ../../devices/virtual/net/eth0\nlrwxrwxrwx 1 root root 0 Sep 11 10:54 eth1 -&gt; ../../devices/virtual/net/eth1\nlrwxrwxrwx 1 root root 0 Sep 11 10:54 eth2 -&gt; ../../devices/virtual/net/eth2\nlrwxrwxrwx 1 root root 0 Sep 11 10:54 e5a1c8fcef0f327 -&gt; ../../devices/virtual/net/e5a1c8fcef0f327\n</code></pre> <p>Access the POD</p> <pre><code>kubectl exec -it ovs-offload-pod1 -- /bin/bash\n</code></pre> <p>Ping other POD on second worker node <pre><code>ping ovs-offload-pod2\n</code></pre></p> <p>Check traffic on the VF representor port. Verify that only the first ICMP packet appears <pre><code>tcpdump -nnn -i e5a1c8fcef0f327\n\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n17:12:41.260487 IP 172.0.0.13 &gt; 172.0.0.10: ICMP echo request, id 1263, seq 1, length 64\n17:12:41.260778 IP 172.0.0.10 &gt; 172.0.0.13: ICMP echo reply, id 1263, seq 1, length 64\n17:12:46.268951 ARP, Request who-has 172.0.0.13 tell 172.0.0.10, length 42\n17:12:46.271771 ARP, Reply 172.0.0.13 is-at fa:16:3e:1a:10:05, length 46\n17:12:55.354737 IP6 fe80::f816:3eff:fe29:8118 &gt; ff02::1: ICMP6, router advertisement, length 64\n17:12:56.106705 IP 0.0.0.0.68 &gt; 255.255.255.255.67: BOOTP/DHCP, Request from 62:21:f0:89:40:73, length 30\n</code></pre></p>"},{"location":"features/hardware-offload/ovs_offload/#ovs-hardware-offload-dpu-support","title":"OVS hardware offload DPU support","text":"<p>Data Processing Units (DPU) combine the advanced capabilities of a Smart-NIC (such as Mellanox ConnectX-6DX NIC) with a general purpose embedded CPU and a high-speed memory controller.</p> <p>Similarly to Smart-NICs, a DPU follows the kernel switchdev model. In this model, every VF/PF net-device on the host has a corresponding representor net-device existing on the embedded CPU.</p>"},{"location":"features/hardware-offload/ovs_offload/#supported-dpus","title":"Supported DPUs","text":"<p>The following manufacturers are known to work:</p> <ul> <li>Mellanox Bluefield-2</li> </ul> <p>Deployment guide can be found here.</p>"},{"location":"features/hardware-offload/ovs_offload/#vdpa","title":"vDPA","text":"<p>vDPA (Virtio DataPath Acceleration) is a technology that enables the acceleration of virtIO devices while allowing the implementations of such devices (e.g: NIC vendors) to use their own control plane.</p> <p>vDPA can be combined with the SR-IOV OVS Hardware offloading setup to expose the workload to an open standard interface such as virtio-net.</p>"},{"location":"features/hardware-offload/ovs_offload/#additional-prerequisites","title":"Additional Prerequisites:","text":"<ul> <li>Linux Kernel &gt;= 5.12</li> <li>iproute &gt;= 5.14</li> </ul>"},{"location":"features/hardware-offload/ovs_offload/#supported-hardware","title":"Supported Hardware:","text":"<ul> <li>Mellanox ConnectX-6DX NIC</li> </ul>"},{"location":"features/hardware-offload/ovs_offload/#additional-configuration","title":"Additional configuration","text":"<p>In addition to all the steps listed above, insert the virtio-vdpa driver and the mlx-vdpa driver:</p> <pre><code>$ modprobe vdpa\n$ modprobe virtio-vdpa\n$ modprobe mlx5-vdpa\n</code></pre> <p>The the <code>vdpa</code> tool (part of iproute package) is used to create a vdpa device on top of an existing VF:</p> <pre><code>$ vdpa mgmtdev show\npci/0000:65:00.2:\n  supported_classes net\n$ vdpa dev add name vdpa2 mgmtdev pci/0000:65:00.2\n$ vdpa dev list\nvdpa2: type network mgmtdev pci/0000:65:00.2 vendor_id 5555 max_vqs 16 max_vq_size 256\n</code></pre> <p>After a device has been created, the SR-IOV Device Plugin plugin configuration has to be modified for it to select and expose the vdpa device:</p> <pre><code>{\n    \"resourceList\": [\n         {\n            \"resourceName\": \"cx6_sriov_vpda_virtio\",\n            \"selectors\": {\n               \"vendors\": [\"15b3\"],\n               \"devices\": [\"101e\"],\n               \"vdpaType\": \"virtio\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"features/infrastructure-security-controls/node-identity/","title":"OVN-Kubernetes Node Identity","text":""},{"location":"features/infrastructure-security-controls/node-identity/#introduction","title":"Introduction","text":"<p>The OVN-Kubernetes node identity feature introduces a per-node client certificate for ovnkube-node pods, together with a validating admission webhook,  enabling granular permission enforcement specific to ovn-kubernetes. Previously, all <code>ovnkube-node</code> pods shared a common service account,  with their API permissions managed only through Kubernetes RBAC rules.\\ The goal of this feature is to limit <code>ovnkube-node</code> permissions to the minimum required for networking management on a specific Kubernetes node. We will mimic the approach used by kubelet in which every node has a unique identity,  and its API write requests are verified using a NodeRestriction validating admission webhook.</p>"},{"location":"features/infrastructure-security-controls/node-identity/#per-node-client-certificates","title":"Per-node client certificates","text":"<p>This process mimics the bootstrap initialization in kubelet. When the <code>ovnkube-node</code> starts for the first time, it uses the host's <code>kubeconfig</code> to create a CertificateSigningRequest that requests a client certificate for the <code>system:ovn-node:&lt;nodeName&gt;</code> user in the <code>system:ovn-nodes</code> group. This request is then signed by the kubernetes.io/kube-apiserver-client signer.</p> <p>For the certificate to be signed, it must first be approved. The newly introduced <code>OVNKubeCSRController</code> component approves or denies <code>CertificateSigningRequests</code> created for users with the <code>system:ovn-node</code> prefix. The <code>OVNKubeCSRController</code> performs several checks to validate the requested certificate, including: - Ensuring that the node name extracted from the request matches the node name of the user that sent it. - Verifying that the group is set to <code>system:ovn-nodes</code>. - Checking that the certificate expiration does not exceed the maximum allowed duration.</p> <p>Once the certificate is approved and signed, <code>ovnkube-node</code> uses it to communicate with the API server and request a new certificate upon expiration. The RBAC rules are consistent across all <code>ovnkube-node</code> pods, we use the <code>system:ovn-nodes</code> group as the subject for Role/ClusterRole bindings to avoid duplication: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n    name: ovnkube-node\nroleRef:\n    name: ovnkube-node\n    kind: ClusterRole\n    apiGroup: rbac.authorization.k8s.io\nsubjects:\n    - kind: Group\n      name: system:ovn-nodes\n      apiGroup: rbac.authorization.k8s.io\n</code></pre> Note: By default, the generated certificates have a brief lifetime set (10min). This is intentional as it helps ensure that the certificate rotation works seamlessly, but it might be adjusted in production environments.</p>"},{"location":"features/infrastructure-security-controls/node-identity/#validating-admission-webhook","title":"Validating Admission Webhook","text":"<p>The feature introduces a validating webhook for updates to <code>pod/status</code> (Interconnect only) and <code>node/status</code>.\\ The <code>ovnkube-node</code> pod exclusively updates the status on both resources, so it is sufficient to verify only update requests.\\ The webhooks include the following checks for each <code>ovnkube-node</code> pod: - Modifying annotations on pods hosted on its own node. - Modifying annotations on its own node. - Modifying only allowed annotations. - Not modifying anything other than annotations.</p> <p>The allowed annotations list contains both common and feature specific values:  - By default, the webhook will verify a set of common node annotations used in all deployments.  - When <code>enable-interconnect</code> parameter is provided the webhook will validate additional pod/node annotations set by the ovnkube-node component in interconnect environments.  - When <code>enable-hybrid-overlay</code> parameter is provided the webhook will validate additional node annotations set by the ovnkube-node component in interconnect environments.</p> <p>The specific annotation values can be found in <code>go-controller/pkg/ovnwebhook/nodeadmission.go</code> and <code>go-controller/pkg/ovnwebhook/podadmission.go</code> files.</p> <p>Some of the allowed annotations have additional checks; for instance, the IP addresses in k8s.ovn.org/pod-networks must match the node's k8s.ovn.org/node-subnets networks.</p>"},{"location":"features/infrastructure-security-controls/node-identity/#deployment","title":"Deployment","text":"<p>In Kind, the feature is enabled by default and can be disabled with <code>--disable-ovnkube-identity</code> when creating the cluster.\\ By default, the webhook listens on the nodes <code>localhost</code> address so there has to be an instance running on the control-plane node: <pre><code>kubectl get pod -lname=ovnkube-identity -n ovn-kubernetes  -o wide\nNAME                                READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES\novnkube-identity-57f9778d99-llfqz   1/1     Running   0          30m   172.18.0.3   ovn-control-plane   &lt;none&gt;           &lt;none&gt;\n</code></pre> This approach allows us to collocate the webhook with the API server for better response times and avoid relying on cluster networking.</p>"},{"location":"features/network-security-controls/admin-network-policy/","title":"AdminNetworkPolicy","text":"<p>Kubernetes AdminNetworkPolicy documentation: https://network-policy-api.sigs.k8s.io/</p> <p>Kubernetes AdminNetworkPolicy API reference: https://github.com/kubernetes-sigs/network-policy-api/blob/429a9e6ae89d411f89d5a16aba38a5d920c969ee/apis/v1alpha1/adminnetworkpolicy_types.go</p> <p>NOTE: This documentation focuses on OVNK's implementation of ANP and is more for developers than for users.</p> <p>NetworkPolicy API was designed mainly for namespace owners or application developers. They are thus namespace scoped. NetworkPolicy API is not suitable for administrators/network-administrators/security-operators of the cluster because of two main reasons:</p> <ul> <li>They are not cluster-scoped and hence its hard to define a network of policies that spawns across namespaces</li> <li>They cannot be created before the namespace is created (kapi server expects the namespace to be created first); thus they cannot satisfy the requirements where admins may want policies in the cluster to be in place before the workloads are even created.</li> <li>The design of NetworkPolicy API is implicit which means the deny is already set in place when we create a policy and then we are expected to do an allowList of rules in the policy. Network administrators prefer to have the power of defining what exactly to deny and allow instead of this implicit model.</li> </ul> <p>Sample API:</p> <pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: pass-example\nspec:\n  priority: 10\n  subject:\n    namespaces:\n      matchLabels:\n          conformance-house: gryffindor\n  ingress:\n  - name: \"deny-all-ingress-from-slytherin\"\n    action: \"Deny\"\n    from:\n    - namespaces:\n        namespaceSelector:\n          matchLabels:\n            conformance-house: slytherin\n  egress:\n  - name: \"deny-all-egress-to-slytherin\"\n    action: \"Deny\"\n    to:\n    - namespaces:\n        namespaceSelector:\n          matchLabels:\n            conformance-house: slytherin\n</code></pre> <p>OVN-Implementation:</p> <ul> <li>Each AdminNetworkPolicy CRD will have a <code>.spec.priority</code> field. The lower the number the higher the precedence. Thus 0 is the highest priority and 1000 (the largest number supported by upstream sig-network-policy-api) is the lowest priority. However the number of admin policies in a cluster are usually expected to be of a maximum of say 30-50 and not more than that based on use cases for which this API was defined for. OVNK plugin will support 100 policies max in a cluster. If anyone creates more than 100, it will not work properly. Thus supported priority values in OVNK are from 0 to 99.</li> <li>Each AdminNetworkPolicy CRD can have upto 100 ingress rules and 100 egress rules, thus 200 rules in total. The ordering of each rule is important. If the rule is at the top of the list then it has the highest precedence and if the rule is at the bottom it has the lowest prededence. Each rule translates to one ACL.</li> <li>Since we can have upto 100 policies and each one can have upto 100 gress rules (100*100), we have blocked out the range: 30,000 - 20,000 priority range for the OVN nbdb.ACL table in the <code>Tier1</code> block for ANP's implementation.</li> <li>Each AdminNetworkPolicy CRD will have a subject on which the policy is applied on - this is translated to one PortGroup on which the ACLs of each rules are attached on.</li> <li>Each gress rule can have upto 100 peers. Each rule will also create an nbdb.AddressSet which will contain the IPs of all the pods that are selected by the peer selector across all the peers of that given rule.</li> </ul> <p>The PortGroup for the above AdminNetworkPolicy is:</p> <pre><code>_uuid               : a10e3675-5260-4e28-9462-b705b9dac862\nacls                : [120082fa-5a70-4c72-9211-529766078278, 2e0f811f-e0db-41ad-b12b-4a0cf1c621ae]\nexternal_ids        : {AdminNetworkPolicy=pass-example}\nname                : a3052488126344707991\nports               : [a22a4c3a-bb65-4b22-8bc1-13e1e8899a7b, c7e4ffe3-73df-4db5-a3bc-a9649394d549]\n</code></pre> <p>The ACLs for the above AdminNetworkPolicy are:</p> <pre><code>_uuid               : 2e0f811f-e0db-41ad-b12b-4a0cf1c621ae\naction              : drop\ndirection           : to-lport\nexternal_ids        : {direction=ANPIngress, \"k8s.ovn.org/id\"=\"admin-network-policy-controller:AdminNetworkPolicy:pass-example:ANPIngress:29000\", \"k8s.ovn.org/name\"=pass-example, \"k8s.ovn.org/owner-controller\"=admin-network-policy-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, priority=\"29000\"}\nlabel               : 0\nlog                 : false\nmatch               : \"((ip4.src == $a10282262890368313763)) &amp;&amp; (outport == @a3052488126344707991)\"\nmeter               : acl-logging\nname                : pass-example_ANPIngress_29000\noptions             : {}\npriority            : 29000\nseverity            : debug\ntier                : 1\n===\n_uuid               : 120082fa-5a70-4c72-9211-529766078278\naction              : drop\ndirection           : from-lport\nexternal_ids        : {direction=ANPEgress, \"k8s.ovn.org/id\"=\"admin-network-policy-controller:AdminNetworkPolicy:pass-example:ANPEgress:29000\", \"k8s.ovn.org/name\"=pass-example, \"k8s.ovn.org/owner-controller\"=admin-network-policy-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, priority=\"29000\"}\nlabel               : 0\nlog                 : false\nmatch               : \"((ip4.dst == $a16426961577074298037)) &amp;&amp; (inport == @a3052488126344707991)\"\nmeter               : acl-logging\nname                : pass-example_ANPEgress_29000\noptions             : {apply-after-lb=\"true\"}\npriority            : 29000\nseverity            : debug\ntier                : 1\n</code></pre> <p>The Address-Sets for the above AdminNetworkPolicy are:</p> <pre><code>_uuid               : df47aab4-ab2c-4530-bb08-2a90476af9a7\naddresses           : [\"10.244.0.5\", \"10.244.1.6\"]\nexternal_ids        : {direction=ANPIngress, ip-family=v4, \"k8s.ovn.org/id\"=\"admin-network-policy-controller:AdminNetworkPolicy:pass-example:ANPIngress:29000:v4\", \"k8s.ovn.org/name\"=pass-example, \"k8s.ovn.org/owner-controller\"=admin-network-policy-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, priority=\"29000\"}\nname                : a10282262890368313763\n===\n_uuid               : d797804a-9401-446e-8295-1f2eebcfa80b\naddresses           : [\"10.244.0.5\", \"10.244.1.6\"]\nexternal_ids        : {direction=ANPEgress, ip-family=v4, \"k8s.ovn.org/id\"=\"admin-network-policy-controller:AdminNetworkPolicy:pass-example:ANPEgress:29000:v4\", \"k8s.ovn.org/name\"=pass-example, \"k8s.ovn.org/owner-controller\"=admin-network-policy-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, priority=\"29000\"}\nname                : a16426961577074298037\n</code></pre> <p>NOTE: Since priority is 10, it is 29000 in the ACL world. If we had a second rule, that rule would get 28999 as its priority. There are no default deny policies for a given ANP unlike NP.</p> <p>Pass Action:</p> <p>In addition to setting <code>Deny</code> and <code>Allow</code> actions on ANP API rules, one can also use the <code>Pass</code> action for a rule. What this means is ANP controller defers the decision of packets that match the pass action rule to either the NetworkPolicy OR to the BaselineAdminNetworkPolicy defined in the cluster (if either of them match the same set of pods, then they will take effect and if not, the result will be an <code>Allow</code>). Order of precedence: AdminNetworkPolicy (Tier1) &gt; NetworkPolicy(Tier2) &gt; BaselineAdminNetworkPolicy (Tier3).</p> <p>Sample Pass ACTION API:</p> <pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: pass-example\nspec:\n  priority: 10\n  subject:\n    namespaces:\n      matchLabels:\n          conformance-house: gryffindor\n  ingress:\n  - name: \"pass-all-ingress-from-slytherin\"\n    action: \"Pass\"\n    from:\n    - namespaces:\n        namespaceSelector:\n          matchLabels:\n            conformance-house: slytherin\n  egress:\n  - name: \"pass-all-egress-to-slytherin\"\n    action: \"Pass\"\n    to:\n    - namespaces:\n        namespaceSelector:\n          matchLabels:\n            conformance-house: slytherin\n</code></pre> <p>Corresponding ACLs:</p> <pre><code>_uuid               : 2e0f811f-e0db-41ad-b12b-4a0cf1c621ae\naction              : pass\ndirection           : to-lport\nexternal_ids        : {direction=ANPIngress, \"k8s.ovn.org/id\"=\"admin-network-policy-controller:AdminNetworkPolicy:pass-example:ANPIngress:29000\", \"k8s.ovn.org/name\"=pass-example, \"k8s.ovn.org/owner-controller\"=admin-network-policy-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, priority=\"29000\"}\nlabel               : 0\nlog                 : false\nmatch               : \"((ip4.src == $a10282262890368313763)) &amp;&amp; (outport == @a3052488126344707991)\"\nmeter               : acl-logging\nname                : pass-example_ANPIngress_29000\noptions             : {}\npriority            : 29000\nseverity            : debug\ntier                : 1\n===\n_uuid               : 120082fa-5a70-4c72-9211-529766078278\naction              : pass\ndirection           : from-lport\nexternal_ids        : {direction=ANPEgress, \"k8s.ovn.org/id\"=\"admin-network-policy-controller:AdminNetworkPolicy:pass-example:ANPEgress:29000\", \"k8s.ovn.org/name\"=pass-example, \"k8s.ovn.org/owner-controller\"=admin-network-policy-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, priority=\"29000\"}\nlabel               : 0\nlog                 : false\nmatch               : \"((ip4.dst == $a16426961577074298037)) &amp;&amp; (inport == @a3052488126344707991)\"\nmeter               : acl-logging\nname                : pass-example_ANPEgress_29000\noptions             : {apply-after-lb=\"true\"}\npriority            : 29000\nseverity            : debug\ntier                : 1\n</code></pre> <p>If we now define a networkpolicy that matches the same set of subjects as our pass-example admin-network-policy, that network-policy will take effect. This is how administrators can delegate a decision making to the namespace owners in a cluster.</p>"},{"location":"features/network-security-controls/admin-network-policy/#baselineadminnetworkpolicy","title":"BaselineAdminNetworkPolicy","text":"<p>Kubernetes AdminNetworkPolicy API reference: https://github.com/kubernetes-sigs/network-policy-api/blob/429a9e6ae89d411f89d5a16aba38a5d920c969ee/apis/v1alpha1/baseline_adminnetworkpolicy_types.go</p> <p>Since we can delegate decisions from administrators to namespace owners, what if namespace owners don't have policies in place for the same set of subjects? Admins in such cases might want to keep a default set of guardrails in the cluster. Thus we allow one BANP to be created in the cluster with the name <code>default</code>. The rules in <code>default</code> BANP are created in Tier3.</p> <p>Sample API:</p> <pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: BaselineAdminNetworkPolicy\nmetadata:\n  name: default\nspec:\n  subject:\n    namespaces:\n      matchLabels:\n          conformance-house: gryffindor\n  ingress:\n  - name: \"deny-all-ingress-from-slytherin\"\n    action: \"Deny\"\n    from:\n    - namespaces:\n        namespaceSelector:\n          matchLabels:\n            conformance-house: slytherin\n  egress:\n  - name: \"deny-all-egress-to-slytherin\"\n    action: \"Deny\"\n    to:\n    - namespaces:\n        namespaceSelector:\n          matchLabels:\n            conformance-house: slytherin\n</code></pre> <ul> <li>BANP doesn't have any priority field, since we can have only one in the cluster</li> <li>We keep nbdb.ACL's priority range 1750 - 1649 range reserved for BANP rules in the cluster.</li> <li>Rest of the implementation details for ANP is applicable to BANP as well.</li> </ul> <p>Corresponding ACLs:</p> <pre><code>_uuid               : 436b5a0f-9616-42b5-865d-489ec1d42666\naction              : drop\ndirection           : to-lport\nexternal_ids        : {direction=BANPIngress, \"k8s.ovn.org/id\"=\"admin-network-policy-controller:BaselineAdminNetworkPolicy:default:BANPIngress:1750\", \"k8s.ovn.org/name\"=default, \"k8s.ovn.org/owner-controller\"=admin-network-policy-controller, \"k8s.ovn.org/owner-type\"=BaselineAdminNetworkPolicy, priority=\"1750\"}\nlabel               : 0\nlog                 : false\nmatch               : \"((ip4.src == $a2535546904205657311)) &amp;&amp; (outport == @a16982411286042166782)\"\nmeter               : acl-logging\nname                : default_BANPIngress_1750\noptions             : {}\npriority            : 1750\nseverity            : debug\ntier                : 3\n===\n_uuid               : d42cb240-fac1-4429-a1bc-02efddda69cf\naction              : drop\ndirection           : from-lport\nexternal_ids        : {direction=BANPEgress, \"k8s.ovn.org/id\"=\"admin-network-policy-controller:BaselineAdminNetworkPolicy:default:BANPEgress:1750\", \"k8s.ovn.org/name\"=default, \"k8s.ovn.org/owner-controller\"=admin-network-policy-controller, \"k8s.ovn.org/owner-type\"=BaselineAdminNetworkPolicy, priority=\"1750\"}\nlabel               : 0\nlog                 : false\nmatch               : \"((ip4.dst == $a6430502402203365)) &amp;&amp; (inport == @a16982411286042166782)\"\nmeter               : acl-logging\nname                : default_BANPEgress_1750\noptions             : {apply-after-lb=\"true\"}\npriority            : 1750\nseverity            : debug\ntier                : 3\n</code></pre> <p>The Address-Set for the above BaselineAdminNetworkPolicy is:</p> <pre><code>_uuid               : a3ac7d6b-9185-4099-84f8-92d28460b4c3\naddresses           : [\"10.244.0.5\", \"10.244.1.6\"]\nexternal_ids        : {direction=BANPIngress, ip-family=v4, \"k8s.ovn.org/id\"=\"admin-network-policy-controller:BaselineAdminNetworkPolicy:default:BANPIngress:1750:v4\", \"k8s.ovn.org/name\"=default, \"k8s.ovn.org/owner-controller\"=admin-network-policy-controller, \"k8s.ovn.org/owner-type\"=BaselineAdminNetworkPolicy, priority=\"1750\"}\nname                : a2535546904205657311\n===\n_uuid               : b3e08332-e6bb-4f4e-bbc4-c36f717f149a\naddresses           : [\"10.244.0.5\", \"10.244.1.6\"]\nexternal_ids        : {direction=BANPEgress, ip-family=v4, \"k8s.ovn.org/id\"=\"admin-network-policy-controller:BaselineAdminNetworkPolicy:default:BANPEgress:1750:v4\", \"k8s.ovn.org/name\"=default, \"k8s.ovn.org/owner-controller\"=admin-network-policy-controller, \"k8s.ovn.org/owner-type\"=BaselineAdminNetworkPolicy, priority=\"1750\"}\nname                : a6430502402203365\n</code></pre> <p>The PortGroup for the above AdminNetworkPolicy is:</p> <pre><code>_uuid               : 9ec16567-6f51-49fb-aedb-40c477ad470d\nacls                : [436b5a0f-9616-42b5-865d-489ec1d42666, d42cb240-fac1-4429-a1bc-02efddda69cf]\nexternal_ids        : {BaselineAdminNetworkPolicy=default}\nname                : a16982411286042166782\nports               : [a22a4c3a-bb65-4b22-8bc1-13e1e8899a7b, c7e4ffe3-73df-4db5-a3bc-a9649394d549]\n</code></pre>"},{"location":"features/network-security-controls/admin-network-policy/#todo","title":"TODO","text":"<p>This section tracks the remaining work (some of these items are work-in-progress already and will be merged in future PRs) that are future items and outside the scope of the initial PR (https://github.com/ovn-org/ovn-kubernetes/pull/3659)</p> <ul> <li>Adding Northbound Support for ANP: https://github.com/kubernetes-sigs/network-policy-api/pull/117</li> <li>Adding support for sameLabels/notSameLabels: https://github.com/kubernetes-sigs/network-policy-api/pull/123</li> <li>Adding support for Named Ports: https://github.com/ovn-org/ovn-kubernetes/pull/3641 (Once the final design here is done will rebase)</li> <li>Adding support for Logging: (PR in progress locally, did not push till these main changes land)<ul> <li>Change to using ovn.acl package for bulding ACLs instead of libovsdb.ACL package: per comment https://github.com/ovn-org/ovn-kubernetes/pull/3659#discussion_r1257988920 if needed (although tssurya thinks using the libovsdbops function causes lesser abstracted and more straightforwardness)</li> </ul> </li> <li>Scale improvements (We will only have max 100 ANP's in a cluster, so we could get away by not doing any scale changes; depends on how pod/namespace add/updates perform.)<ul> <li>Reducing ACLs on L4 (Max ACL Count: 100x200 = 20K without ports) - with ports this can go upto 100x200x100 = 200K ACLs: https://github.com/ovn-org/ovn-kubernetes/pull/3582</li> <li>Investigating better locking (if needed after scale runs)</li> <li>Adding support for sharing address-sets with namespaces (depends on use cases as ANPs   are created before namespaces most of times and the adiquate support on the namespace   controller side needs to be added) - similar to what's done for NPs</li> <li>Adding support for sharing port-groups (depends on use cases as ANPs span   across namespaces maybe we can combine per namespace ones with an || expression   but need to see if its worth the effort): https://github.com/ovn-org/ovn-kubernetes/pull/2740</li> </ul> </li> </ul>"},{"location":"features/network-security-controls/admin-network-policy/#constraints","title":"Constraints","text":"<ul> <li>The v1alpha1 CRDs upstream support upto 1000 priorities (<code>.Spec.Priority</code>) but OVNK only allows users to have maximum 100 ANPs in a cluster.   This means you can create an ANP with priority between 0 and 99 - we do not support creating ANPs with higher priorities in OVNK.   Since each ANP can have 100 ingress and egress rules, administrators must be able to express relations using 30-50 policies max from our assumptions.   Changing this to support beyond 200 will need OVN RFEs</li> <li>It is for the best if two ANPs are not created with the same priority. The outcome is nondeterministic and this is a case we do not support. So ensure   your policies have unique priorities</li> </ul>"},{"location":"features/network-security-controls/egress-firewall/","title":"EgressFirewall","text":""},{"location":"features/network-security-controls/egress-firewall/#introduction","title":"Introduction","text":"<p>The EgressFirewall feature enables a cluster administrator to limit the external hosts that a pod in a project can access. The EgressFirewall object rules apply to all pods that share the namespace with the egressfirewall object. A namespace only supports having one EgressFirewallObject.</p>"},{"location":"features/network-security-controls/egress-firewall/#example","title":"Example","text":"<p>The yaml  below is an example of a simple egressFirewall object</p> <pre><code>kind: EgressFirewall\napiVersion: k8s.ovn.org/v1\nmetadata:\n  name: default\n  namespace: default\nspec:\n  egress:\n  - type: Allow\n    to:\n      dnsName: www.openvswitch.org\n  - type: Allow\n    to:\n      cidrSelector: 1.2.3.0/24\n  - type: Allow\n    to:\n      cidrSelector: 4.5.6.0/24\n    ports:\n      - protocol: UDP\n        port: 55\n  - type: Deny\n    to:\n      cidrSelector: 0.0.0.0/0\n</code></pre> <p>This example allows Pods in the default namespace to connect to the host(s) that www.openvswitch.org translates to, any external host within the range 1.2.3.0 to 1.2.3.255, and in addtion allows traffic to 4.5.6.0 to 4.5.6.255 only for the UDP protocol on port number 55 and denies traffic to all other external hosts. The ports  section is optional and allows the user to specify specific ports  to and protocols to allow or deny traffic.</p> <p>The priority of a rule is determined by its placement in the egress array. An earlier rule is processed before a later rule. In the  previous example, if the rules are reversed, all traffic is denied, including any traffic to hosts in the 1.2.3.0/24 CIDR block.</p> <p>Using the DNS feature assumes that the nodes and masters are located in a similar location as the DNS entries that are added to the ovn database are generated by the master.</p> <p>NOTE: use Caution when using DNS names in deny rules. The DNS interceptor will never work flawlessly and could allow access to a denied host if the DNS resolution on the node is different then in the master.</p>"},{"location":"features/network-security-controls/network-policy/","title":"NetworkPolicy","text":"<p>Kubernetes NetworkPolicy documentation: https://kubernetes.io/docs/concepts/services-networking/network-policies</p> <p>Kubernetes NetworkPolicy API reference: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#networkpolicy-v1-networking-k8s-io</p> <p>By default the network traffic from and to K8s pods is not restricted in any way. Using NetworkPolicy is a way to enforce network isolation of selected pods. When a pod is selected by a NetworkPolicy allowed traffic is specified by the <code>Ingress</code> and <code>Egress</code> sections.  </p> <p>Each NetworkPolicy object consists of four sections:</p> <ol> <li><code>podSelector</code>: a label selector that determines which pods the NetworkPolicy applies to</li> <li><code>policyTypes</code>: determines which policy types are included, if none are selected then <code>Ingress</code> will always be set and <code>Egress</code> will be set if any <code>Egress</code> rules are applied </li> <li><code>Ingress rules</code>: determines the sources that pods selected by the ingress rule can receive traffic from </li> <li><code>Egress rules</code>: determines the sinks that pods selected by the egress rule can send trafic to </li> </ol>"},{"location":"features/network-security-controls/network-policy/#networkpolicy-features","title":"NetworkPolicy features","text":"<p>These are described in order and are additive </p>"},{"location":"features/network-security-controls/network-policy/#unicast-default-deny","title":"Unicast default-deny","text":"<p>When a pod is selected by one or more NetworkPolicies, the <code>policyTypes</code> is set to both <code>Ingress</code> and <code>Egress</code>, and if no rules are specified it becomes isolated and all unicast ingress and egress traffic is blocked for pods in the same namespce as the NetworkPolicy. </p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre> <p>If only ingress traffic to all pods in a namespace needs to be blocked the following can be used </p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n</code></pre> <p>And finally if only Egress traffic from all pods in a Namespace needs to be blocked </p> <p><pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n</code></pre> OVN-Implementation:</p> <p>Every new NetworkPolicy creates a port group named <code>FOO_bar</code> where <code>FOO</code> is the policy's Namespace and <code>bar</code> is the policy's name.  All pods that the policy's <code>podSelector</code> selects are added to the port group.</p> <p>Additionally, two global deny PortGroups are also used, specifially: <code>IngressDefaultDeny</code> and <code>EgressDefaultDeny</code>.  Any pod selected by a NetworkPolicy in any Namespace is added to these PortGroups.</p> <p>subset of <code>ovn-nbctl find port-group</code> <pre><code>    _uuid               : 1deeac49-e87e-4e05-9324-beb8ef0dcef4\n    acls                : [3f864884-cdb7-44be-a60e-e4f743afc9d0, f174fcf1-a7c2-496d-9b96-136aaccc014f]\n    external_ids        : {name=ingressDefaultDeny}\n    name                : ingressDefaultDeny\n    ports               : [ce1bc4e5-0309-463f-9fd1-80f6e487e2d4]\n\n    _uuid               : 5249b7a2-36bf-4246-98ea-13d5c5e17c68\n    acls                : [36ed4154-71ab-4b8f-8119-5c4cc92708d9, c6663e29-99d0-4410-a2ff-f694a896a035]\n    external_ids        : {name=egressDefaultDeny}\n    name                : egressDefaultDeny\n    ports               : []\n</code></pre></p> <p>Two ACLs (four total) are added to each PortGroup:</p> <ol> <li>a drop policy with <code>priority=1000</code> and <code>direction=to-lport</code></li> </ol> <p>subset of <code>ovn-nbctl find ACL</code> <pre><code>   _uuid               : f174fcf1-a7c2-496d-9b96-136aaccc014f\n    action              : drop\n    direction           : to-lport\n    external_ids        : {default-deny-policy-type=Ingress}\n    log                 : false\n    match               : \"outport == @ingressDefaultDeny\"\n    meter               : []\n    name                : []\n    priority            : 1000\n    severity            : []\n</code></pre></p> <ol> <li>an allow policy for ARP traffic with <code>priority=1001</code>, <code>direction=to-lport</code>, and <code>match=arp</code></li> </ol> <p>subset of <code>ovn-nbctl find ACL</code> <pre><code>_uuid               : 3f864884-cdb7-44be-a60e-e4f743afc9d0\naction              : allow\ndirection           : to-lport\nexternal_ids        : {default-deny-policy-type=Ingress}\nlog                 : false\nmatch               : \"outport == @ingressDefaultDeny &amp;&amp; arp\"\nmeter               : []\nname                : []\npriority            : 1001\nseverity            : []\n</code></pre></p>"},{"location":"features/network-security-controls/network-policy/#applying-the-network-policy-to-specific-pods-using-specpodselector","title":"Applying the network policy to specific pods using <code>spec.podSelector</code>","text":"<p>In some cases only certain pods in a Namespace may need to be selected by a NetworkPolicy. To handle this feature the <code>spec.podSelector</code> field can be used as follows </p> <p>The <code>spec.podSelector</code> is a label selector, which can be either a list of labels (<code>app=nginx</code>) or a match expression. Only pods in the same Namespace as the NetworkPolicy can be selected by it. The end result is a list of zero or more pods to which this NetworkPolicy's <code>Ingress</code> and <code>Egress</code> sections will be applied.</p> <p>For example, to block all traffic to and from a pod labeled with <code>app=demo</code> the following can be used </p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\nspec:\n  podSelector:\n    matchLabels:\n          app: demo\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre>"},{"location":"features/network-security-controls/network-policy/#applying-ingress-and-egress-rules-using-specingress-and-specegress","title":"Applying Ingress and Egress Rules using <code>spec.Ingress</code> and <code>spec.Egress</code>","text":"<p>In some cases we need to explicilty define what sources and sinks a pod is allowed to communicate with, to handle this feature the <code>spec.Ingress</code> and <code>spec.Egress</code> fields of a NeworkPolicy can be used </p> <p>These sections contain a list of ingress or egress \"peers\" (the <code>from</code> section for <code>ingress</code> and the <code>to</code> section for <code>egress</code>) and a list of IP ports/protocols (the <code>ports</code> section) to or from which traffic should be allowed. Each list element of each section is logically OR-ed with other elements.</p> <p>Each <code>from</code>/<code>to</code> section can contain the following selectors </p> <ol> <li><code>namespaceSelector</code>: a label selector matching all pods in zero or more Namespaces</li> <li><code>podSelector</code>: a label selector matching zero or more pods in the same Namespace as the NetworkPolicy</li> <li><code>namespaceSelector</code> and <code>podSelector</code>: when both are present in an element, selects only pods matching the <code>podSelector</code> from Namespaces matching the <code>namespaceSelector</code></li> <li><code>ipBlock</code>: an IP network in CIDR notation that can be either internal or external, with optional exceptions</li> </ol>"},{"location":"features/network-security-controls/network-policy/#specingress","title":"<code>spec.ingress</code>","text":"<p>Rules defined in <code>spec.Ingress</code> can match on two main sections, 1.<code>spec.Ingress.from</code> and 2.<code>spec.Ingress.ports</code></p> <ol> <li> <p><code>spec.Ingress.from</code> </p> <p>Specifies FROM what sources a network policy will allow traffic </p> <p>It contains three selectors which are described further below </p> <ul> <li> <p><code>spec.Ingress.from.ipBlock</code> </p> <p>The ip addresses from which to allow traffic, contains fields <code>spec.Ingress.from.ipBlock.cidr</code> to specify which ip address are allowed and  <code>spec.Ingress.from.ipBlock.except</code> to specifiy which address's are not allowed </p> </li> <li> <p><code>spec.Ingress.from.namespaceSelector</code> </p> <p>The Namespaces from which to allow traffic, uses matchLabels to select much like the <code>spec.Podselector</code> field</p> </li> <li> <p><code>spec.Ingress.from.podSelector</code> </p> <p>The pods from which to allow traffic, matches the same as described above</p> </li> </ul> </li> <li> <p><code>spec.Ingress.ports</code></p> <p>The ports that the <code>Ingress</code> rule matches on, contains fields <code>spec.Ingress.ports.port</code> which can be either numerical or named, if set all port names and numbers will be matched, and <code>spec.Ingress.ports.protocol</code> matches to the protocol of the provided port </p> </li> </ol>"},{"location":"features/network-security-controls/network-policy/#specegress","title":"<code>spec.egress</code>","text":"<p>Rules defined in <code>spec.Egress</code> can match on two main sections, 1.<code>spec.Egress.to</code> and 2.<code>spec.Egress.ports</code></p> <ol> <li> <p><code>spec.Egress.to</code> </p> <p>specifies TO what destinations a network policy will allow a pod to send traffic </p> <p>It contains three selectors which are described further below </p> <ul> <li> <p><code>spec.Egress.to.ipBlock</code> </p> <p>The ip addresses which a pod can send traffic to, contains fields <code>spec.Egress.from.ipBlock.cidr</code> to specify which ip address are allowed and  <code>spec.Egress.from.ipBlock.except</code> to specifiy which address's are not allowed</p> </li> <li> <p><code>spec.Egress.to.namespaceSelector</code> </p> <p>The Namespaces allowed to receive traffic, uses matchLabels to select much like the <code>spec.Podselector</code> field</p> </li> <li> <p><code>spec.Egress.to.podSelector</code> </p> <p>The pods allowed to receive traffic, uses matchLabels to select much like described <code>spec.Podselector</code> field</p> </li> </ul> </li> <li> <p><code>spec.Egress.ports</code></p> <p>The ports that the <code>Egress</code> rule matches on, contains fields <code>spec.Egress.ports.port</code> which can be either numerical or named, if set all port names and numbers will be matched, and <code>spec.Egress.ports.protocol</code> matches to the protocol of the provided port </p> </li> </ol>"},{"location":"features/network-security-controls/network-policy/#specingress-and-specegress-ovn-implementation","title":"<code>spec.ingress</code> and <code>spec.egress</code> OVN implementation","text":"<p>Each Namespace creates an AddressSet to which the IPs of all pods in that Namespace are added. This is used in NetworkPolicy <code>Ingress</code> and <code>Egress</code> sections to implement the Namespace selector.</p> <p>Each element in the <code>from</code> or <code>to</code> list results in an AddressSet containing the IP addresses of all peer pods(i.e all pods touched by this policy) As pods are created, updated, or deleted each AddressSet is updated to add the new pod if it matches the selectors, or to remove the pod if it used to match selectors but no longer does. Namespace label changes may also result in AddressSet updates to add or remove pods if the Namespace now matches or no longer matches the <code>namespaceSelector</code>.</p> <p>If an <code>ipBlock</code> is specified, an ACL with the label <code>ipblock_cidr=\"false\"</code> is added to the policy's PortGroup with <code>priority=1001</code> that allows traffic to or from the list of CIDRs in the <code>ipBlock</code>, any exceptions are added as <code>drop</code> ACLs to the policy's PortGroup with <code>priority=1010</code>.</p> <p>Examples: </p> <p>Given two pods in Namespace <code>default</code> called  <code>client1</code> and <code>client2</code> , and one pod in Mamespace <code>demo</code>, called <code>server</code> lets make a network policy that allows ingress traffic to the server from <code>client1</code> but bocks traffic from <code>client2</code> </p> <p>Notice the pod <code>client1</code> and Namespace <code>default</code> are labeled with <code>app=demo</code></p> <pre><code>[astoycos@localhost demo]$ kubectl get pods -o wide --show-labels --all-namespaces\nNAMESPACE            NAME                                        READY   STATUS    RESTARTS   AGE     IP           NODE                NOMINATED NODE   READINESS GATES   LABELS\ndefault              client1                                     1/1     Running   0          5m5s    10.244.2.5   ovn-worker          &lt;none&gt;           &lt;none&gt;            app=demo\ndefault              client2                                     1/1     Running   0          4m59s   10.244.1.4   ovn-worker2         &lt;none&gt;           &lt;none&gt;            &lt;none&gt;\ndemo                 server                                      1/1     Running   0          42s     10.244.2.6   ovn-worker          &lt;none&gt;           &lt;none&gt;            &lt;none&gt;\n</code></pre> <pre><code>[astoycos@localhost demo]$ kubectl get namespace --show-labels\nNAME                 STATUS   AGE   LABELS\ndefault              Active   94m   ns=default\ndemo                 Active   66m   &lt;none&gt;\n</code></pre> <p>Before applying the following network policy both pods <code>client1</code> and <code>client2</code> can reach the <code>server</code> pod</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-client\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          ns: default\n      podSelector:\n        matchLabels:\n          app: demo\n</code></pre> <p>after applying this Network policy in Namespace <code>demo</code> (<code>oc create -n demo -f policy.yaml</code>) only the pod <code>client1</code> can reach the <code>server</code> pod </p> <p>NOTE: in the above definition there is only a single <code>from</code> element allowing connections from Pods labeled <code>app=demo</code> in Namespaces with the label <code>app=demo</code></p> <p>if the from section was applied as follows </p> <p><pre><code>- from:\n    - namespaceSelector:\n        matchLabels:\n          ns: demo\n    - podSelector:\n        matchLabels:\n          app: demo\n</code></pre>   Then there would be two elements in the <code>from</code> array which allows connections from Pods labeled <code>app=demo</code> OR and Pod from Namespaces with the label <code>app=demo</code></p> <p>Now let's have a look at some of the OVN resources that are created along with this Network Policy </p> <p>For all worker nodes we can see the UUIDs of the logical ports corresponding to the pods <code>client1</code>, <code>client2</code>, and <code>server</code> </p> <p>ovn-worker</p> <p><pre><code>[root@ovn-control-plane ~]# ovn-nbctl lsp-list ovn-worker\nedb290cf-b250-4699-b102-7acbb6300dc9 (default_client1)\nc754a19d-1e8c-4415-99b9-66fdcdaed196 (demo_server)\n24c789a2-fc4b-42a5-bb16-5b1c19490b50 (k8s-ovn-worker)\n484b2004-a5c1-447c-b857-eb8e524a73f3 (kube-system_coredns-f9fd979d6-qp6xd)\n45163af0-08c2-4d42-9fc7-7b0ddc935bd8 (local-path-storage_local-path-provisioner-78776bfc44-lgzdf)\n0bb378f1-4e89-46a8-a455-7a891f64c7c8 (stor-ovn-worker)\n</code></pre> ovn-worker2</p> <pre><code>[root@ovn-control-plane ~]# ovn-nbctl lsp-list ovn-worker2\nd5030b96-1163-4ed0-90f8-41b3831d2a0b (default_client2)\n4da8fb64-0a43-4d76-a7ba-18941c078862 (k8s-ovn-worker2)\n37f58eeb-8c1a-437b-8b21-bcc1337b2e3f (kube-system_coredns-f9fd979d6-628xd)\n65019041-51b2-4599-913d-7f01c8eaa394 (stor-ovn-worker2)\n</code></pre> <p>Port Groups </p> <pre><code>[root@ovn-control-plane ~]# ovn-nbctl find port-group\n_uuid               : 2b74086c-9986-4f4d-8c97-3388625230e9\nacls                : []\nexternal_ids        : {name=clusterPortGroup}\nname                : clusterPortGroup\nports               : [24c789a2-fc4b-42a5-bb16-5b1c19490b50, 4da8fb64-0a43-4d76-a7ba-18941c078862, e556e329-d624-473a-8827-f022c17a8f60]\n\n_uuid               : a132ecce-dbca-4989-87f7-96e2f0b62a2c\nacls                : [b4e57f83-8b8f-4b37-b5e5-1f82704c49c4]\nexternal_ids        : {name=demo_allow-from-client}\nname                : a13757631697825269621\nports               : [c754a19d-1e8c-4415-99b9-66fdcdaed196]\n\n_uuid               : a32d9dda-d7fb-4ae8-b6a9-3af17d62aa7f\nacls                : [510d797c-6302-4171-8a08-eeaab67063f4, f9079cce-29aa-4d1b-b36b-ca39933ad4e6]\nexternal_ids        : {name=ingressDefaultDeny}\nname                : ingressDefaultDeny\nports               : [c754a19d-1e8c-4415-99b9-66fdcdaed196]\n\n_uuid               : 896a80ff-46f7-4837-a105-7b52cee0c625\nacls                : [660b10ea-0f2e-49cb-b620-ca4218e87ac6, 9bb634ff-cb69-44b6-a64d-09147cf337b5]\nexternal_ids        : {name=egressDefaultDeny}\nname                : egressDefaultDeny\nports               : []\n</code></pre> <p>Notice that the port corresponding to the pod <code>server</code> is included in the <code>ingressDefaultDeny</code> port group </p> <p>To bypass the ingress default deny and allow traffic from pod <code>client1</code> in Namespace <code>demo</code> as specificed in the network policy, an address set is created containing the ip address for the pod <code>client1</code> </p> <p>subset of <code>ovn-nbctl find address_set</code> <pre><code>_uuid               : 7dc68ee9-9628-4a6a-83f0-a92bfa0970c6\naddresses           : [\"10.244.2.5\"]\nexternal_ids        : {name=demo.allow-from-client.ingress.0_v4}\nname                : a14783882619065065142\n</code></pre></p> <p>Finally we can see the ingress ACL that allows traffic to the <code>server</code> pod by allowing <code>ip4.src</code> traffic FROM the address's in the address set <code>a14783882619065065142</code> TO the port group <code>@a13757631697825269621</code> which contains the port <code>c754a19d-1e8c-4415-99b9-66fdcdaed196</code> (corresponding to the <code>server</code>'s logical port)</p> <p>subset of <code>ovn-nbctl find ACL</code> <pre><code>_uuid               : b4e57f83-8b8f-4b37-b5e5-1f82704c49c4\naction              : allow-related\ndirection           : to-lport\nexternal_ids        : {Ingress_num=\"0\", ipblock_cidr=\"false\", l4Match=None, namespace=demo, policy=allow-from-client, policy_type=Ingress}\nlog                 : false\nmatch               : \"ip4.src == {$a14783882619065065142} &amp;&amp; outport == @a13757631697825269621\"\nmeter               : []\nname                : []\npriority            : 1001\nseverity            : []\n</code></pre></p> <p>TODO: Add more examples(good for first PRs), specifically replicate above scenario by matching on the pod's network(<code>ip_block</code>) rather than the pod itself </p>"},{"location":"governance/CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"governance/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"governance/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"governance/CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"governance/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"governance/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement directly. Maintainers are identified in the MAINTAINERS.md file and their contact information is on their GitHub profile page. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"governance/CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"governance/CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"governance/CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"governance/CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"governance/CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"governance/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"governance/CONTRIBUTING/","title":"Contributing Guide","text":"<ul> <li>New Contributor Guide</li> <li>Ways to Contribute</li> <li>Find an Issue</li> <li>Ask for Help</li> <li>Pull Request Lifecycle</li> <li>Development Environment Setup</li> <li>Sign Your Commits</li> <li>Pull Request Checklist</li> </ul> <p>Welcome! We are glad that you want to contribute to our project! \ud83d\udc96</p> <p>As you get started, you are in the best position to give us feedback on areas of our project that we need help with including:</p> <ul> <li>Problems found during setting up a new developer environment</li> <li>Gaps in our Quickstart Guide or documentation</li> <li>Bugs in our automation scripts</li> </ul> <p>If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know!</p>"},{"location":"governance/CONTRIBUTING/#ways-to-contribute","title":"Ways to Contribute","text":"<p>We welcome many different types of contributions including:</p> <ul> <li>New features</li> <li>Builds, CI/CD</li> <li>Bug fixes</li> <li>Documentation</li> <li>Issue Triage</li> <li>Answering questions on Slack/Mailing List</li> <li>Web design</li> <li>Communications / Social Media / Blog Posts</li> <li>Release management</li> </ul> <p>Not everything happens through a GitHub pull request. Please come to our meetings or contact us and let's discuss how we can work together. </p>"},{"location":"governance/CONTRIBUTING/#come-to-meetings","title":"Come to Meetings","text":"<p>Absolutely everyone is welcome to come to any of our meetings. You never need an invite to join us. In fact, we want you to join us, even if you don\u2019t have anything you feel like you want to contribute. Just being there is enough!</p> <p>You can find out more about our meetings here. You don\u2019t have to turn on your video. The first time you come, introducing yourself is more than enough. Over time, we hope that you feel comfortable voicing your opinions, giving feedback on others\u2019 ideas, and even sharing your own ideas, and experiences.</p>"},{"location":"governance/CONTRIBUTING/#find-an-issue","title":"Find an Issue","text":"<p>We have good first issues for new contributors and help wanted issues suitable for any contributor. good first issue has extra information to help you make your first contribution. help wanted are issues suitable for someone who isn't a core maintainer and is good to move onto after your first pull request.</p> <p>Sometimes there won\u2019t be any issues with these labels. That\u2019s ok! There is likely still something for you to work on. If you want to contribute but you don\u2019t know where to start or can't find a suitable issue, you can you can reach out to us on Slack and we will be happy to help.</p> <p>Once you see an issue that you'd like to work on, please post a comment saying that you want to work on it. Something like \"I want to work on this\" is fine.</p>"},{"location":"governance/CONTRIBUTING/#ask-for-help","title":"Ask for Help","text":"<p>The best way to reach us with a question when contributing is to ask on:</p> <ul> <li>The original github issue</li> <li>The developer mailing list (mailing-list: https://groups.google.com/g/ovn-kubernetes)</li> <li>Our Slack channel (workspace: https://ovn-org.slack.com/, channel: #ovn-kubernetes)</li> </ul>"},{"location":"governance/CONTRIBUTING/#pull-request-lifecycle","title":"Pull Request Lifecycle","text":"<ol> <li>When you open a PR a maintainer will automatically be assigned for review</li> <li>Make sure that your PR is passing CI - if you need help with failing checks please feel free to ask!</li> <li>Once it is passing all CI checks, a maintainer will review your PR and you may be asked to make changes.</li> <li>When you have received at least one approval from a maintainer, that maintainer will merge your PR.</li> </ol> <p>In some cases, other changes may conflict with your PR. If this happens, you will get notified by a comment in the issue that your PR requires a rebase, and the <code>needs-rebase</code> label will be applied. Once a rebase has been performed, this label will be automatically removed.</p>"},{"location":"governance/CONTRIBUTING/#development-environment-setup","title":"Development Environment Setup","text":"<p>You can easily setup a developer environment by following the instructions here.</p>"},{"location":"governance/CONTRIBUTING/#sign-your-commits","title":"Sign Your Commits","text":""},{"location":"governance/CONTRIBUTING/#dco","title":"DCO","text":"<p>Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project.</p> <p>You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit.</p> <pre><code>This is my commit message\n\nSigned-off-by: Your Name &lt;your.name@example.com&gt;\n</code></pre> <p>Git has a <code>-s</code> command line option to do this automatically:</p> <pre><code>git commit -s -m 'This is my commit message'\n</code></pre> <p>If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running </p> <pre><code>git commit --amend -s\n</code></pre>"},{"location":"governance/CONTRIBUTING/#logical-grouping-of-commits","title":"Logical Grouping of Commits","text":"<p>It is a recommended best practice to keep your changes as logically grouped as possible within individual commits. If while you're developing you prefer doing a number of commits that are \"checkpoints\" and don't represent a single logical change, please squash those together before asking for a review. When addressing review comments, please perform an interactive rebase and edit commits directly rather than adding new commits with messages like \"Fix review comments\".</p>"},{"location":"governance/CONTRIBUTING/#commit-message-guidelines","title":"Commit message guidelines","text":"<p>A good commit message should describe what changed and why.</p> <ol> <li> <p>The first line should:</p> </li> <li> <p>contain a short description of the change (preferably 50 characters or less,     and no more than 72 characters)</p> </li> <li>be entirely in lowercase with the exception of proper nouns, acronyms, and     the words that refer to code, like areas/function/variable names</li> <li>be prefixed with the name of the sub component being changed</li> </ol> <p>Examples:</p> <ul> <li>networkpolicy: validate ipBlock strictly</li> <li>egressip: fix frequently rebalancing IPs</li> <li> <p>services: fix etp=local + session-affnity integration</p> </li> <li> <p>Keep the second line blank.</p> </li> <li>Wrap all other lines at 72 columns (except for long URLs).</li> <li>If your patch fixes an open issue, you can add a reference to it at the end    of the log. Use the <code>Fixes: #</code> prefix and the issue number. For other    references use <code>Refs: #</code>. <code>Refs</code> may include multiple issues, separated by a    comma.</li> </ul> <p>Examples:</p> <ul> <li><code>Fixes: #1337</code></li> <li><code>Refs: #1234</code></li> </ul> <p>Sample complete commit message:</p> <pre><code>subcomponent: explain the commit in one line\n\nBody of commit message is a few lines of text, explaining things\nin more detail, possibly giving some background about the issue\nbeing fixed, etc.\n\nThe body of the commit message can be several paragraphs, and\nplease do proper word-wrap and keep columns shorter than about\n72 characters or so. That way, `git log` will show things\nnicely even when it is indented.\n\nFixes: #1337\nRefs: #453, #154\n</code></pre>"},{"location":"governance/CONTRIBUTING/#pull-request-checklist","title":"Pull Request Checklist","text":"<p>When you submit your pull request, or you push new commits to it, our automated systems will run some checks on your new code. We require that your pull request passes these checks, but we also have more criteria than just that before we can accept and merge it. We recommend that you check the following things locally before you submit your code:</p> <ul> <li>Verify that Go code has been formatted and linted</li> </ul> <pre><code>cd ovn-kubernetes/go-controller/\nmake lint\n</code></pre> <ul> <li>If you are introducing new CRDs verify that Yaml files have been formatted (see   codegen generator)</li> <li>Verify that unit tests are passing locally</li> </ul> <pre><code>cd ovn-kubernetes/go-controller/\nmake test\n</code></pre> <ul> <li> <p>All modular changes must be accompanied by new unit tests if they don't exist already.</p> </li> <li> <p>All functional changes and new features must be accompanied by extensive end-to-end test coverage</p> </li> </ul>"},{"location":"governance/GOVERNANCE/","title":"ovn-kubernetes Project Governance","text":"<p>The ovn-kubernetes  project is dedicated to creating a robust Kubernetes Networking platform built from the ground up by leveraging Open vSwitch (OVS) as the data plane, and Open Virtual Network (OVN) as the SDN Controller. The project focuses strictly on enhancing networking for the Kubernetes platform and includes a wide variety of features that are critical to enterprise and telco users.</p> <p>This governance explains how the project is run.</p> <ul> <li>Values</li> <li>Maintainers</li> <li>Becoming a Maintainer</li> <li>Meetings</li> <li>CNCF Resources</li> <li>Code of Conduct Enforcement</li> <li>Security Response Team</li> <li>Voting</li> <li>Modifications</li> </ul>"},{"location":"governance/GOVERNANCE/#values","title":"Values","text":"<p>The ovn-kubernetes and its leadership embrace the following values:</p> <ul> <li> <p>Openness: Communication and decision-making happens in the open and is discoverable for future   reference. As much as possible, all discussions and work take place in public   forums and open repositories.</p> </li> <li> <p>Fairness: All stakeholders have the opportunity to provide feedback and submit   contributions, which will be considered on their merits.</p> </li> <li> <p>Community over Product or Company: Sustaining and growing our community takes   priority over shipping code or sponsors' organizational goals.  Each   contributor participates in the project as an individual.</p> </li> <li> <p>Inclusivity: We innovate through different perspectives and skill sets, which   can only be accomplished in a welcoming and respectful environment.</p> </li> <li> <p>Participation: Responsibilities within the project are earned through   participation, and there is a clear path up the contributor ladder into leadership   positions.</p> </li> </ul>"},{"location":"governance/GOVERNANCE/#maintainers","title":"Maintainers","text":"<p>ovn-kubernetes Maintainers have write access to the project GitHub repository. They can merge their own patches or patches from others. The current maintainers can be found in MAINTAINERS.md.  Maintainers collectively manage the project's resources and contributors.</p> <p>This privilege is granted with some expectation of responsibility: maintainers are people who care about the ovn-kubernetes project and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests).</p> <p>A maintainer is a contributor to the project's success and a citizen helping the project succeed.</p> <p>The collective team of all Maintainers is known as the Maintainer Council, which is the governing body for the project.</p>"},{"location":"governance/GOVERNANCE/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>To become a Maintainer you need to demonstrate the following:</p> <ul> <li>commitment to the project:<ul> <li>participate in discussions, contributions, code and documentation reviews   for 10 months or more,</li> <li>perform reviews for 10 non-trivial pull requests,</li> <li>contribute 15 non-trivial pull requests and have them merged,</li> </ul> </li> <li>ability to write quality code and/or documentation,</li> <li>ability to collaborate with the team,</li> <li>understanding of how the team works (policies, processes for testing and code review, etc),</li> <li>understanding of the project's code base and coding and documentation style.</li> </ul> <p>A new Maintainer must be proposed by an existing maintainer by sending a message to the developer mailing list. A simple majority vote of existing Maintainers approves the application.  Maintainers nominations will be evaluated without prejudice to employer or demographics.</p> <p>Maintainers who are selected will be granted the necessary GitHub rights.</p>"},{"location":"governance/GOVERNANCE/#removing-a-maintainer","title":"Removing a Maintainer","text":"<p>Maintainers may resign at any time if they feel that they will not be able to continue fulfilling their project duties.</p> <p>Maintainers may also be removed after being inactive, failure to fulfill their  Maintainer responsibilities, violating the Code of Conduct, or other reasons. Inactivity is defined as a period of very low or no activity in the project  for a year or more, with no definite schedule to return to full Maintainer  activity.</p> <p>A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers.</p> <p>Depending on the reason for removal, a Maintainer may be converted to Emeritus status.  Emeritus Maintainers will still be consulted on some project matters, and can be rapidly returned to Maintainer status if their availability changes.</p>"},{"location":"governance/GOVERNANCE/#meetings","title":"Meetings","text":"<p>Time zones permitting, Maintainers are expected to participate in the public developer meeting, details of which can be found here.  </p> <p>Maintainers will also have closed meetings in order to discuss security reports or Code of Conduct violations.  Such meetings should be scheduled by any Maintainer on receipt of a security issue or CoC report.  All current Maintainers must be invited to such closed meetings, except for any Maintainer who is accused of a CoC violation.</p>"},{"location":"governance/GOVERNANCE/#code-of-conduct","title":"Code of Conduct","text":"<p>Code of Conduct violations by community members will be discussed and resolved on the private Slack Maintainer channel.</p>"},{"location":"governance/GOVERNANCE/#security-response-team","title":"Security Response Team","text":"<p>The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves.  If this responsibility is delegated, the Maintainers will appoint a team of at least two  contributors to handle it.  The Maintainers will review who is assigned to this at least once a year.</p> <p>The Security Response Team is responsible for handling all reports of security holes and breaches according to the security policy.</p>"},{"location":"governance/GOVERNANCE/#voting","title":"Voting","text":"<p>While most business in ovn-kubernetes is conducted by \"lazy consensus\",  periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the developer mailing list or the private Maintainer Slack Channel for security or conduct matters. Votes may also be taken at the developer meeting.  Any Maintainer may demand a vote be taken.</p> <p>Most votes require a simple majority of all Maintainers to succeed, except where otherwise noted.  Two-thirds majority votes mean at least two-thirds of all  existing maintainers.</p>"},{"location":"governance/GOVERNANCE/#modifying-this-charter","title":"Modifying this Charter","text":"<p>Changes to this Governance and its supporting documents may be approved by  a 2/3 vote of the Maintainers.</p>"},{"location":"governance/MAINTAINERS/","title":"MAINTAINERS","text":"<p>The current Maintainers Group for the ovn-kubernetes Project consists of:</p> Name Employer Responsibilities Dan Williams Red Hat All things ovnkube Girish Moodalbail NVIDIA All things ovnkube Jaime Caama\u00f1o Ruiz Red Hat All things ovnkube Surya Seetharaman Red Hat All things ovnkube Tim Rozet Red Hat All things ovnkube <p>See CONTRIBUTING.md for general contribution guidelines. See GOVERNANCE.md for governance guidelines and maintainer responsibilities.</p>"},{"location":"governance/MEETINGS/","title":"ovn-kubernetes Community Meetings","text":""},{"location":"governance/MEETINGS/#information","title":"Information","text":"<p>All are welcome to join our meetings! If you want to discuss something with the wider community, please add your items to the agenda. If you are starting to contribute but unsure of the first steps, please feel free to reach out to us during the meetings and introduce yourself!</p>"},{"location":"governance/MEETINGS/#meeting-time","title":"Meeting time","text":"<p>We meet alternate Monday's at 10:00 AM US Pacific Time. In order to figure out when our next meeting is, please check our agenda for previous meeting history. The meetings last up to 1 hour.</p>"},{"location":"governance/MEETINGS/#meeting-location","title":"Meeting location","text":"<ul> <li>Video call link: https://meet.google.com/tgr-pqke-cen</li> <li>Or dial: (DE) +49 30 300195060 PIN: 846 145 117 7213#</li> <li>More phone numbers: https://tel.meet/tgr-pqke-cen?pin=8461451177213</li> </ul>"},{"location":"governance/MEETINGS/#meeting-agenda-and-minutes","title":"Meeting agenda and minutes","text":"<p>Meeting agenda  - edit responsibly</p>"},{"location":"governance/REVIEWING/","title":"Reviewing Guide","text":"<p>This document covers who may review pull requests for this project, and provides guidance on how to perform code reviews that meet our community standards and code of conduct. All reviewers must read this document and agree to follow the project review guidelines. Reviewers who do not follow these guidelines may have their privileges revoked.</p>"},{"location":"governance/REVIEWING/#the-reviewer-role","title":"The Reviewer Role","text":"<p>We welcome all contributors to wear their reviewer hats! The reviewer role is distinct from the approver/maintainer role. Anyone is welcome to be a reviewer and take on the reviewer role in our community. Reviewers can LGTM a pull request but they cannot merge it. A maintainer/approver handles the final approval and merging of the pull request. The current maintainers can be found in MAINTAINERS.md.</p>"},{"location":"governance/REVIEWING/#values","title":"Values","text":"<p>All reviewers must abide by the Code of Conduct and are also protected by it. A reviewer should not tolerate poor behavior and is encouraged to report any behavior that violates the Code of Conduct. All of our values listed above are distilled from our Code of Conduct.</p> <p>Below are concrete examples of how it applies to code review specifically:</p>"},{"location":"governance/REVIEWING/#inclusion","title":"Inclusion","text":"<p>Be welcoming and inclusive. You should proactively ensure that the author is successful. While any particular pull request may not ultimately be merged, overall we want people to have a great experience and be willing to contribute again. Answer the questions they didn't know to ask or offer concrete help when they appear stuck.</p>"},{"location":"governance/REVIEWING/#sustainability","title":"Sustainability","text":"<p>Avoid burnout by enforcing healthy boundaries. Here are some examples of how a reviewer is encouraged to act to take care of themselves:</p> <ul> <li>Authors should meet baseline expectations when submitting a pull request, such as writing tests and relevant documentation.</li> <li>If your availability changes, you can step down from a pull request and have someone else assigned.</li> <li>If interactions with an author are not following code of conduct, raise it up with your Code of Conduct committee or point of contact. It's not your job to coax people into behaving.</li> <li>The code of conduct committee for this project is the same as the maintainers list for this project. The current maintainers can be found in MAINTAINERS.md. If you face any issues please reach out to one of the maintainers on our slack channel (workspace: https://ovn-org.slack.com/, channel: #ovn-kubernetes)</li> </ul>"},{"location":"governance/REVIEWING/#trust","title":"Trust","text":"<p>Be trustworthy. During a review, your actions both build and help maintain the trust that the community has placed in this project. Below are examples of ways that we build trust:</p> <ul> <li>Transparency - If a pull request won't be merged or shouldn't be merged, clearly say why and tag a maintainer to close it. If a pull request won't be reviewed for a while, let the author know so they can set expectations and understand why it's blocked.</li> <li>Integrity - Put the project's best interests ahead of personal relationships or company affiliations when deciding if a change should be merged.</li> <li>Stability - Only LGTM when then change won't negatively impact project stability. It can be tempting to LGTM a pull request that doesn't meet our quality standards, for example when the review has been delayed, or because we are trying to deliver new features quickly, but regressions can significantly hurt trust in our project.</li> </ul>"},{"location":"governance/REVIEWING/#process","title":"Process","text":"<ul> <li>Approvers are automatically assigned based on the CODEOWNERS file.</li> <li>Reviewers should wait for automated checks to pass before reviewing</li> <li>At least 1 approved review is required from a maintainer before a pull request can be merged</li> <li>All CI checks must pass</li> <li>If a PR is stuck for some reason it is down to the reviewer to determine the best course of action:</li> <li>PRs may be closed if they are no longer relevant</li> <li>A maintainer may choose to carry a PR forward on their own, but they should ALWAYS include the original author's commits</li> <li>A maintainer may choose to open additional PRs to help lay a foundation on which the stuck PR can be unstuck. They may either rebase the stuck PR themselves or leave this to the author</li> <li>Maintainers should not merge their pull requests without a review</li> <li>Maintainers should let the Mergify bot merge PRs and not merge PRs directly</li> <li>In times of need, i.e. to fix pressing security issues or fix critical panic issues, the Maintainers may, at their discretion, merge PRs without review. They must add a comment to the PR explaining why they did so.</li> </ul>"},{"location":"governance/REVIEWING/#checklist","title":"Checklist","text":"<p>Below are a set of common questions that apply to all pull requests:</p> <ul> <li>[ ] Is this PR targeting the correct branch?</li> <li>[ ] Does the commit message provide an adequate description of the change?</li> <li>[ ] Does the affected code have corresponding unit, end-to-end and feature integration tests?</li> <li>[ ] Are the changes documented, not just with inline documentation, but also with conceptual documentation such as an overview of a new feature, or task-based documentation like a tutorial? Consider if this change should be announced on your project blog.</li> <li>[ ] Does this introduce breaking changes that would require an announcement or bumping the major version?</li> </ul>"},{"location":"governance/REVIEWING/#reading-list","title":"Reading List","text":"<p>Reviewers are encouraged to read the following articles for help with common reviewer tasks:</p> <ul> <li>The Art of Closing: How to closing an unfinished or rejected pull request</li> <li>Kindness and Code Reviews: Improving the Way We Give Feedback</li> <li>Code Review Guidelines for Humans: Examples of good and back feedback</li> </ul>"},{"location":"governance/SECURITY/","title":"Security Policy","text":"<p>OVNKubernetes repo uses the dependabot which does automatic security updates by scanning the repo and opening PRs to update the effected libraries.</p>"},{"location":"governance/SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>To report a vulnerability, please use the Private Vulnerability Reporting Feature on GitHub. We will endevour to respond within 48hrs of reporting. If a vulnerability is reported but considered low priority it may be converted into an issue and handled on the public issue tracker. Should a vulnerability be considered severe we will endeavour to patch it within 48hrs of acceptance, and may ask for you to collaborate with us on a temporary private fork of the repository.</p>"},{"location":"installation/INSTALL.KUBEADM/","title":"INSTALL.KUBEADM","text":"<p>The following is a walkthrough for an installation in an environment with 4 virtual machines, and a cluster deployed with <code>kubeadm</code>. This shall serve as a guide for people who are curious enough to deploy OVN Kubernetes on a manually created cluster and to play around with the components. </p> <p>Note that the resulting environment might be highly unstable.</p> <p>If your goal is to set up an environment quickly or to set up a development environment, see the kind installation documentation instead.</p>"},{"location":"installation/INSTALL.KUBEADM/#environment-setup","title":"Environment setup","text":""},{"location":"installation/INSTALL.KUBEADM/#overview","title":"Overview","text":"<p>The environment consists of 4 libvirt/qemu virtual machines, all deployed with Rocky Linux 8 or CentOS 8. <code>node1</code> will serve as the sole master node and nodes <code>node2</code> and <code>node3</code> as the worker nodes. <code>gw1</code> will be the default gateway for the cluster via the <code>Isolated Network</code>. It will also host an HTTP registry to store the OVN Kubernetes images.</p> <pre><code>       to hypervisor         to hypervisor         to hypervisor\n             \u2502                     \u2502                     \u2502\n             \u2502                     \u2502                     \u2502\n           \u250c\u2500\u2534\u2500\u2510                 \u250c\u2500\u2534\u2500\u2510                 \u250c\u2500\u2534\u2500\u2510\n           \u2502if1\u2502                 \u2502if1\u2502                 \u2502if1\u2502\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502               \u2502     \u2502               \u2502     \u2502               \u2502\n     \u2502               \u2502     \u2502               \u2502     \u2502               \u2502\n     \u2502     node1     \u2502     \u2502     node2     \u2502     \u2502     node3     \u2502\n     \u2502               \u2502     \u2502               \u2502     \u2502               \u2502\n     \u2502               \u2502     \u2502               \u2502     \u2502               \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502if2\u2502                 \u2502if2\u2502                 \u2502if2\u2502\n           \u2514\u2500\u252c\u2500\u2518                 \u2514\u2500\u252c\u2500\u2518                 \u2514\u2500\u252c\u2500\u2518\n             \u2502                     \u2502                     \u2502\n             \u2502                     \u2502                     \u2502\n             \u2502                    xxxxxxxx               \u2502\n             \u2502                 xxx       xxx             \u2502\n             \u2502                xx           xx            \u2502\n             \u2502               x   Isolated   x            \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500x     Network   x\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            xxx            x\n                              xxxxxx  xxxxx\n                                   xxxx\n                                   \u2502\n                                 \u250c\u2500\u2534\u2500\u2510\n                                 \u2502if2\u2502\n                           \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502               \u2502\n                           \u2502               \u2502\n                           \u2502      gw1      \u2502\n                           \u2502               \u2502\n                           \u2502               \u2502\n                           \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502if1\u2502\n                                 \u2514\u2500\u252c\u2500\u2518\n                                   \u2502\n                                   \u2502\n                              to hypervisor\n</code></pre> <p>Legend: * if1 - enp1s0 | 192.168.122.0/24 * if2 - enp7s0 | 192.168.123.0/24</p> <p><code>to hypervisor</code> is libvirt's default network with full DHCP. It will be used as management access to all nodes as well as on <code>gw1</code> as the interface for outside connectivity: <pre><code>$ sudo virsh net-dumpxml default\n&lt;network connections='2'&gt;\n  &lt;name&gt;default&lt;/name&gt;\n  &lt;uuid&gt;76b7e8c1-7c2c-456b-ac10-09c98c6275a5&lt;/uuid&gt;\n  &lt;forward mode='nat'&gt;\n    &lt;nat&gt;\n      &lt;port start='1024' end='65535'/&gt;\n    &lt;/nat&gt;\n  &lt;/forward&gt;\n  &lt;bridge name='virbr0' stp='on' delay='0'/&gt;\n  &lt;mac address='52:54:00:4b:4d:f8'/&gt;\n  &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;\n    &lt;dhcp&gt;\n      &lt;range start='192.168.122.2' end='192.168.122.254'/&gt;\n    &lt;/dhcp&gt;\n  &lt;/ip&gt;\n&lt;/network&gt;\n</code></pre></p> <p>And <code>Isolated Network</code> is an isolated network. <code>gw1</code> will be the default gateway for this network, and <code>node1</code> through <code>node3</code> will have their default route go through this network: <pre><code>$ sudo virsh net-dumpxml ovn\n&lt;network connections='2'&gt;\n  &lt;name&gt;ovn&lt;/name&gt;\n  &lt;uuid&gt;fecea98b-8b92-438e-a759-f6cfb366614c&lt;/uuid&gt;\n  &lt;bridge name='virbr2' stp='on' delay='0'/&gt;\n  &lt;mac address='52:54:00:d4:f2:cc'/&gt;\n  &lt;domain name='ovn'/&gt;\n&lt;/network&gt;\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#gateway-setup-gw1","title":"Gateway setup (gw1)","text":"<p>Deploy the gateway virtual machine first. Set it up as a simple gateway which will NAT everything that comes in on interface enp7s0: <pre><code>IF1=enp1s0\nIF2=enp7s0\nhostnamectl set-hostname gw1\nnmcli conn mod ${IF1} connection.autoconnect yes\nnmcli conn mod ${IF2} ipv4.address 192.168.123.254/24\nnmcli conn mod ${IF2} ipv4.method static\nnmcli conn mod ${IF2} connection.autoconnect yes\nnmcli conn reload\nsystemctl stop firewalld\ncat /proc/sys/net/ipv4/ip_forward\nsysctl -a | grep ip_forward\necho \"net.ipv4.ip_forward=1\" &gt;&gt; /etc/sysctl.d/99-sysctl.conf\nsysctl --system\nyum install iptables-services -y\nyum remove firewalld -y\nsystemctl enable --now iptables\niptables-save\niptables -t nat -I POSTROUTING --src 192.168.123.0/24  -j MASQUERADE\niptables -I FORWARD --j ACCEPT\niptables -I INPUT -p tcp --dport 5000 -j ACCEPT\niptables-save &gt; /etc/sysconfig/iptables\n</code></pre></p> <p>Also set up an HTTP registry: <pre><code>yum install podman -y\nmkdir -p /opt/registry/data\npodman run --name mirror-registry \\\n  -p 5000:5000 -v /opt/registry/data:/var/lib/registry:z      \\\n  -d docker.io/library/registry:2\npodman generate systemd --name mirror-registry &gt; /etc/systemd/system/mirror-registry-container.service\nsystemctl daemon-reload\nsystemctl enable --now mirror-registry-container\n</code></pre></p> <p>Now, reboot the gateway: <pre><code>reboot\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#node1-through-node3-base-setup","title":"node1 through node3 base setup","text":"<p>You must install Open vSwitch on <code>node1</code> through <code>node3</code>. You will then connect <code>enp7s0</code> to an OVS bridge called <code>br-ex</code>. This bridge will be used later by OVN Kubernetes. Furthermore, you  must assign IP addresses to <code>br-ex</code> and point the nodes' default route via <code>br-ex</code> to <code>gw1</code>. </p>"},{"location":"installation/INSTALL.KUBEADM/#set-hostnames","title":"Set hostnames","text":"<p>Set the hostnames manually, even if they are set correctly by DHCP. Set them manually to: <pre><code>hostnamectl set-hostname node&lt;x&gt;\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#disable-swap","title":"Disable swap","text":"<p>Make sure to disable swap. Kubelet will not run otherwise: <pre><code>sed -i '/ swap /d' /etc/fstab\nreboot\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#remove-firewalld","title":"Remove firewalld","text":"<p>Make sure to uninstall firewalld. Otherwise, it will block the kubernetes management ports (that can easily be fixed by configuration) and it will also preempt and block the OVN Kubernetes installed NAT and FORWARD rules (this is more difficult to remediate). The easiest fix is hence not to use firewalld at all: <pre><code>systemctl disable --now firewalld\nyum remove -y firewalld\n</code></pre></p> <p>For more details, see https://gitmemory.com/issue/firewalld/firewalld/767/790687269; this is about Calico, but it highlights the same issue.</p>"},{"location":"installation/INSTALL.KUBEADM/#install-open-vswitch","title":"Install Open vSwitch","text":"<p>Install Open vSwitch from https://wiki.centos.org/SpecialInterestGroup/NFV</p>"},{"location":"installation/INSTALL.KUBEADM/#on-centos","title":"On CentOS:","text":"<pre><code>yum install centos-release-nfv-openvswitch -y\nyum install openvswitch2.13 --nobest -y\nyum install NetworkManager-ovs.x86_64 -y\nsystemctl enable --now openvswitch\n</code></pre>"},{"location":"installation/INSTALL.KUBEADM/#on-rocky-linux","title":"On Rocky Linux","text":"<p>Rocky doesn't have access to CentOS's repositories. However, you can still use the CentOS NFV repositories: <pre><code>rpm -ivh http://mirror.centos.org/centos/8-stream/extras/x86_64/os/Packages/centos-release-nfv-common-1-3.el8.noarch.rpm --nodeps\nrpm -ivh http://mirror.centos.org/centos/8-stream/extras/x86_64/os/Packages/centos-release-nfv-openvswitch-1-3.el8.noarch.rpm\nyum install openvswitch2.13 --nobest -y\nyum install NetworkManager-ovs.x86_64 -y\nsystemctl enable --now openvswitch\n</code></pre></p> <p>Alternatively, on Rocky Linux, you can also build your own RPMs directly from the SRPMs, e.g.: <pre><code>yum install '@Development Tools'\nyum install desktop-file-utils libcap-ng-devel libmnl-devel numactl-devel openssl-devel python3-devel python3-pyOpenSSL python3-setuptools python3-sphinx rdma-core-devel unbound-devel -y\nrpmbuild --rebuild  http://ftp.redhat.com/pub/redhat/linux/enterprise/8Base/en/Fast-Datapath/SRPMS/openvswitch2.13-2.13.0-79.el8fdp.src.rpm\nyum install selinux-policy-devel -y\nrpmbuild --rebuild http://ftp.redhat.com/pub/redhat/linux/enterprise/8Base/en/Fast-Datapath/SRPMS/openvswitch-selinux-extra-policy-1.0-28.el8fdp.src.rpm\nyum localinstall /root/rpmbuild/RPMS/noarch/openvswitch-selinux-extra-policy-1.0-28.el8.noarch.rpm /root/rpmbuild/RPMS/x86_64/openvswitch2.13-2.13.0-79.el8.x86_64.rpm -y\nyum install NetworkManager-ovs.x86_64 -y\nsystemctl enable --now openvswitch\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#configure-networking","title":"Configure networking","text":"<p>Set up networking: <pre><code>BRIDGE_NAME=br-ex\nIF1=enp1s0\nIF2=enp7s0\nIP_ADDRESS=\"192.168.123.$(hostname | sed 's/node//')/24\"\n</code></pre></p> <p>Verify the <code>IP_ADDRESS</code> - it should be unique for every node and the last octet should be the same as the node's numeric identifier: <pre><code>echo $IP_ADDRESS\n</code></pre></p> <p>Then, continue: <pre><code>nmcli c add type ovs-bridge conn.interface ${BRIDGE_NAME} con-name ${BRIDGE_NAME}\nnmcli c add type ovs-port conn.interface ${BRIDGE_NAME} master ${BRIDGE_NAME} con-name ovs-port-${BRIDGE_NAME}\nnmcli c add type ovs-interface slave-type ovs-port conn.interface ${BRIDGE_NAME} master ovs-port-${BRIDGE_NAME}  con-name ovs-if-${BRIDGE_NAME}\nnmcli c add type ovs-port conn.interface ${IF2} master ${BRIDGE_NAME} con-name ovs-port-${IF2}\nnmcli c add type ethernet conn.interface ${IF2} master ovs-port-${IF2} con-name ovs-if-${IF2}\nnmcli conn delete ${IF2}\nnmcli conn mod ${BRIDGE_NAME} connection.autoconnect yes\nnmcli conn mod ovs-if-${BRIDGE_NAME} connection.autoconnect yes\nnmcli conn mod ovs-if-${IF2} connection.autoconnect yes\nnmcli conn mod ovs-port-${IF2} connection.autoconnect yes\nnmcli conn mod ovs-port-${BRIDGE_NAME} connection.autoconnect yes\nnmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.address ${IP_ADDRESS}\nnmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.method static\nnmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.route-metric 50\n\n# move the default route to br-ex\nBRIDGE_NAME=br-ex\nnmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.gateway \"192.168.123.254\"\nnmcli conn mod ${IF1} ipv4.never-default yes\n# Change DNS to 8.8.8.8\nnmcli conn mod ${IF1} ipv4.ignore-auto-dns yes\nnmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.dns \"8.8.8.8\"\n</code></pre></p> <p>Now, reboot the node: <pre><code>reboot\n</code></pre></p> <p>After the reboot, you should see something like this, for example on node1: <pre><code>[root@node1 ~]# cat /etc/resolv.conf \n# Generated by NetworkManager\nnameserver 8.8.8.8\n[root@node1 ~]# ovs-vsctl show\nc1aee179-b425-4b48-8648-dd8746f59add\n    Bridge br-ex\n        Port enp7s0\n            Interface enp7s0\n                type: system\n        Port br-ex\n            Interface br-ex\n                type: internal\n    ovs_version: \"2.13.4\"\n[root@node1 ~]# ip r\ndefault via 192.168.123.254 dev br-ex proto static metric 800 \n192.168.122.0/24 dev enp1s0 proto kernel scope link src 192.168.122.205 metric 100 \n192.168.123.0/24 dev br-ex proto kernel scope link src 192.168.123.1 metric 800 \n[root@node1 ~]# ip a ls dev br-ex\n6: br-ex: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/ether 26:98:69:4a:d7:43 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.123.1/24 brd 192.168.123.255 scope global noprefixroute br-ex\n       valid_lft forever preferred_lft forever\n    inet6 fe80::4a1d:4d35:7c28:1ff2/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n[root@node1 ~]# nmcli conn\nNAME             UUID                                  TYPE           DEVICE \novs-if-br-ex     d434980e-ea23-4ab4-8414-289b7af44c50  ovs-interface  br-ex  \nenp1s0           52060cdd-913e-4df8-9e9e-776f31647323  ethernet       enp1s0 \nbr-ex            950f405f-cd5c-4d51-b2ab-3d8e1e938c8b  ovs-bridge     br-ex  \novs-if-enp7s0    0279d1c9-212c-4be8-8dfe-88a7b0b6d623  ethernet       enp7s0 \novs-port-br-ex   3b47e5ae-a27a-4522-bea5-1fbf9c8c08eb  ovs-port       br-ex  \novs-port-enp7s0  1baea5a3-09ee-4972-8f6b-bb8195ae46c4  ovs-port       enp7s0 \n</code></pre></p> <p>And you should be able to ping outside of the cluster: <pre><code>[root@node1 ~]# ping -c1 -W1 8.8.8.8\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=112 time=18.5 ms\n\n--- 8.8.8.8 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 18.506/18.506/18.506/0.000 ms\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#install-container-runtime-engine-and-kubeadm-node1-node2-node3","title":"Install container runtime engine and kubeadm (node1, node2, node3)","text":"<p>The following will be a brief walkthrough of what's requried to install the container runtime and kubernetes. For further details, follow the <code>kubeadm</code> documentation: * https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ * https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</p>"},{"location":"installation/INSTALL.KUBEADM/#install-the-container-runtime","title":"Install the container runtime","text":"<p>See https://kubernetes.io/docs/setup/production-environment/container-runtimes/ for further details.</p> <p>Set up iptables: <pre><code># Create the .conf file to load the modules at bootup\ncat &lt;&lt;EOF | sudo tee /etc/modules-load.d/crio.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\n# Set up required sysctl params, these persist across reboots.\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n\nsudo sysctl --system\n</code></pre></p> <p>Then, install cri-o. At time of this writing, the latest version was 1.21: <pre><code>OS=CentOS_8\nVERSION=1.21\ncurl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/devel:kubic:libcontainers:stable.repo\ncurl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/devel:kubic:libcontainers:stable:cri-o:$VERSION.repo\nyum install cri-o -y\n</code></pre></p> <p>Make sure to set 192.168.123.254 (gw1) as an insecure registry: <pre><code>cat &lt;&lt;'EOF' | tee /etc/containers/registries.conf.d/999-insecure.conf\n[[registry]]\nlocation = \"192.168.123.254:5000\"\ninsecure = true\nEOF\n</code></pre></p> <p>Also, make sure to remove <code>/etc/cni/net.d/100-crio-bridge.conf</code> as we do not want to fall back to crio's default networking: <pre><code>mv /etc/cni/net.d/100-crio-bridge.conf /root/.\n</code></pre></p> <p>Note: If you forget to move or delete this file, your CoreDNS pods will come up with an IP address in the 10.0.0.0/8 range.</p> <p>Finally, start crio: <pre><code>systemctl daemon-reload\nsystemctl enable crio --now\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#install-kubelet-kubectl-kubeadm","title":"Install kubelet, kubectl, kubeadm","text":"<p>See https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl for further details.</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nexclude=kubelet kubeadm kubectl\nEOF\n\n# Set SELinux in permissive mode (effectively disabling it)\nsudo setenforce 0\nsudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\n\nsudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes\n\nsudo systemctl enable --now kubelet\n</code></pre>"},{"location":"installation/INSTALL.KUBEADM/#deploying-a-cluster-with-ovn-kubernetes","title":"Deploying a cluster with OVN Kubernetes","text":"<p>Execute the following instructions only on the master node, <code>node1</code>.</p>"},{"location":"installation/INSTALL.KUBEADM/#install-instructions-for-kubeadm","title":"Install instructions for kubeadm","text":"<p>Deploy on the master node <code>node1</code>: <pre><code>kubeadm init --pod-network-cidr 172.16.0.0/16 --service-cidr 172.17.0.0/16 --apiserver-advertise-address 192.168.123.1\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/ovn.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre></p> <p>Write down the join command for worker nodes - you will need it later.</p> <p>You will now have a one node cluster without a CNI plugin and as such the CoreDNS pods will not start: <pre><code>[root@node1 ~]# kubectl get pods -o wide -A\nNAMESPACE     NAME                            READY   STATUS              RESTARTS   AGE   IP                NODE    NOMINATED NODE   READINESS GATES\nkube-system   coredns-78fcd69978-dvpjg        0/1     ContainerCreating   0          21s   &lt;none&gt;            node1   &lt;none&gt;           &lt;none&gt;\nkube-system   coredns-78fcd69978-mzpzr        0/1     ContainerCreating   0          21s   &lt;none&gt;            node1   &lt;none&gt;           &lt;none&gt;\nkube-system   etcd-node1                      1/1     Running             2          33s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-apiserver-node1            1/1     Running             2          33s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-controller-manager-node1   1/1     Running             3          33s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-vm44k                1/1     Running             0          22s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-scheduler-node1            1/1     Running             3          28s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>Now, deploy OVN Kubernetes - see below.</p>"},{"location":"installation/INSTALL.KUBEADM/#deploying-ovn-kubernetes-on-node1","title":"Deploying OVN Kubernetes on node1","text":"<p>Install build dependencies and create a softlink for <code>pip</code> to <code>pip3</code>: <pre><code>yum install git python3-pip make podman buildah -y\nln -s $(which pip3) /usr/local/bin/pip\n</code></pre></p> <p>Install golang, for further details see https://golang.org/doc/install: <pre><code>curl -L -O https://golang.org/dl/go1.17.linux-amd64.tar.gz\nrm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.17.linux-amd64.tar.gz\necho \"export PATH=$PATH:/usr/local/go/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\ngo version\n</code></pre></p> <p>Now, clone the OVN Kubernetes repository: <pre><code>mkdir -p $HOME/work/src/github.com/ovn-org\ncd $HOME/work/src/github.com/ovn-org\ngit clone https://github.com/ovn-org/ovn-kubernetes\ncd $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/images\n</code></pre></p> <p>Build the latest ovn-daemonset image and push it to the registry. Prepare the binaries: <pre><code># Build ovn docker image\npushd ../../go-controller\nmake\npopd\n\n# Build ovn kube image\n# Find all built executables, but ignore the 'windows' directory if it exists\nfind ../../go-controller/_output/go/bin/ -maxdepth 1 -type f -exec cp -f {} . \\;\necho \"ref: $(git rev-parse  --symbolic-full-name HEAD)  commit: $(git rev-parse  HEAD)\" &gt; git_info\n</code></pre></p> <p>Now, build and push the image with: <pre><code>OVN_IMAGE=192.168.123.254:5000/ovn-daemonset-f:latest\nbuildah bud -t $OVN_IMAGE -f Dockerfile.fedora .\npodman push $OVN_IMAGE\n</code></pre></p> <p>Next, run: <pre><code>OVN_IMAGE=192.168.123.254:5000/ovn-daemonset-f:latest\nMASTER_IP=192.168.123.1\nNET_CIDR=\"172.16.0.0/16/24\"\nSVC_CIDR=\"172.17.0.0/16\"\n./daemonset.sh --image=${OVN_IMAGE} \\\n    --net-cidr=\"${NET_CIDR}\" --svc-cidr=\"${SVC_CIDR}\" \\\n    --gateway-mode=\"local\" \\\n    --k8s-apiserver=https://${MASTER_IP}:6443\n</code></pre></p> <p>You might also have to work around an issue where br-int is added by OVN, but the necessary files in /var/run/openvswitch are not created until Open vSwitch is restarted - see here for more details. This only happens on the master, so let's pre-create <code>br-int</code> there: <pre><code>ovs-vsctl add-br br-int\n</code></pre></p> <p>Now, set up ovnkube: <pre><code># set up the namespace\nkubectl apply -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovn-setup.yaml\n# set up the database pods - wait until the pods are up and running before progressing to the next command:\nkubectl apply -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-db.yaml\n# set up the master pods - wait until the pods are up and running before progressing to the next command:\nkubectl apply -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-master.yaml\n# set up the ovnkube-node pods - wait until the pods are up and running before progressing to the next command:\nkubectl apply -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-node.yaml\n</code></pre></p> <p>Once all OVN related pods are up, you should see that the CoreDNS pods have started as well and they should be in the correct network. <pre><code>[root@node1 images]# kubectl get pods -A -o wide | grep coredns\nkube-system      coredns-78fcd69978-ms969         1/1     Running   0          29s     172.16.0.6        node1   &lt;none&gt;           &lt;none&gt;\nkube-system      coredns-78fcd69978-w6k2z         1/1     Running   0          36s     172.16.0.5        node1   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>Finally, delete the kube-proxy DaemonSet: <pre><code>kubectl delete ds -n kube-system kube-proxy\n</code></pre></p> <p>You should now see the following when listing all pods: <pre><code>[root@node1 ~]# kubectl get pods -A -o wide\nNAMESPACE        NAME                             READY   STATUS    RESTARTS   AGE     IP                NODE    NOMINATED NODE   READINESS GATES\nkube-system      coredns-78fcd69978-rhjgh         1/1     Running   0          10s     172.16.0.4        node1   &lt;none&gt;           &lt;none&gt;\nkube-system      coredns-78fcd69978-xcxnx         1/1     Running   0          17s     172.16.0.3        node1   &lt;none&gt;           &lt;none&gt;\nkube-system      etcd-node1                       1/1     Running   1          74m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system      kube-apiserver-node1             1/1     Running   1          74m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system      kube-controller-manager-node1    1/1     Running   1          74m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system      kube-scheduler-node1             1/1     Running   1          74m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\novn-kubernetes   ovnkube-db-7767c6b7c5-25drn      2/2     Running   2          11m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\novn-kubernetes   ovnkube-master-775d45fd5-mzkcb   3/3     Running   3          10m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\novn-kubernetes   ovnkube-node-xmgrj               3/3     Running   3          8m49s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#verifying-the-deployment","title":"Verifying the deployment","text":"<p>Create a test deployment to make sure that everything works as expected: <pre><code>cd ~\ncat &lt;&lt;'EOF' &gt; fedora.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fedora-deployment\n  labels:\n    app: fedora-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: fedora-pod\n  template:\n    metadata:\n      labels:\n        app: fedora-pod\n    spec:\n      tolerations:\n      - key: \"node-role.kubernetes.io/control-plane\"\n        operator: \"Exists\"\n      containers:\n      - name: fedora\n        image: fedora\n        command:\n          - sleep\n          - infinity\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n          capabilities:\n            add:\n              - \"SETFCAP\"\n              - \"CAP_NET_RAW\"\n              - \"CAP_NET_ADMIN\"\nEOF\nkubectl apply -f fedora.yaml\n</code></pre></p> <p>Make sure that the pods have a correct IP address and that they can reach the outside world, e.g. by installing some software: <pre><code>[root@node1 ~]# kubectl get pods -o wide\nNAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE    NOMINATED NODE   READINESS GATES\nfedora-deployment-86f7647bd6-dllbs   1/1     Running   0          58s   172.16.0.5   node1   &lt;none&gt;           &lt;none&gt;\nfedora-deployment-86f7647bd6-k42wm   1/1     Running   0          36s   172.16.0.6   node1   &lt;none&gt;           &lt;none&gt;\n[root@node1 ~]# kubectl exec -it fedora-deployment-86f7647bd6-dllbs -- /bin/bash\n[root@fedora-deployment-86f7647bd6-dllbs /]# yum install iputils -y\nFedora 34 - x86_64                                                                   4.2 MB/s |  74 MB     00:17    \nFedora 34 openh264 (From Cisco) - x86_64                                             1.7 kB/s | 2.5 kB     00:01    \nFedora Modular 34 - x86_64                                                           2.8 MB/s | 4.9 MB     00:01    \nFedora 34 - x86_64 - Updates                                                         3.7 MB/s |  25 MB     00:06    \nFedora Modular 34 - x86_64 - Updates                                                 2.0 MB/s | 4.6 MB     00:02    \nLast metadata expiration check: 0:00:01 ago on Tue Aug 24 17:04:04 2021.\nDependencies resolved.\n=====================================================================================================================\n Package                   Architecture             Version                           Repository                Size\n=====================================================================================================================\nInstalling:\n iputils                   x86_64                   20210202-2.fc34                   fedora                   170 k\n\nTransaction Summary\n=====================================================================================================================\nInstall  1 Package\n\nTotal download size: 170 k\nInstalled size: 527 k\nDownloading Packages:\niputils-20210202-2.fc34.x86_64.rpm                                                   1.2 MB/s | 170 kB     00:00    \n---------------------------------------------------------------------------------------------------------------------\nTotal                                                                                265 kB/s | 170 kB     00:00     \nRunning transaction check\nTransaction check succeeded.\nRunning transaction test\nTransaction test succeeded.\nRunning transaction\n  Preparing        :                                                                                             1/1 \n  Installing       : iputils-20210202-2.fc34.x86_64                                                              1/1 \n  Running scriptlet: iputils-20210202-2.fc34.x86_64                                                              1/1 \n  Verifying        : iputils-20210202-2.fc34.x86_64                                                              1/1 \n\nInstalled:\n  iputils-20210202-2.fc34.x86_64                                                                                     \n\nComplete!\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#uninstalling-ovn-kubernetes","title":"Uninstalling OVN Kubernetes","text":"<p>In order to uninstall OVN kubernetes: <pre><code>kubectl delete -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-node.yaml\nkubectl delete -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-master.yaml\nkubectl delete -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-db.yaml\nkubectl delete -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovn-setup.yaml\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#issues-workarounds","title":"Issues / workarounds:","text":"<p>br-int might be added by OVN, but the files for it are not created in /var/run/openvswitch. <code>ovs-ofctl dump-flows br-int</code> fails, and one will see the following log messages among others: <pre><code>2021-08-24T12:42:43.810Z|00025|rconn|WARN|unix:/var/run/openvswitch/br-int.mgmt: connection failed (No such file or directory)\n</code></pre></p> <p>The best workaroud is to pre-create br-int before the OVN Kubernetes installation: <pre><code>ovs-vsctl add-br br-int\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#joining-worker-nodes-to-the-environment","title":"Joining worker nodes to the environment","text":"<p>Finally, join your worker nodes. Set them up using the base setup steps for the nodes and the CRI and kubeadm installation steps. Then, use the output from the <code>kubeadm init</code> command that you ran earlier to join the node to the cluster: <pre><code>kubeadm join 192.168.123.10:6443 --token &lt;...&gt; \\\n    --discovery-token-ca-cert-hash &lt;...&gt;\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#kubeadm-reset-instructions","title":"kubeadm reset instructions","text":"<p>If you must reset your master and worker nodes, the following commands can be used to reset the lab environment. Run this on each node and then ideally reboot the node right after: <pre><code>IF2=enp7s0\necho \"y\" | kubeadm reset\nrm -f /etc/cni/net.d/10-*\nrm -Rf ~/.kube\nrm -f /etc/openvswitch/conf.db\nnmcli conn del cni0\nsystemctl restart openvswitch\nsystemctl restart NetworkManager\nnmcli conn up ovs-if-${IF2}\n</code></pre></p>"},{"location":"installation/INSTALL.OPENSHIFT/","title":"OVN overlay network on Openshift","text":"<p>This section describes how an OVN overlay network is setup on Openshift 4.10 and later. It explains the various components and how they come together to establish the OVN overlay network. People that are interested in understanding how the ovn cni plugin is installed will find this useful.</p> <p>As of OCP-4.10, the Openshift overlay network is managed using daemonsets. The ovs/ovn components are built into a Docker image that has all of the needed packages. Daemonsets are deployed on the nodes in the cluster to provide the network. The daemonsets use a common Docker image when creating the needed pods.</p>"},{"location":"installation/INSTALL.OPENSHIFT/#overview","title":"Overview","text":"<p>Ovn has a master service that runs on a compute node in the cluster and node controllers that run on all nodes in the cluster. The master service runs ovn north and ovn south daemons that work with the ovs north and south databases. All of the nodes run the ovn controller which accesses the ovn north and south databases.</p> <p>There are two daemonsets, ovnkube-master.yaml and ovnkube.yaml that mangage the parts of the ovn architecture. The ovnkube-master daemonset runs both master and node services, ovnkube daemonset just runs the node service. The daemonset uses selector labels in the cluster's nodes to specify where the daemonsets run.</p> <p>Both daemonsets use the same docker image. Configuration is passed from the daemonset to the image using environment variables and mounting directories from the host. The environment variables are set from a configmap that is created during installation. The image includes an control script that is the entrypoint for the container, ovnkube and the openvswitch and ovn rpms.</p> <p>The ovnkube program (in this repository) provides the interface between Openshift and Ovn. The arguments used when starting it determine its role in the configuration which are master or node.</p> <p>The ovn cni service is configured and installed after Openshift is installed and running. Openshift comes up with no network support. One of many networking choices, including ovn, is selected, installed, configured and started to complete Openshift installation.</p> <p>The installer creates a configmap that contains the configuration information. The two daemonsets use the configuration information.</p> <p>The configmap is mounted in the daemonsets and is used to initialize environment variables that are passed to the Docker image running in the pod.</p>"},{"location":"installation/INSTALL.OPENSHIFT/#daemonset","title":"Daemonset","text":"<p>The daemonsets govern where the components run, what is run, and how they interface with other components. The ovnkube-master.yaml and ovnkube.yaml files bring together the needed pieces and set up the context for running the pods. Here is some of how it is done.</p> <p>The nodes are chosen based on labels:</p> <p><pre><code>spec:\n  selector:\n    matchLabels:\n      node-role.kubernetes.io/compute: \"true\"\n             or\n      node-role.kubernetes.io/control-plane: \"true\"\n  template:\n    spec:\n      nodeSelector:\n        node-role.kubernetes.io/compute: \"true\"\n             or\n        node-role.kubernetes.io/control-plane: \"true\"\n</code></pre> The daemonset will start a pod on every nodes that has the desired labels.</p> <p>NOTE: The above labels are openshift labels created when openshift is installed. As this evolves ovn specific labels will be used.</p> <p>The docker image is configured as follows: <pre><code>spec:\n  template:\n    spec:\n      containers:\n      - name: ovnkube-master\n        image: \"netdev22:5000/ovn-kube:latest\"\n</code></pre> The image is in a docker repository that all nodes can access.</p> <p>NOTE: The above uses a local development docker repository, netdev22:5000</p> <p>The openshift service account is created during installation and configured here: <pre><code>spec:\n  template:\n    spec:\n      serviceAccountName: ovn\n</code></pre></p> <p>The daemonsets use host networking and provide the cni plugin when they startup. Docker uses the ovn-cni-plugin to access networking. Docker must be able to access the cni plugin that is in the image. To do this the host /opt/cni/bin directory is mounted in the pod and the pod startup script copies the plugin to the host. Since the host operating environment and the pod are different, the installed plugin is just a shim that passes requests to the server that is running in the pod.</p> <pre><code>spec:\n  template:\n    spec:\n      hostNetwork: true\n      hostPID: true\n      containers:\n        volumeMounts:\n        - mountPath: /host/opt/cni/bin\n          name: host-opt-cni-bin\n        - mountPath: /etc/cni/net.d\n          name: host-etc-cni-netd\n        - mountPath: /var/lib/cni/networks/ovn-k8s-cni-overlay\n          name: host-var-lib-cni-networks-openshift-ovn\n      volumes:\n      - name: host-opt-cni-bin\n        hostPath:\n          path: /opt/cni/bin\n      - name: host-etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n      - name: host-var-lib-cni-networks-openshift-ovn\n        hostPath:\n          path: /var/lib/cni/networks/ovn-k8s-cni-overlay\n</code></pre>"},{"location":"installation/INSTALL.OPENSHIFT/#docker-image","title":"Docker Image","text":"<p>The Dockerfile directs construction of the image. It installs a base OS, rpms, and local scripts into the image. It also sets up directories in the image and copies files into them.  In particular it copies in the entrypoint script, ovnkube.sh.</p> <p><pre><code># docker build -t ovn-kube .\n# docker tag ovn-kube netdev22:5000/ovn-kube:latest\n# docker push netdev22:5000/ovn-kube:latest\n</code></pre> The image is tagged and pushed to a docker repository. This example uses the local development docker repo, netdev22:5000</p> <p>NOTE: At present go_controller is built and the resultant files are copied to the docker build directory. In the future, these files will be installed from openvswitch-ovn-kubernetes rpm.</p>"},{"location":"installation/INSTALL.OPENSHIFT/#configuration","title":"Configuration","text":"<p>There are three sources of configuration information in the daemonsets:</p> <ol> <li>/etc/origin/node/ files</li> </ol> <p>These files are installed by the Openshift install and are in the node daemonset which mounts them on the host. The ovn deamonsets mount them in the pod.</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n        volumeMounts:\n        # Directory which contains the host configuration.\n        - mountPath: /etc/origin/node/\n          name: host-config\n          readOnly: true\n        - mountPath: /etc/sysconfig/origin-node\n          name: host-sysconfig-node\n          readOnly: true\n      volumes:\n      - name: host-config\n        hostPath:\n          path: /etc/origin/node\n      - name: host-sysconfig-node\n        hostPath:\n          path: /etc/sysconfig/origin-node\n</code></pre> <ol> <li>/etc/openvswitch/ovn_k8s.conf</li> </ol> <p>This file is used by ovn daemons and ovnkube to set needed values. This file is build into the image. It holds configuration constants.</p> <ol> <li>ovn-config configmap</li> </ol> <p>This configmap is created after Openshift is installed and running and before the network is installed. It contains information that is specific to the cluster that is needed for ovn to access the cluster apiserver.</p> <p><pre><code># oc get configmap ovn-config -o yaml\napiVersion: v1\ndata:\n  OvnNorth: tcp:10.19.188.22:6641\n  OvnSouth: tcp:10.19.188.22:6642\n  k8s_apiserver: https://wsfd-netdev22.ntdv.lab.eng.bos.redhat.com:8443\n  net_cidr: 10.128.0.0/14\n  svc_cidr: 172.30.0.0/16\nkind: ConfigMap\n</code></pre> OvnNorth and OvnSouth are used by ovn to access its daemons. They are host IP address of the daemons.</p> <p>k8s_apiserver allows access to Openshift's api server.</p> <p>net_cidr and svc_cidr are the configuration configuration cidrs</p> <p>The configmap keys become environment variable values in the daemonset yaml files.</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n        env:\n        - name: OVN_NORTH\n          valueFrom:\n            configMapKeyRef:\n              name: ovn-config\n              key: OvnNorth\n</code></pre>"},{"location":"installation/INSTALL.SSL/","title":"INSTALL.SSL","text":"<p>This document explains the way one could use SSL for connectivity between OVN components.  The details in this documentation needs a minimum version of Open vSwitch 2.7.</p>"},{"location":"installation/INSTALL.SSL/#generating-certificates","title":"Generating certificates","text":"<p>Detailed explanation of how OVS utilites and daemons use SSL certificates is explained at OVS.SSL.  The following section summarizes it for ovn-kubernetes.</p>"},{"location":"installation/INSTALL.SSL/#create-a-certificate-authority","title":"Create a certificate authority.","text":"<p>On a secure machine (separate from your k8s cluster), where you have installed the ovs-pki utility (comes with OVS installations), create a certificate authority by running</p> <pre><code>ovs-pki init --force\n</code></pre> <p>The above command creates 2 certificate authorities.  But we are concerned only with one of them, i.e the \"switch\" certificate authority.  We will use this certificate authority to sign individual certificates of all the nodes.  We will then use the same certificate authority's certificate to verify a node's connections to the master.</p> <p>Copy this certificate to the master and each of the nodes. $CENTRAL_IP is the IP address of the master.</p> <pre><code>scp /var/lib/openvswitch/pki/switchca/cacert.pem \\\n    root@$CENTRAL_IP:/etc/openvswitch/.\n</code></pre>"},{"location":"installation/INSTALL.SSL/#generate-signed-certificates-for-ovn-components-running-on-the-master","title":"Generate signed certificates for OVN components running on the master.","text":""},{"location":"installation/INSTALL.SSL/#generate-signed-certificates-for-ovn-nb-database","title":"Generate signed certificates for OVN NB Database","text":"<p>On the master, run the following commands.</p> <pre><code>cd /etc/openvswitch\novs-pki req ovnnb\n</code></pre> <p>The above command will generate a private key \"ovnnb-privkey.pem\" and a corresponding certificate request named \"ovnnb-req.pem\". Copy over the ovnnb-req.pem to the aforementioned secure machine's /var/lib/openvswitch/pki folder to sign it using the command below.</p> <pre><code>ovs-pki -b sign ovnnb\n</code></pre> <p>The above command will generate ovnnb-cert.pem. Copy over this file back to the master's /etc/openvswitch. The ovnnb-privkey.pem and ovnnb-cert.pem will be used by the ovsdb-server that fronts the OVN NB database.</p> <p>Now run the following commands to ask ovsdb-server to use these certificates and also to open up SSL ports via which the database can be accessed.</p> <pre><code>ovn-nbctl set-ssl /etc/openvswitch/ovnnb-privkey.pem \\\n    /etc/openvswitch/ovnnb-cert.pem  /etc/openvswitch/cacert.pem\n\novn-nbctl set-connection pssl:6641\n</code></pre>"},{"location":"installation/INSTALL.SSL/#generate-signed-certificates-for-ovn-sb-database","title":"Generate signed certificates for OVN SB Database","text":"<pre><code>cd /etc/openvswitch\novs-pki req ovnsb\n</code></pre> <p>The above command will generate a private key \"ovnsb-privkey.pem\" and a corresponding certificate request named \"ovnsb-req.pem\". Copy over the ovnsb-req.pem to the aforementioned secure machine's /var/lib/openvswitch/pki to sign it using the command below.</p> <pre><code>ovs-pki -b sign ovnsb\n</code></pre> <p>The above command will generate ovnsb-cert.pem. Copy over this file back to the master's /etc/openvswitch. The ovnsb-privkey.pem and ovnsb-cert.pem will be used by the ovsdb-server that fronts the OVN SB database.</p> <p>Now run the following commands to ask ovsdb-server to use these certificates  and also to open up SSL ports via which the database can be accessed.</p> <pre><code>ovn-sbctl set-ssl /etc/openvswitch/ovnsb-privkey.pem \\\n    /etc/openvswitch/ovnsb-cert.pem  /etc/openvswitch/cacert.pem\n\novn-sbctl set-connection pssl:6642\n</code></pre>"},{"location":"installation/INSTALL.SSL/#generate-signed-certificates-for-ovn-northd","title":"Generate signed certificates for OVN Northd","text":"<p>If you are running ovn-northd on the same host as the OVN NB and SB database servers, then there is no need to secure the communication between ovn-northd and OVN NB/SB daemons. ovn-northd will communicate using UNIX path.</p> <p>In case you still want to secure the communication, or the daemons are running on separate hosts, then follow the instructions on this page [OVN-NORTHD.SSL.md]</p>"},{"location":"installation/INSTALL.SSL/#generate-certificates-for-the-nodes","title":"Generate certificates for the nodes","text":"<p>On each node, create a certificate request.</p> <pre><code>cd /etc/openvswitch\novs-pki req ovncontroller\n</code></pre> <p>The above command will create a new private key for the node called ovncontroller-privkey.pem and a certificate request file called ovncontroller-req.pem.  Copy this certificate request file to the secure machine where you created the certificate authority and from the directory where the copied file exists, run:</p> <pre><code>ovs-pki -b sign ovncontroller switch\n</code></pre> <p>The above will create the certificate for the node called \"ovncontroller-cert.pem\". You should copy this certificate back to the node's /etc/openvswitch directory.</p> <p>Additionally, the common name used for signing the certificates (<code>ovncontroller</code>) needs to be passed in for TLS server certificate verification using the  <code>-nb-cert-common-name</code> and the <code>-sb-cert-common-name</code> CLI options.</p>"},{"location":"installation/INSTALL.SSL/#one-time-setup","title":"One time setup.","text":"<p>As explained in README.md, OVN architecture has a central component which stores your networking intent in a database.  You start this central component on the node where you intend to start your k8s central daemons by running:</p> <pre><code>/usr/share/openvswitch/scripts/ovn-ctl start_northd\n</code></pre> <p>You should now restart the ovn-controller on each host with the following additional options.</p> <pre><code>/usr/share/openvswitch/scripts/ovn-ctl \\\n    --ovn-controller-ssl-key=\"/etc/openvswitch/ovncontroller-privkey.pem\"  \\\n    --ovn-controller-ssl-cert=\"/etc/openvswitch/ovncontroller-cert.pem\"    \\\n    --ovn-controller-ssl-ca-cert=\"/etc/openvswitch/cacert.pem\" \\\n    restart_controller\n</code></pre> <p>To make sure that ovn-controller restarts with the above options during system reboots, you should add the above options to your startup script's defaults file.  For e.g. on Ubuntu, if you installed ovn-controller via the package 'ovn-host*.deb', write the following to your /etc/default/ovn-host file</p> <p><pre><code>OVN_CTL_OPTS=\"--ovn-controller-ssl-key=/etc/openvswitch/ovncontroller-privkey.pem  --ovn-controller-ssl-cert=/etc/openvswitch/ovncontroller-cert.pem --ovn-controller-ssl-ca-cert=/etc/openvswitch/cacert.pem\"\n</code></pre> If you start the ovnkube utility on master with \"--init-master\" or with \"--init-network-control-manager\", you should pass the SSL certificates to it. For e.g:</p> <pre><code>sudo ovnkube -k8s-kubeconfig kubeconfig.yaml -loglevel=4 \\\n -k8s-apiserver=\"http://$CENTRAL_IP:8080\" \\\n -logfile=\"/var/log/ovn-kubernetes/ovnkube.log\" \\\n -init-master=\"$NODE_NAME\" -cluster-subnets=$CLUSTER_IP_SUBNET \\\n -k8s-service-cidr=$SERVICE_IP_SUBNET \\\n -nodeport \\\n -nb-address=\"ssl:$CENTRAL_IP:6641\" \\\n -sb-address=\"ssl:$CENTRAL_IP:6642\" \\\n -nb-client-privkey /etc/openvswitch/ovncontroller-privkey.pem \\\n -nb-client-cert /etc/openvswitch/ovncontroller-cert.pem \\\n -nb-client-cacert /etc/openvswitch/cacert.pem \\\n -nb-cert-common-name ovncontroller \\\n -sb-client-privkey /etc/openvswitch/ovncontroller-privkey.pem \\\n -sb-client-cert /etc/openvswitch/ovncontroller-cert.pem \\\n -sb-client-cacert /etc/openvswitch/cacert.pem \\\n -sb-cert-common-name ovncontroller\n</code></pre> <p>If you start the ovnkube utility with \"--init-cluster-manager\", there is no need to pass the SSL certificates to it as it doesn't connect to the OVN databases. For e.g:</p> <pre><code>sudo ovnkube -k8s-kubeconfig kubeconfig.yaml -loglevel=4 \\\n -k8s-apiserver=\"http://$CENTRAL_IP:8080\" \\\n -logfile=\"/var/log/ovn-kubernetes/ovnkube-cluster-manager.log\" \\\n -init-cluster-manager=\"$NODE_NAME\" -cluster-subnets=$CLUSTER_IP_SUBNET \\\n -k8s-service-cidr=$SERVICE_IP_SUBNET\n</code></pre> <p>And when you start your ovnkube utility on nodes, you should pass the SSL certificates to it. For e.g:</p> <pre><code>sudo ovnkube -k8s-kubeconfig $HOME/kubeconfig.yaml -loglevel=4 \\\n    -k8s-apiserver=\"http://$CENTRAL_IP:8080\" \\\n    -init-node=\"$NODE_NAME\"  \\\n    -nodeport \\\n    -nb-address=\"ssl:$CENTRAL_IP:6641\" \\\n    -sb-address=\"ssl:$CENTRAL_IP:6642\" -k8s-token=$TOKEN \\\n    -nb-client-privkey /etc/openvswitch/ovncontroller-privkey.pem \\\n    -nb-client-cert /etc/openvswitch/ovncontroller-cert.pem \\\n    -nb-client-cacert /etc/openvswitch/cacert.pem \\\n    -nb-cert-common-name ovncontroller \\\n    -sb-client-privkey /etc/openvswitch/ovncontroller-privkey.pem \\\n    -sb-client-cert /etc/openvswitch/ovncontroller-cert.pem \\\n    -sb-client-cacert /etc/openvswitch/cacert.pem \\\n    -sb-cert-common-name ovncontroller \\\n    -init-gateways \\\n    -k8s-service-cidr=$SERVICE_IP_SUBNET \\\n    -cluster-subnets=$CLUSTER_IP_SUBNET\n</code></pre>"},{"location":"installation/INSTALL.UBUNTU/","title":"Installing OVS and OVN on Ubuntu","text":""},{"location":"installation/INSTALL.UBUNTU/#installing-ovs-and-ovn-from-packages","title":"Installing OVS and OVN from packages","text":"<p>To install OVS bits on all nodes, run:</p> <pre><code>sudo apt-get install python-six openssl -y\n\nsudo apt-get install openvswitch-switch openvswitch-common -y\n</code></pre> <p>On the master, where you intend to start OVN's central components, run:</p> <pre><code>sudo apt-get install ovn-central ovn-common ovn-host -y\n</code></pre> <p>On the agent nodes, run:</p> <pre><code>sudo apt-get install ovn-host ovn-common -y\n</code></pre>"},{"location":"installation/INSTALL.UBUNTU/#installing-ovs-and-ovn-from-sources","title":"Installing OVS and OVN from sources","text":"<p>Install a few pre-requisite packages.</p> <pre><code>apt-get update\napt-get install -y build-essential fakeroot debhelper \\\n                    autoconf automake libssl-dev \\\n                    openssl python-all \\\n                    python-setuptools \\\n                    python-six \\\n                    libtool git dh-autoreconf \\\n                    linux-headers-$(uname -r)\n</code></pre> <p>Clone the OVS repo.</p> <pre><code>git clone https://github.com/openvswitch/ovs.git\ncd ovs\n</code></pre> <p>Configure and compile the sources</p> <pre><code>./boot.sh\n./configure --prefix=/usr --localstatedir=/var  --sysconfdir=/etc --enable-ssl --with-linux=/lib/modules/`uname -r`/build\nmake -j3\n</code></pre> <p>Install the executables</p> <pre><code>make install\nmake modules_install\n</code></pre> <p>Create a depmod.d file to use OVS kernel modules from this repo instead of upstream linux.</p> <pre><code>cat &gt; /etc/depmod.d/openvswitch.conf &lt;&lt; EOF\noverride openvswitch * extra\noverride vport-geneve * extra\noverride vport-stt * extra\noverride vport-* * extra\nEOF\n</code></pre> <p>Copy a startup script and start OVS</p> <pre><code>depmod -a\ncp debian/openvswitch-switch.init /etc/init.d/openvswitch-switch\n/etc/init.d/openvswitch-switch force-reload-kmod\n</code></pre>"},{"location":"installation/OVN-NORTHD.SSL/","title":"OVN NORTHD.SSL","text":"<p>If the ovn-northd instance is not running on the same node as OVN NB and OVN SB database, then you will need to follow the steps below to secure the communication between ovn-northd and NB/SB databases.</p>"},{"location":"installation/OVN-NORTHD.SSL/#generate-signed-certificates-for-ovn-northd","title":"Generate signed certificates for OVN Northd","text":"<p>On the node running ovn-northd daemon, run the following commands. <pre><code>cd /etc/openvswitch\novs-pki req ovnnorthd\n</code></pre></p> <p>The above command will generate a private key \"ovnnorthd-privkey.pem\" and a corresponding certificate request named \"ovnnorthd-req.pem\". Copy over the ovnnorthd-req.pem to the secure machine's (where you have installed the ovs-pki utility) /var/lib/openvswitch/pki folder to sign it using the command below.</p> <pre><code>ovs-pki -b sign ovnnorthd\n</code></pre> <p>The above command will generate ovnnorthd-cert.pem. Copy over this file back to the ovn-northd node's /etc/openvswitch foler. The ovnnorthd-privkey.pem and ovnnorthd-cert.pem will be used by the ovn-northd to communicate with OVN NB  and SB database.</p> <p>Now run the following command to ask ovn-northd to use these certificates.</p> <pre><code>cat &gt; /etc/openvswitch/ovn-northd-db-params.conf &lt;&lt; EOF\n--ovnnb-db=ssl:$CENTRAL_IP:6641 --ovnsb-db=ssl:$CENTRAL_IP:6642 \\\n-p /etc/openvswitch/ovnnorthd-privkey.pem -c /etc/openvswitch/ovnnorthd-cert.pem \\\n-C /etc/openvswitch/cacert.pem\nEOF\n\n/usr/share/openvswitch/scripts/ovn-ctl restart_northd\n</code></pre>"},{"location":"installation/config/","title":"Config variables","text":""},{"location":"installation/config/#initial-setup","title":"Initial setup","text":"<p>If you want to override the default values for some config options, then the file available in this repo (etc/ovn_k8s.conf) must be copied to the following locations:</p> <ul> <li> <p>on Linux: <pre><code>/etc/openvswitch/ovn_k8s.conf\n</code></pre> The following command copies the config file if it is run from inside the repo: <pre><code>cp etc/ovn_k8s.conf /etc/openvswitch/ovn_k8s.conf\n</code></pre></p> </li> <li> <p>on Windows: <pre><code>C:\\etc\\ovn_k8s.conf\n</code></pre> The following PowerShell command copies the config file if is run from inside the repo: <pre><code>Copy-Item \".\\etc\\ovn_k8s.conf\" -Destination (New-Item \"C:\\etc\" -Type container -Force)\n</code></pre></p> </li> </ul>"},{"location":"installation/config/#config-values","title":"Config values","text":"<p>The config file contains common configuration options shared between the various ovn-kubernetes programs (ovnkube, ovn-k8s-cni-overlay, etc).  All configuration file options can also be specified as command-line arguments which override config file options; see the -help output of each program for more details.</p>"},{"location":"installation/config/#default-section","title":"[default] section","text":"<p>The following config option represents the MTU value which should be used for the overlay networks. <pre><code>mtu=1400\n</code></pre></p> <p>The following option affects only the gateway nodes. This value is used to track connections that are initiated from the pods so that the reverse connections go back to the pods. This represents the conntrack zone used for the conntrack flow rules. <pre><code>conntrack-zone=64000\n</code></pre></p> <p>The following option only affects ovn-controller. This is the maximum number of milliseconds of idle time on connection to the server before sending an inactivity probe message.  As a client connects to the server over TCP, it may take a while for the kernel to figure out if the connection is broken.  But the client can overcome this by periodically sending probes over the TCP connection to make sure that the connection is up.  On the flip side, when there are hundreds of nodes, the server can get bogged down by client probe messages.</p> <p>The default value is set as 100000ms. But can be changed with this config option</p> <p>inactivity-probe=600000</p>"},{"location":"installation/config/#logging-section","title":"[logging] section","text":"<p>The following config values control what verbosity level logging is written at and to what file (if any). <pre><code>loglevel=5\nlogfile=/var/log/ovnkube.log\n</code></pre></p>"},{"location":"installation/config/#cni-section","title":"[cni] section","text":"<p>The following config values are used for the CNI plugin. <pre><code>conf-dir=/etc/cni/net.d\nplugin=ovn-k8s-cni-overlay\n</code></pre></p>"},{"location":"installation/config/#kubernetes-section","title":"[kubernetes] section","text":"<p>Kubernetes API options are stored in the following section. <pre><code>apiserver=https://1.2.3.4:6443\ntoken=TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQsIGNvbnNlY3RldHVyIGFkaXBpc2NpbmcgZWxpdC4gQ3JhcyBhdCB1bHRyaWNpZXMgZWxpdC4gVXQgc2l0IGFtZXQgdm9sdXRwYXQgbnVuYy4K\ncacert=/etc/kubernetes/ca.crt\n</code></pre></p>"},{"location":"installation/config/#ovnnorth-section","title":"[ovnnorth] section","text":"<p>This section contains the address and (if the 'ssl' method is used) certificates needed to use the OVN northbound database API. Only the the ovn-kubernetes master needs to specify the 'server' options. <pre><code>address=ssl:1.2.3.4:6641\nclient-privkey=/path/to/private.key\nclient-cert=/path/to/client.crt\nclient-cacert=/path/to/client-ca.crt\nserver-privkey=/path/to/private.key\nserver-cert=/path/to/server.crt\nserver-cacert=/path/to/server-ca.crt\n</code></pre></p>"},{"location":"installation/config/#ovnsouth-section","title":"[ovnsouth] section","text":"<p>This section contains the address and (if the 'ssl' method is used) certificates needed to use the OVN southbound database API. Only the the ovn-kubernetes master needs to specify the 'server' options. <pre><code>address=ssl:1.2.3.4:6642\nclient-privkey=/path/to/private.key\nclient-cert=/path/to/client.crt\nclient-cacert=/path/to/client-ca.crt\nserver-privkey=/path/to/private.key\nserver-cert=/path/to/server.crt\nserver-cacert=/path/to/server-ca.crt\n</code></pre></p>"},{"location":"installation/ha/","title":"OVN central database High-availability","text":"<p>OVN architecture has two central databases that can be clustered. The databases are OVN_Northbound and OVN_Southbound.  This document explains how to cluster them and start various daemons for the ovn-kubernetes integration.  You will ideally need at least 3 masters for a HA cluster. (You will need a miniumum of OVS/OVN 2.9.2 for clustering.)</p>"},{"location":"installation/ha/#master1-initialization","title":"Master1 initialization","text":"<p>To bootstrap your cluster, you need to start on one master. For a lack of better name, let's call it MASTER1 with an IP address of $MASTER1</p> <p>On MASTER1, delete any stale OVN databases and stop any ovn-northd running. e.g:</p> <pre><code>sudo /usr/share/openvswitch/scripts/ovn-ctl stop_nb_ovsdb\nsudo /usr/share/openvswitch/scripts/ovn-ctl stop_sb_ovsdb\nsudo rm /etc/openvswitch/ovn*.db\nsudo /usr/share/openvswitch/scripts/ovn-ctl stop_northd\n</code></pre> <p>Start the two databases on that host with:</p> <pre><code>LOCAL_IP=$MASTER1\nsudo /usr/share/openvswitch/scripts/ovn-ctl \\\n    --db-nb-cluster-local-addr=$LOCAL_IP start_nb_ovsdb\n\nsudo /usr/share/openvswitch/scripts/ovn-ctl \\\n    --db-sb-cluster-local-addr=$LOCAL_IP start_sb_ovsdb\n</code></pre>"},{"location":"installation/ha/#master2-master3-initialization","title":"Master2, Master3... initialization","text":"<p>Delete any stale databases and stop any running ovn-northd daemons. e.g:</p> <pre><code>sudo /usr/share/openvswitch/scripts/ovn-ctl stop_nb_ovsdb\nsudo /usr/share/openvswitch/scripts/ovn-ctl stop_sb_ovsdb\nsudo rm /etc/openvswitch/ovn*.db\nsudo /usr/share/openvswitch/scripts/ovn-ctl stop_northd\n</code></pre> <p>On master with a IP of $LOCAL_IP, start the databases and ask it to join $MASTER1</p> <pre><code>LOCAL_IP=$LOCAL_IP\nMASTER_IP=$MASTER1\n\nsudo /usr/share/openvswitch/scripts/ovn-ctl  \\\n    --db-nb-cluster-local-addr=$LOCAL_IP \\\n    --db-nb-cluster-remote-addr=$MASTER_IP start_nb_ovsdb\n\nsudo /usr/share/openvswitch/scripts/ovn-ctl  \\\n    --db-sb-cluster-local-addr=$LOCAL_IP \\\n    --db-sb-cluster-remote-addr=$MASTER_IP start_sb_ovsdb\n</code></pre> <p>This should get your cluster up and running. You can verify the status of your cluster with:</p> <pre><code>sudo ovs-appctl -t /var/run/openvswitch/ovnnb_db.ctl \\\n    cluster/status OVN_Northbound\n\nsudo ovs-appctl -t /var/run/openvswitch/ovnsb_db.ctl \\\n    cluster/status OVN_Southbound\n</code></pre>"},{"location":"installation/ha/#ovnkube-master-ha-setup","title":"ovnkube master HA setup","text":"<p>ovnkube master has 2 main components - cluster-manager and ovnkube-controller.</p> <p>Starting ovnkube with '-init-master', runs both the components.  It is also possible to run these components individually by starting 2 ovnkube's one with '-init-cluster-manager' and the other with '-init-ovnkube-controller'.</p> <p>On the master nodes, we can either    * start ovnkube with '-init-master'      This should be a deployment running on master nodes. Eg.</p> <p>IP1=\"$MASTER1\" IP2=\"$MASTER2\" IP3=\"$MASTER3\"</p> <p>ovn_nb=\"tcp:$IP1:6641,tcp:$IP2:6641,tcp:$IP3:6641\" ovn_sb=\"tcp:$IP1:6642,tcp:$IP2:6642,tcp:$IP3:6642\"</p> <p>nohup sudo ovnkube -k8s-kubeconfig kubeconfig.yaml \\  -loglevel=4 \\  -k8s-apiserver=\"http://$K8S_APISERVER_IP:8080\" \\  -logfile=\"/var/log/openvswitch/ovnkube.log\" \\  -init-master=\"$NODENAME\" -cluster-subnets=\"$CLUSTER_IP_SUBNET\" \\  -init-node=\"$NODENAME\" \\  -k8s-service-cidr=\"$SERVICE_IP_SUBNET\" \\  -k8s-token=\"$TOKEN\" \\  -nodeport \\  -nb-address=\"${ovn_nb}\" \\  -sb-address=\"${ovn_sb}\"  2&gt;&amp;1 &amp;</p> <ul> <li>start 'ovnkube -init-cluster-manager' and 'ovnkube -init-ovnkube-controller'    This should be a deployment with these 2 as containers</li> </ul> <p>Eg.</p> <p>ovnkube master supports running in 3 modes. init-master mode, init-cluster-manager mode or init-ovnkube-controller mode.  If ovnkube is run with \"-init-master\" mode, then there is no need to run the other modes because master mode enables both cluster-manager and ovnkube-controller.  If the user desires to run cluster-manager and ovnkube-controller separately, then it is possible to do so by running </p> <p>nohup sudo ovnkube -k8s-kubeconfig kubeconfig.yaml \\  -loglevel=4 \\  -k8s-apiserver=\"http://$K8S_APISERVER_IP:8080\" \\  -logfile=\"/var/log/openvswitch/ovnkube.log\" \\  -init-ovnkube-controller=\"$NODENAME\" -cluster-subnets=\"$CLUSTER_IP_SUBNET\" \\  -init-node=\"$NODENAME\" \\  -k8s-service-cidr=\"$SERVICE_IP_SUBNET\" \\  -k8s-token=\"$TOKEN\" \\  -nodeport \\  -nb-address=\"${ovn_nb}\" \\  -sb-address=\"${ovn_sb}\"  2&gt;&amp;1 &amp;</p> <p>nohup sudo ovnkube -k8s-kubeconfig kubeconfig.yaml \\  -loglevel=4 \\  -k8s-apiserver=\"http://$K8S_APISERVER_IP:8080\" \\  -logfile=\"/var/log/openvswitch/ovnkube.log\" \\  -init-cluster-manager=\"$NODENAME\" -cluster-subnets=\"$CLUSTER_IP_SUBNET\" \\  -init-node=\"$NODENAME\" \\  -k8s-service-cidr=\"$SERVICE_IP_SUBNET\" \\  -k8s-token=\"$TOKEN\" \\  -nodeport  2&gt;&amp;1 &amp;</p>"},{"location":"installation/ha/#start-ovn-northd","title":"start ovn-northd","text":"<p>On any one of the masters (ideally via a daemonset with replica count as 1), start ovn-northd. Let the 3 master IPs be $IP1, $IP2 and $IP3.</p> <pre><code>IP1=\"$MASTER1\"\nIP2=\"$MASTER2\"\nIP3=\"$MASTER3\"\n\nexport ovn_nb=\"tcp:$IP1:6641,tcp:$IP2:6641,tcp:$IP3:6641\"\nexport ovn_sb=\"tcp:$IP1:6642,tcp:$IP2:6642,tcp:$IP3:6642\"\n\nsudo ovn-northd -vconsole:emer -vsyslog:err -vfile:info \\\n    --ovnnb-db=\"$ovn_nb\" --ovnsb-db=\"$ovn_sb\" --no-chdir \\\n    --log-file=/var/log/openvswitch/ovn-northd.log \\\n    --pidfile=/var/run/openvswitch/ovn-northd.pid --detach --monitor\n</code></pre>"},{"location":"installation/ha/#start-ovn-kube-init-node","title":"Start 'ovn-kube -init-node'","text":"<p>On all nodes (and if needed on other masters), start ovnkube with '-init-node'. For e.g:</p> <pre><code>IP1=\"$MASTER1\"\nIP2=\"$MASTER2\"\nIP3=\"$MASTER3\"\n\novn_nb=\"tcp:$IP1:6641,tcp:$IP2:6641,tcp:$IP3:6641\"\novn_sb=\"tcp:$IP1:6642,tcp:$IP2:6642,tcp:$IP3:6642\"\n\nnohup sudo ovnkube -k8s-kubeconfig $HOME/kubeconfig.yaml -loglevel=4 \\\n    -logfile=\"/var/log/openvswitch/ovnkube.log\" \\\n    -k8s-apiserver=\"http://$K8S_APISERVER_IP:8080\" \\\n    -init-node=\"$NODE_NAME\"  \\\n    -nb-address=\"${ovn_nb}\" \\\n    -sb-address=\"${ovn_sb}\" \\\n    -k8s-token=\"$TOKEN\" \\\n    -init-gateways \\\n    -k8s-service-cidr= \\\n    -cluster-subnets=\"$SERVICE_IP_SUBNET\" 2&gt;&amp;1 &amp;\n</code></pre>"},{"location":"installation/kind/","title":"OVN kubernetes KIND Setup","text":"<p>KIND (Kubernetes in Docker) deployment of OVN kubernetes is a fast and easy means to quickly install and test kubernetes with OVN kubernetes CNI. The value proposition is really for developers who want to reproduce an issue or test a fix in an environment that can be brought up locally and within a few minutes.</p>"},{"location":"installation/kind/#prerequisites","title":"Prerequisites","text":"<ul> <li>20 GB of free space in root file system</li> <li>Docker run time or podman</li> <li>KIND</li> <li>Installation instructions can be found at https://github.com/kubernetes-sigs/kind#installation-and-usage. </li> <li> <p>NOTE: The OVN-Kubernetes ovn-kubernetes/contrib/kind.sh and ovn-kubernetes/contrib/kind.yaml files provision port 11337. If firewalld is enabled, this port will need to be unblocked:</p> <p><pre><code>sudo firewall-cmd --permanent --add-port=11337/tcp; sudo firewall-cmd --reload\n</code></pre> - kubectl - Python and pip - jq</p> </li> </ul> <p>NOTE :  In certain operating systems such as CentOS 8.x, pip2 and pip3 binaries are installed instead of pip. In such situations create a softlink for \"pip\" that points to \"pip2\".</p>"},{"location":"installation/kind/#run-the-kind-deployment-with-docker","title":"Run the KIND deployment with docker","text":"<p>For OVN kubernetes KIND deployment, use the <code>kind.sh</code> script.</p> <p>First Download and build the OVN-Kubernetes repo: </p> <pre><code>$ go env -w GO111MODULE=auto\n$ go get github.com/ovn-org/ovn-kubernetes; cd $(go env GOPATH)/src/github.com/ovn-org/ovn-kubernetes\n</code></pre> <p>The <code>kind.sh</code> script builds OVN-Kubernetes into a container image. To verify local changes before building in KIND, run the following:</p> <pre><code>$ pushd go-controller\n$ make\n$ popd\n\n$ pushd dist/images\n$ make fedora\n$ popd\n</code></pre> <p>Launch the KIND Deployment.</p> <pre><code>$ pushd contrib\n$ export KUBECONFIG=${HOME}/ovn.conf\n$ ./kind.sh\n$ popd\n</code></pre> <p>This will launch a KIND deployment. By default the cluster is named <code>ovn</code>.</p> <pre><code>$ kubectl get nodes\nNAME                STATUS   ROLES    AGE     VERSION\novn-control-plane   Ready    master   5h13m   v1.16.4\novn-worker          Ready    &lt;none&gt;   5h12m   v1.16.4\novn-worker2         Ready    &lt;none&gt;   5h12m   v1.16.4\n\n$ kubectl get pods --all-namespaces\nNAMESPACE            NAME                                        READY   STATUS    RESTARTS   AGE\nkube-system          coredns-5644d7b6d9-kw2xc                    1/1     Running   0          5h13m\nkube-system          coredns-5644d7b6d9-sd9wh                    1/1     Running   0          5h13m\nkube-system          etcd-ovn-control-plane                      1/1     Running   0          5h11m\nkube-system          kube-apiserver-ovn-control-plane            1/1     Running   0          5h12m\nkube-system          kube-controller-manager-ovn-control-plane   1/1     Running   0          5h12m\nkube-system          kube-scheduler-ovn-control-plane            1/1     Running   0          5h11m\nlocal-path-storage   local-path-provisioner-7745554f7f-9r8dz     1/1     Running   0          5h13m\novn-kubernetes       ovnkube-db-5588bd699c-kb8h7                 2/2     Running   0          5h11m\novn-kubernetes       ovnkube-master-6f44d456df-bv2x8             2/2     Running   0          5h11m\novn-kubernetes       ovnkube-node-2t6m2                          3/3     Running   0          5h11m\novn-kubernetes       ovnkube-node-hhsmk                          3/3     Running   0          5h11m\novn-kubernetes       ovnkube-node-xvqh4                          3/3     Running   0          5h11m\n</code></pre> <p>The <code>kind.sh</code> script defaults the cluster to HA disabled. There are numerous configuration options when deploying. Use <code>./kind.sh -h</code> to see the latest options.</p> <pre><code>[root@ovnkubernetes contrib]# ./kind.sh --help\nusage: kind.sh [[[-cf |--config-file &lt;file&gt;] [-kt|keep-taint] [-ha|--ha-enabled]\n                 [-ho |--hybrid-enabled] [-ii|--install-ingress] [-n4|--no-ipv4]\n                 [-i6 |--ipv6] [-wk|--num-workers &lt;num&gt;] [-ds|--disable-snat-multiple-gws]\n                 [-dp |--disable-pkt-mtu-check] [-df|--disable-forwarding]\n                 [-nf |--netflow-targets &lt;targets&gt;] [sf|--sflow-targets &lt;targets&gt;]\n                 [-if |--ipfix-targets &lt;targets&gt;] [-ifs|--ipfix-sampling &lt;num&gt;]\n                 [-ifm|--ipfix-cache-max-flows &lt;num&gt;] [-ifa|--ipfix-cache-active-timeout &lt;num&gt;]\n                 [-sw |--allow-system-writes] [-gm|--gateway-mode &lt;mode&gt;]\n                 [-nl |--node-loglevel &lt;num&gt;] [-ml|--master-loglevel &lt;num&gt;]\n                 [-dbl|--dbchecker-loglevel &lt;num&gt;] [-ndl|--ovn-loglevel-northd &lt;loglevel&gt;]\n                 [-nbl|--ovn-loglevel-nb &lt;loglevel&gt;] [-sbl|--ovn-loglevel-sb &lt;loglevel&gt;]\n                 [-cl |--ovn-loglevel-controller &lt;loglevel&gt;] [-me|--multicast-enabled]\n                 [-ep |--experimental-provider &lt;name&gt;] |\n                 [-eb |--egress-gw-separate-bridge]\n                 [-h]]\n\n-cf  | --config-file                Name of the KIND J2 configuration file.\n                                    DEFAULT: ./kind.yaml.j2\n-kt  | --keep-taint                 Do not remove taint components.\n                                    DEFAULT: Remove taint components.\n-ha  | --ha-enabled                 Enable high availability. DEFAULT: HA Disabled.\n-me  | --multicast-enabled          Enable multicast. DEFAULT: Disabled.\n-scm | --separate-cluster-manager   Separate cluster manager from ovnkube-master and run as a separate container within ovnkube-master deployment.\n-ho  | --hybrid-enabled             Enable hybrid overlay. DEFAULT: Disabled.\n-ds  | --disable-snat-multiple-gws  Disable SNAT for multiple gws. DEFAULT: Disabled.\n-dp  | --disable-pkt-mtu-check      Disable checking packet size greater than MTU. Default: Disabled\n-df  | --disable-forwarding         Disable forwarding on OVNK controlled interfaces. Default: Enabled\n-nf  | --netflow-targets            Comma delimited list of ip:port or :port (using node IP) netflow collectors. DEFAULT: Disabled.\n-sf  | --sflow-targets              Comma delimited list of ip:port or :port (using node IP) sflow collectors. DEFAULT: Disabled.\n-if  | --ipfix-targets              Comma delimited list of ip:port or :port (using node IP) ipfix collectors. DEFAULT: Disabled.\n-ifs | --ipfix-sampling             Fraction of packets that are sampled and sent to each target collector: 1 packet out of every &lt;num&gt;. DEFAULT: 400 (1 out of 400 packets).\n-ifm | --ipfix-cache-max-flows      Maximum number of IPFIX flow records that can be cached at a time. If 0, caching is disabled. DEFAULT: Disabled.\n-ifa | --ipfix-cache-active-timeout Maximum period in seconds for which an IPFIX flow record is cached and aggregated before being sent. If 0, caching is disabled. DEFAULT: 60.\n-el  | --ovn-empty-lb-events        Enable empty-lb-events generation for LB without backends. DEFAULT: Disabled\n-ii  | --install-ingress            Flag to install Ingress Components.\n                                    DEFAULT: Don't install ingress components.\n-n4  | --no-ipv4                    Disable IPv4. DEFAULT: IPv4 Enabled.\n-i6  | --ipv6                       Enable IPv6. DEFAULT: IPv6 Disabled.\n-wk  | --num-workers                Number of worker nodes. DEFAULT: HA - 2 worker\n                                    nodes and no HA - 0 worker nodes.\n-sw  | --allow-system-writes        Allow script to update system. Intended to allow\n                                    github CI to be updated with IPv6 settings.\n                                    DEFAULT: Don't allow.\n-gm  | --gateway-mode               Enable 'shared' or 'local' gateway mode.\n                                    DEFAULT: shared.\n-ov  | --ovn-image                  Use the specified docker image instead of building locally. DEFAULT: local build.\n-ml  | --master-loglevel            Log level for ovnkube (master), DEFAULT: 5.\n-nl  | --node-loglevel              Log level for ovnkube (node), DEFAULT: 5\n-dbl | --dbchecker-loglevel         Log level for ovn-dbchecker (ovnkube-db), DEFAULT: 5.\n-ndl | --ovn-loglevel-northd        Log config for ovn northd, DEFAULT: '-vconsole:info -vfile:info'.\n-nbl | --ovn-loglevel-nb            Log config for northbound DB DEFAULT: '-vconsole:info -vfile:info'.\n-sbl | --ovn-loglevel-sb            Log config for southboudn DB DEFAULT: '-vconsole:info -vfile:info'.\n-cl  | --ovn-loglevel-controller    Log config for ovn-controller DEFAULT: '-vconsole:info'.\n-ep  | --experimental-provider      Use an experimental OCI provider such as podman, instead of docker. DEFAULT: Disabled.\n-eb  | --egress-gw-separate-bridge  The external gateway traffic uses a separate bridge.\n-lr  |--local-kind-registry         Will start and connect a kind local registry to push/retrieve images\n--delete                            Delete current cluster\n--deploy                            Deploy ovn kubernetes without restarting kind\n</code></pre> <p>As seen above, if you do not specify any options the script will assume the default values.</p>"},{"location":"installation/kind/#run-the-kind-deployment-with-podman","title":"Run the KIND deployment with podman","text":"<p>To verify local changes, the steps are mostly the same as with docker, except the <code>fedora</code> make target:</p> <pre><code>$ OCI_BIN=podman make fedora\n</code></pre> <p>To deploy KIND however, you need to start it as root and then copy root's kube config to use it as non-root:</p> <pre><code>$ pushd contrib\n$ sudo ./kind.sh -ep podman\n$ sudo cp /root/ovn.conf ~/.kube/kind-config\n$ sudo chown $(id -u):$(id -g) ~/.kube/kind-config\n$ export KUBECONFIG=~/.kube/kind-config\n$ popd\n</code></pre> <p>Notes / troubleshooting:</p> <ul> <li>Issue with /dev/dma_heap: if you get the error <code>kind \"Error: open /dev/dma_heap: permission denied\"</code>, there's a known issue about it (directory mislabelled with selinux). Workaround:</li> </ul> <pre><code>sudo setenforce 0\nsudo chcon system\\_u:object\\_r:device\\_t:s0 /dev/dma\\_heap/\nsudo setenforce 1\n</code></pre> <ul> <li>If you see errors related to go, you may not have go <code>$PATH</code> configured as root. Make sure it is configured, or define it while running <code>kind.sh</code>:</li> </ul> <pre><code>sudo PATH=$PATH:/usr/local/go/bin ./kind.sh -ep podman\n</code></pre>"},{"location":"installation/kind/#usage-notes","title":"Usage Notes","text":"<ul> <li> <p>You can create your own KIND J2 configuration file if the default one is not sufficient</p> </li> <li> <p>You can also specify these values as environment variables. Command line parameters will override the environment variables.</p> </li> <li> <p>To tear down the KIND cluster when finished simply run </p> </li> </ul> <pre><code>$ ./kind.sh --delete\n</code></pre>"},{"location":"installation/kind/#running-ovn-kubernetes-with-ipv6-or-dual-stack-in-kind","title":"Running OVN-Kubernetes with IPv6 or Dual-stack In KIND","text":"<p>This section describes the configuration needed for IPv6 and dual-stack environments.</p>"},{"location":"installation/kind/#kind-with-ipv6","title":"KIND with IPv6","text":""},{"location":"installation/kind/#docker-changes-for-ipv6","title":"Docker Changes For IPv6","text":"<p>For KIND clusters using KIND v0.7.0 or older (CI currently is using v0.8.1), to use IPv6, IPv6 needs to be enable in Docker on the host:</p> <pre><code>$ sudo vi /etc/docker/daemon.json\n{\n  \"ipv6\": true\n}\n\n$ sudo systemctl reload docker\n</code></pre> <p>On a CentOS host running Docker version 19.03.6, the above configuration worked. After the host was rebooted, Docker failed to start. To fix, change <code>daemon.json</code> as follows:</p> <pre><code>$ sudo vi /etc/docker/daemon.json\n{\n  \"ipv6\": true,\n  \"fixed-cidr-v6\": \"2001:db8:1::/64\"\n}\n\n$ sudo systemctl reload docker\n</code></pre> <p>IPv6 from Docker repo provided the fix. Newer documentation does not include this change, so change may be dependent on Docker version.</p> <p>To verify IPv6 is enabled in Docker, run:</p> <pre><code>$ docker run --rm busybox ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n341: eth0@if342: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue\n    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 2001:db8:1::242:ac11:2/64 scope global flags 02\n       valid_lft forever preferred_lft forever\n    inet6 fe80::42:acff:fe11:2/64 scope link tentative\n       valid_lft forever preferred_lft forever\n</code></pre> <p>For the eth0 vEth-pair, there should be the two IPv6 entries (global and link addresses).</p>"},{"location":"installation/kind/#disable-firewalld","title":"Disable firewalld","text":"<p>Currently, to run OVN-Kubernetes with IPv6 only in a KIND deployment, firewalld needs to be disabled. To disable:</p> <pre><code>sudo systemctl stop firewalld\n</code></pre> <p>NOTE: To run with IPv4, firewalld needs to be enabled, so to reenable:</p> <pre><code>sudo systemctl start firewalld\n</code></pre> <p>If firewalld is enabled during a IPv6 deployment, additional nodes fail to join the cluster:</p> <pre><code>:\nCreating cluster \"ovn\" ...\n \u2713 Ensuring node image (kindest/node:v1.18.2) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6 \ud83d\udce6 \ud83d\udce6\n \u2713 Writing configuration \ud83d\udcdc\n \u2713 Starting control-plane \ud83d\udd79\ufe0f\n \u2713 Installing StorageClass \ud83d\udcbe\n \u2717 Joining worker nodes \ud83d\ude9c\nERROR: failed to create cluster: failed to join node with kubeadm: command \"docker exec --privileged ovn-worker kubeadm join --config /kind/kubeadm.conf --ignore-preflight-errors=all --v=6\" failed with error: exit status 1\n</code></pre> <p>And logs show:</p> <pre><code>I0430 16:40:44.590181     579 token.go:215] [discovery] Failed to request cluster-info, will try again: Get https://[2001:db8:1::242:ac11:3]:6443/api/v1/namespaces/kube-public/configmaps/cluster-info?timeout=10s: dial tcp [2001:db8:1::242:ac11:3]:6443: connect: permission denied\nGet https://[2001:db8:1::242:ac11:3]:6443/api/v1/namespaces/kube-public/configmaps/cluster-info?timeout=10s: dial tcp [2001:db8:1::242:ac11:3]:6443: connect: permission denied\n</code></pre> <p>This issue was reported upstream in KIND 1257 and blamed on firewalld.</p>"},{"location":"installation/kind/#ovn-kubernetes-with-ipv6","title":"OVN-Kubernetes With IPv6","text":"<p>To run OVN-Kubernetes with IPv6 in a KIND deployment, run:</p> <pre><code>$ go get github.com/ovn-org/ovn-kubernetes; cd $GOPATH/src/github.com/ovn-org/ovn-kubernetes\n\n$ cd go-controller/\n$ make\n\n$ cd ../dist/images/\n$ make fedora\n\n$ cd ../../contrib/\n$ KIND_IPV4_SUPPORT=false KIND_IPV6_SUPPORT=true ./kind.sh\n</code></pre> <p>Once <code>kind.sh</code> completes, setup kube config file:</p> <pre><code>$ cp ~/ovn.conf ~/.kube/config\n-- OR --\n$ KUBECONFIG=~/ovn.conf\n</code></pre> <p>Once testing is complete, to tear down the KIND deployment:</p> <pre><code>$ kind delete cluster --name ovn\n</code></pre>"},{"location":"installation/kind/#kind-with-dual-stack","title":"KIND with Dual-stack","text":"<p>Currently, IP dual-stack is not fully supported in: * Kubernetes * KIND * OVN-Kubernetes</p>"},{"location":"installation/kind/#kubernetes-and-docker-with-ip-dual-stack","title":"Kubernetes And Docker With IP Dual-stack","text":""},{"location":"installation/kind/#update-kubectl","title":"Update kubectl","text":"<p>Kubernetes has some IP dual-stack support but the feature is not complete. Additional changes are constantly being added. This setup is using the latest Kubernetes release to test against. Kubernetes is being installed below using OVN-Kubernetes KIND script, however to test, an equivalent version of <code>kubectl</code> needs to be installed.</p> <p>First determine what version of <code>kubectl</code> is currently being used and save it:</p> <pre><code>$ which kubectl\n/usr/bin/kubectl\n$ kubectl version --client\nClient Version: version.Info{Major:\"1\", Minor:\"28\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T18:14:22Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nsudo mv /usr/bin/kubectl /usr/bin/kubectl-v1.17.3\nsudo ln -s /usr/bin/kubectl-v1.17.3 /usr/bin/kubectl\n</code></pre> <p>Download and install latest version of <code>kubectl</code>:</p> <pre><code>$ K8S_VERSION=v1.29.2\n$ curl -LO https://storage.googleapis.com/kubernetes-release/release/$K8S_VERSION/bin/linux/amd64/kubectl\n$ chmod +x kubectl\n$ sudo mv kubectl /usr/bin/kubectl-$K8S_VERSION\n$ sudo rm /usr/bin/kubectl\n$ sudo ln -s /usr/bin/kubectl-$K8S_VERSION /usr/bin/kubectl\n$ kubectl version --client\nClient Version: version.Info{Major:\"1\", Minor:\"28\", GitVersion:\"v1.18.0\", GitCommit:\"9e991415386e4cf155a24b1da15becaa390438d8\", GitTreeState:\"clean\", BuildDate:\"2020-03-25T14:58:59Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n</code></pre>"},{"location":"installation/kind/#docker-changes-for-dual-stack","title":"Docker Changes For Dual-stack","text":"<p>For dual-stack, IPv6 needs to be enable in Docker on the host same as for IPv6 only. See above: Docker Changes For IPv6</p>"},{"location":"installation/kind/#kind-with-ip-dual-stack","title":"KIND With IP Dual-stack","text":"<p>IP dual-stack is not currently supported in KIND. There is a PR (692) with IP dual-stack changes. Currently using this to test with.</p> <p>Optionally, save previous version of KIND (if it exists):</p> <pre><code>cp $GOPATH/bin/kind $GOPATH/bin/kind.orig\n</code></pre>"},{"location":"installation/kind/#build-kind-with-dual-stack-locally","title":"Build KIND With Dual-stack Locally","text":"<p>To build locally (if additional needed):</p> <pre><code>go get github.com/kubernetes-sigs/kind; cd $GOPATH/src/github.com/kubernetes-sigs/kind\ngit pull --no-edit --strategy=ours origin pull/692/head\nmake clean\nmake install INSTALL_DIR=$GOPATH/bin\n</code></pre>"},{"location":"installation/kind/#ovn-kubernetes-with-ip-dual-stack","title":"OVN-Kubernetes With IP Dual-stack","text":"<p>For status of IP dual-stack in OVN-Kubernetes, see 1142.</p> <p>To run OVN-Kubernetes with IP dual-stack in a KIND deployment, run:</p> <pre><code>$ go get github.com/ovn-org/ovn-kubernetes; cd $GOPATH/src/github.com/ovn-org/ovn-kubernetes\n\n$ cd go-controller/\n$ make\n\n$ cd ../dist/images/\n$ make fedora\n\n$ cd ../../contrib/\n$ KIND_IPV4_SUPPORT=true KIND_IPV6_SUPPORT=true K8S_VERSION=v1.29.2 ./kind.sh\n</code></pre> <p>Once <code>kind.sh</code> completes, setup kube config file:</p> <pre><code>$ cp ~/ovn.conf ~/.kube/config\n-- OR --\n$ KUBECONFIG=~/ovn.conf\n</code></pre> <p>Once testing is complete, to tear down the KIND deployment:</p> <pre><code>$ kind delete cluster --name ovn\n</code></pre>"},{"location":"installation/kind/#using-specific-kind-container-image-and-tag","title":"Using specific Kind container image and tag","text":"<p> Use with caution, as kind expects this image to have all it needs.</p> <p>In order to use an image/tag other than the default hardcoded in kind.sh, specify one (or both of) the following variables:</p> <pre><code>$ cd ../../contrib/\n$ KIND_IMAGE=example.com/kindest/node K8S_VERSION=v1.29.2 ./kind.sh\n</code></pre>"},{"location":"installation/kind/#using-kind-local-registry-to-deploy-non-ovn-k-containers","title":"Using kind local registry to deploy non ovn-k containers","text":"<p>A local registry can be made available to the cluster if started with: <pre><code>./kind.sh --local-kind-registry\n</code></pre> This is useful if you want to make your own local images available to the  cluster. These images can be pushed, fetched or used  in manifests using the prefix <code>localhost:5000</code>.</p>"},{"location":"installation/kind/#loading-ovn-kubernetes-changes-without-restarting-kind","title":"Loading ovn-kubernetes changes without restarting kind","text":"<p>Sometimes it is useful to update ovn-kubernetes without redeploying the whole  cluster all over again. For example, when testing the update itself.  This can be achieve with the \"--deploy\" flag:</p> <pre><code># Default options will use kind mechanism to push images directly to the\n./kind.sh --deploy\n\n# Using a local registry is an alternative to deploy ovn-kubernetes updates \n# while also being useful to deploy other local images\n./kind.sh --deploy --local-kind-registry\n</code></pre>"},{"location":"installation/kind/#current-status","title":"Current Status","text":"<p>This is subject to change because code is being updated constantly. But this is more a cautionary note that this feature is not completely working at the moment.</p> <p>The nodes do not go to ready because the OVN-Kubernetes hasn't setup the network completely:</p> <pre><code>$ kubectl get nodes\nNAME                STATUS     ROLES    AGE   VERSION\novn-control-plane   NotReady   master   94s   v1.18.0\novn-worker          NotReady   &lt;none&gt;   61s   v1.18.0\novn-worker2         NotReady   &lt;none&gt;   62s   v1.18.0\n\n$ kubectl get pods -o wide --all-namespaces\nNAMESPACE          NAME                                      READY STATUS   RESTARTS AGE    IP          NODE\nkube-system        coredns-66bff467f8-hh4c9                  0/1   Pending  0        2m45s  &lt;none&gt;      &lt;none&gt;\nkube-system        coredns-66bff467f8-vwbcj                  0/1   Pending  0        2m45s  &lt;none&gt;      &lt;none&gt;\nkube-system        etcd-ovn-control-plane                    1/1   Running  0        2m56s  172.17.0.2  ovn-control-plane\nkube-system        kube-apiserver-ovn-control-plane          1/1   Running  0        2m56s  172.17.0.2  ovn-control-plane\nkube-system        kube-controller-manager-ovn-control-plane 1/1   Running  0        2m56s  172.17.0.2  ovn-control-plane\nkube-system        kube-scheduler-ovn-control-plane          1/1   Running  0        2m56s  172.17.0.2  ovn-control-plane\nlocal-path-storage local-path-provisioner-774f7f8fdb-msmd2   0/1   Pending  0        2m45s  &lt;none&gt;      &lt;none&gt;\novn-kubernetes     ovnkube-db-cf4cc89b7-8d4xq                2/2   Running  0        107s   172.17.0.2  ovn-control-plane\novn-kubernetes     ovnkube-master-87fb56d6d-7qmnb            2/2   Running  0        107s   172.17.0.2  ovn-control-plane\novn-kubernetes     ovnkube-node-278l9                        2/3   Running  0        107s   172.17.0.3  ovn-worker2\novn-kubernetes     ovnkube-node-bm7v6                        2/3   Running  0        107s   172.17.0.2  ovn-control-plane\novn-kubernetes     ovnkube-node-p4k4t                        2/3   Running  0        107s   172.17.0.4  ovn-worker\n</code></pre>"},{"location":"installation/kind/#known-issues","title":"Known issues","text":"<p>Some environments (Fedora32,31 on desktop), have problems when the cluster is deleted directly with kind <code>kind delete cluster --name ovn</code>, it restarts the host. The root cause is unknown, this also can not be reproduced in Ubuntu 20.04 or with Fedora32 Cloud, but it does not happen if we clean first the ovn-kubernetes resources.</p> <p>You can use the following command to delete the cluster:</p> <p><code>contrib/kind.sh --delete</code></p>"},{"location":"observability/metrics/","title":"Metrics","text":""},{"location":"observability/metrics/#ovn-kubernetes-master","title":"OVN-Kubernetes master","text":"<p>This includes a description of a selective set of metrics and to explore the exhausted set, see <code>go-controller/pkg/metrics/master.go</code></p>"},{"location":"observability/metrics/#configuration-duration-recorder","title":"Configuration duration recorder","text":""},{"location":"observability/metrics/#setup","title":"Setup","text":"<p>Enabled by default with the <code>kind.sh</code> (in directory <code>$ROOT/contrib</code>) Kind setup script. Disabled by default for binary ovnkube-master and enabled with flag <code>--metrics-enable-config-duration</code>.</p>"},{"location":"observability/metrics/#high-level-description","title":"High-level description","text":"<p>This set of metrics gives a result for the upper bound duration which means, it has taken at most this amount of seconds to apply the configuration to all nodes. It does not represent the exact accurate time to apply only this configuration. Measurement accuracy can be impacted by other parallel processing that might be occurring while the measurement is in progress therefore, the accuracy of the measurements should only indicate upper bound duration to roll out configuration changes.</p>"},{"location":"observability/metrics/#metrics_1","title":"Metrics","text":"Name Prometheus type Description ovnkube_master_network_programming_duration_seconds Histogram The duration to apply network configuration for a kind (e.g. pod, service, networkpolicy). Configuration includes add, update and delete events for kinds. This includes OVN-Kubernetes master and OVN duration. ovnkube_master_network_programming_ovn_duration_seconds Histogram The duration for OVN to apply network configuration for a kind (e.g. pod, service, networkpolicy)."},{"location":"observability/metrics/#change-log","title":"Change log","text":"<p>This list is to help notify if there are additions, changes or removals to metrics. Latest changes are at the top of this list.</p> <ul> <li>Add metrics to track logfile size for ovnkube processes - ovnkube_node_logfile_size_bytes and ovnkube_controller_logfile_size_bytes</li> <li>Remove ovnkube_controller_ovn_cli_latency_seconds metrics since we have moved most of the OVN DB operations to libovsdb.</li> <li>Effect of OVN IC architecture:</li> <li>Move all the metrics from subsystem \"ovnkube-master\" to subsystem \"ovnkube-controller\". The non-IC and IC deployments will each continue to have their ovnkube-master and ovnkube-controller containers running inside the ovnkube-master and ovnkube-controller pods. The metrics scraping should work seemlessly. See https://github.com/ovn-org/ovn-kubernetes/pull/3723 for details</li> <li>Move the following metrics from subsystem \"master\" to subsystem \"clustermanager\". Therefore, the follow metrics are renamed.<ul> <li><code>ovnkube_master_num_v4_host_subnets</code> -&gt; <code>ovnkube_clustermanager_num_v4_host_subnets</code></li> <li><code>ovnkube_master_num_v6_host_subnets</code> -&gt; <code>ovnkube_clustermanager_num_v6_host_subnets</code></li> <li><code>ovnkube_master_allocated_v4_host_subnets</code> -&gt; <code>ovnkube_clustermanager_allocated_v4_host_subnets</code></li> <li><code>ovnkube_master_allocated_v6_host_subnets</code> -&gt; <code>ovnkube_clustermanager_allocated_v6_host_subnets</code></li> <li><code>ovnkube_master_num_egress_ips</code> -&gt; <code>ovnkube_clustermanager_num_egress_ips</code></li> <li><code>ovnkube_master_egress_ips_node_unreachable_total</code> -&gt; <code>ovnkube_clustermanager_egress_ips_node_unreachable_total</code></li> <li><code>ovnkube_master_egress_ips_rebalance_total</code> -&gt; <code>ovnkube_clustermanager_egress_ips_rebalance_total</code></li> </ul> </li> <li>Update description of ovnkube_master_pod_creation_latency_seconds</li> <li>Add libovsdb metrics - ovnkube_master_libovsdb_disconnects_total and ovnkube_master_libovsdb_monitors.</li> <li>Add ovn_controller_southbound_database_connected metric (https://github.com/ovn-org/ovn-kubernetes/pull/3117).</li> <li>Stopwatch metrics now report in seconds instead of milliseconds.</li> <li>Rename (https://github.com/ovn-org/ovn-kubernetes/pull/3022):</li> <li><code>ovs_vswitchd_interface_link_resets</code> -&gt; <code>ovs_vswitchd_interface_resets_total</code></li> <li><code>ovs_vswitchd_interface_rx_dropped</code> -&gt; <code>ovs_vswitchd_interface_rx_dropped_total</code></li> <li><code>ovs_vswitchd_interface_tx_dropped</code> -&gt; <code>ovs_vswitchd_interface_tx_dropped_total</code></li> <li><code>ovs_vswitchd_interface_rx_errors</code> -&gt; <code>ovs_vswitchd_interface_rx_errors_total</code></li> <li><code>ovs_vswitchd_interface_tx_errors</code> -&gt; <code>ovs_vswitchd_interface_tx_errors_total</code></li> <li><code>ovs_vswitchd_interface_collisions</code> -&gt; <code>ovs_vswitchd_interface_collisions_total</code></li> <li>Remove (https://github.com/ovn-org/ovn-kubernetes/pull/3022):</li> <li><code>ovs_vswitchd_dp_if</code></li> <li><code>ovs_vswitchd_interface_driver_name</code></li> <li><code>ovs_vswitchd_interface_driver_version</code></li> <li><code>ovs_vswitchd_interface_firmware_version</code></li> <li><code>ovs_vswitchd_interface_rx_packets</code></li> <li><code>ovs_vswitchd_interface_tx_packets</code></li> <li><code>ovs_vswitchd_interface_rx_bytes</code></li> <li><code>ovs_vswitchd_interface_tx_bytes</code></li> <li><code>ovs_vswitchd_interface_rx_frame_err</code></li> <li><code>ovs_vswitchd_interface_rx_over_err</code></li> <li><code>ovs_vswitchd_interface_rx_crc_err</code></li> <li><code>ovs_vswitchd_interface_name</code></li> <li><code>ovs_vswitchd_interface_duplex</code></li> <li><code>ovs_vswitchd_interface_type</code></li> <li><code>ovs_vswitchd_interface_admin_state</code></li> <li><code>ovs_vswitchd_interface_link_state</code></li> <li><code>ovs_vswitchd_interface_ifindex</code></li> <li><code>ovs_vswitchd_interface_link_speed</code></li> <li><code>ovs_vswitchd_interface_mtu</code></li> <li><code>ovs_vswitchd_interface_ofport</code></li> <li><code>ovs_vswitchd_interface_ingress_policing_burst</code></li> <li><code>ovs_vswitchd_interface_ingress_policing_rate</code></li> <li>Add <code>ovnkube_master_network_programming_duration_seconds</code> and <code>ovnkube_master_network_programming_ovn_duration_seconds</code> (https://github.com/ovn-org/ovn-kubernetes/pull/2878)</li> <li>Remove <code>ovnkube_master_skipped_nbctl_daemon_total</code> (https://github.com/ovn-org/ovn-kubernetes/pull/2707)</li> <li>Add <code>ovnkube_master_egress_routing_via_host</code> (https://github.com/ovn-org/ovn-kubernetes/pull/2833)</li> <li>Add <code>ovnkube_resource_retry_failures_total</code> (https://github.com/ovn-org/ovn-kubernetes/pull/3314)</li> <li>Add <code>ovs_vswitchd_interfaces_total</code> and <code>ovs_vswitchd_interface_up_wait_seconds_total</code> (https://github.com/ovn-org/ovn-kubernetes/pull/3391)</li> <li>Add <code>ovnkube_controller_admin_network_policies</code> and <code>ovnkube_controller_baseline_admin_network_policies</code> (https://github.com/ovn-org/ovn-kubernetes/pull/4239)</li> <li>Add <code>ovnkube_controller_admin_network_policies_db_objects</code> and <code>ovnkube_controller_baseline_admin_network_policies_db_objects</code> (https://github.com/ovn-org/ovn-kubernetes/pull/4254)</li> </ul>"},{"location":"troubleshooting/debugging/","title":"Introduction","text":""},{"location":"troubleshooting/debugging/#initial-setup-issues","title":"Initial setup issues.","text":""},{"location":"troubleshooting/debugging/#each-host-should-have-unique-system-ids","title":"Each host should have unique system-ids.","text":"<p>In a OVN cluster, each host should have a unique system-id.  This value is set in external-ids:system-id of the Open_vSwitch table of each host's Open_vSwitch database.  You can fetch its value with:</p> <pre><code>ovs-vsctl get Open_vSwitch . external-ids:system-id\n</code></pre> <p>This is automatically set by OVS startup scripts (that come with packages). But if you installed OVS from source, and don't use a startup script, this will be empty.  If you clone your VM, then you may endup with same system-id for all your VMs - which is a problem.</p>"},{"location":"troubleshooting/debugging/#all-nodes-should-register-with-ovn-sb-database","title":"All nodes should register with OVN SB database.","text":"<p>On the master, run:</p> <pre><code>ovn-sbctl list chassis\n</code></pre> <p>You should see as many records as the number of nodes in your cluster.  The \"name\" column in each record represents the system-id of each host and they should all be unique.</p>"},{"location":"troubleshooting/debugging/#hosts-should-have-unique-node-names","title":"Hosts should have unique node names.","text":"<p>When you run 'ovnkube -init-node', 'ovnkube -init-master', 'ovnkube -init-network-control-manager' or 'ovnkube -init-cluster-manager' commands, it will ask for node names.  This should all be unique. This should also be the same as used by \"kubelet\" on each node. So the names you see when you run the below command should be the same ones that you supply to ovnkube:</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"troubleshooting/debugging/#check-the-presence-of-geneve-or-stt-kernel-module","title":"Check the presence of \"geneve\" or \"stt\" kernel module.","text":"<p>If you cannot ping between pods in 2 nodes, make sure to check whether you have the OVS kernel modules correctly installed by running:</p> <pre><code>lsmod | grep openvswitch\nlsmod | grep geneve  # Or \"stt\" if your encap type was chosen to be \"stt\".\n</code></pre> <p>If you do not see the kernel module, modprobe them</p> <pre><code>modprobe openvswitch\nmodprobe vport-geneve  # or vport-stt for \"stt\".\n</code></pre>"},{"location":"troubleshooting/debugging/#sanity-check-cross-host-ping","title":"Sanity check cross host ping.","text":"<p>Since on each host, a OVS internal device named \"k8s-$NODENAME\" is created with the second IP address in the subnet assigned for that node, you should be able to check cross host connectivity by pinging these internal devices across hosts.</p>"},{"location":"troubleshooting/debugging/#firewall-blocking-overlay-networks","title":"Firewall blocking overlay networks","text":"<p>If your host has a firewall blocking incoming connections as a default policy, you should open up ports to allow overlay networks.  If you use \"geneve\" as the encapsulation type, you should open up UDP port 6081.  If you use \"stt\" as the encapsulation type, you should open up TCP port 7471.</p> <p>You can use the following command to achieve it via iptables.</p> <pre><code># To open up Geneve port.\n/usr/share/openvswitch/scripts/ovs-ctl --protocol=udp \\\n        --dport=6081 enable-protocol\n\n# To open up STT port.\n/usr/share/openvswitch/scripts/ovs-ctl --protocol=tcp \\\n        --dport=7471 enable-protocol\n</code></pre>"},{"location":"troubleshooting/debugging/#check-ovn-northds-log-file","title":"Check ovn-northd's log file.","text":"<p>On the master, look at /var/log/openvswitch/ovn-northd.log to see for any errors with the setup of the OVN central node.</p>"},{"location":"troubleshooting/debugging/#runtime-issues","title":"Runtime issues","text":""},{"location":"troubleshooting/debugging/#check-the-watchers-log-file","title":"Check the watcher's log file.","text":"<p>On the master, check whether ovnkube is running by:</p> <pre><code>ps -ef | grep ovnkube\n</code></pre> <p>Check the watcher's log file at \"/var/log/ovn-kubernetes/ovnkube.log\" to see whether it is creating logical ports whenever a pod is created and for any obvious errors.</p>"},{"location":"troubleshooting/debugging/#check-the-ovn-cni-log-file","title":"Check the OVN CNI log file.","text":"<p>When you create a pod and it gets scheduled on a particular host, the OVN CNI plugin on that host, tries to access the pod's information from the K8s api server.  Specifically, it tries to get the IP address and mac address for that pod.  This information is logged in the OVN CNI log file on each node if you have specified a log file via \"/etc/openvswitch/ovn_k8s.conf\". You can read how to provide a logfile by reading 'man ovn_k8s.conf.5'.</p>"},{"location":"troubleshooting/debugging/#check-the-kubelets-log-file","title":"Check the kubelet's log file.","text":"<p>If there were any issues with downloading upstream CNI plugins, then kubelet will complain about them in its log file.</p>"},{"location":"troubleshooting/debugging/#check-ovn-controllers-log-file","title":"Check ovn-controller's log file.","text":"<p>If you suspect issues on only one of the host, look at the log file of ovn-controller at /var/log/openvswitch/ovn-controller.log to see any obvious error messages.</p>"},{"location":"troubleshooting/ovnkube-trace/","title":"ovnkube-trace","text":"<p>A tool to trace packet simulations for arbitrary UDP or TCP traffic between points in an ovn-kubernetes driven cluster.</p>"},{"location":"troubleshooting/ovnkube-trace/#usage","title":"Usage:","text":"<p>Given the command-line arguments, ovnkube-trace would inspect the cluster to determine the addresses (MAC and IP) of the source and destination and perform <code>ovn-trace</code>, <code>ovs-appctl ofproto/trace</code>, and <code>ovn-detrace</code> from/to both directions.</p> <pre><code>Usage of _output/go/bin/ovnkube-trace:\n  -addr-family string\n        Address family (ip4 or ip6) to be used for tracing (default \"ip4\")\n  -dst string\n        dest: destination pod name\n  -dst-ip string\n        destination IP address (meant for tests to external targets)\n  -dst-namespace string\n        k8s namespace of dest pod (default \"default\")\n  -dst-port string\n        dst-port: destination port (default \"80\")\n  -kubeconfig string\n        absolute path to the kubeconfig file\n  -loglevel string\n        loglevel: klog level (default \"0\")\n  -ovn-config-namespace string\n        namespace used by ovn-config itself\n  -service string\n        service: destination service name\n  -skip-detrace\n        skip ovn-detrace command\n  -src string\n        src: source pod name\n  -src-namespace string\n        k8s namespace of source pod (default \"default\")\n  -tcp\n        use tcp transport protocol\n  -udp\n        use udp transport protocol\n</code></pre> <p>Currently implemented loglevels are:  * <code>0</code> (minimal output) * <code>2</code> (more verbose output showing results of trace commands)  * and <code>5</code> (debug output)</p>"},{"location":"troubleshooting/ovnkube-trace/#example","title":"Example","text":"<p>In an environment between 2 pods in namespace <code>default</code>, where the pods are named <code>fedora-deployment-7d49fddf69-chmvh</code> and <code>fedora-deployment-7d49fddf69-t4hqw</code>, the goal would be to trace UDP traffic on port 53 between both pods. Each node in the cluster is running in a different interconnect zone. <pre><code># kubectl get pods -o wide\nNAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES\nfedora-deployment-7d49fddf69-chmvh   1/1     Running   0          10m   10.244.2.3   ovn-worker2         &lt;none&gt;           &lt;none&gt;\nfedora-deployment-7d49fddf69-t4hqw   1/1     Running   0          10m   10.244.1.6   ovn-worker          &lt;none&gt;           &lt;none&gt;\nfedora-deployment-7d49fddf69-vwjt7   1/1     Running   0          10m   10.244.0.3   ovn-control-plane   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>The command that one would run in this case would be: <pre><code>ovnkube-trace \\\n  -src-namespace default \\\n  -src fedora-deployment-7d49fddf69-chmvh \\\n  -dst-namespace default \\\n  -dst fedora-deployment-7d49fddf69-t4hqw \\\n  -udp -dst-port 53 \\\n  -loglevel 0\n</code></pre></p> <p>The result with loglevel 0 would be for a successful trace: <pre><code># ovnkube-trace -src-namespace default -src fedora-deployment-7d49fddf69-chmvh -dst-namespace default -dst fedora-deployment-7d49fddf69-t4hqw -udp -dst-port 53 -loglevel 0\novn-trace source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\novn-trace (remote) source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\novn-trace destination pod to source pod indicates success from fedora-deployment-7d49fddf69-t4hqw to fedora-deployment-7d49fddf69-chmvh\novn-trace (remote) destination pod to source pod indicates success from fedora-deployment-7d49fddf69-t4hqw to fedora-deployment-7d49fddf69-chmvh\novs-appctl ofproto/trace source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\novs-appctl ofproto/trace destination pod to source pod indicates success from fedora-deployment-7d49fddf69-t4hqw to fedora-deployment-7d49fddf69-chmvh\novn-detrace source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\novn-detrace destination pod to source pod indicates success from fedora-deployment-7d49fddf69-t4hqw to fedora-deployment-7d49fddf69-chmvh\n</code></pre></p> <p>In order to see the actual trace output of the <code>ovn-trace</code> and <code>ovs-appctl ofproto/trace</code> commands, one can increase the loglevel to <code>2</code>, and in order to see debug output for the ovnkube-trace application one can raise the loglevel to <code>5</code>: <pre><code># ovnkube-trace -src-namespace default -src fedora-deployment-7d49fddf69-chmvh -dst-namespace default -dst fedora-deployment-7d49fddf69-t4hqw -udp -dst-port 53 -loglevel 2\nI0823 21:33:18.112821 2457963 ovnkube-trace.go:1157] Log level set to: 2\nI0823 21:33:18.857705 2457963 ovnkube-trace.go:693] ovn-trace source pod to destination pod Output:\n# udp,reg14=0x3,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.6,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=52888,tp_dst=53\n\ningress(dp=\"ovn-worker2\", inport=\"default_fedora-deployment-7d49fddf69-chmvh\")\n------------------------------------------------------------------------------\n 0. ls_in_check_port_sec (northd.c:8583): 1, priority 50, uuid de664d3a\n    reg0[15] = check_in_port_sec();\n    next;\n 4. ls_in_pre_acl (northd.c:5991): ip, priority 100, uuid d9a60156\n    reg0[0] = 1;\n    next;\n 5. ls_in_pre_lb (northd.c:6178): ip, priority 100, uuid 4f6825b1\n    reg0[2] = 1;\n    next;\n 6. ls_in_pre_stateful (northd.c:6201): reg0[2] == 1, priority 110, uuid 82c039a6\n    ct_lb_mark;\n\nct_lb_mark /* default (use --ct to customize) */\n------------------------------------------------\n 7. ls_in_acl_hint (northd.c:6297): !ct.new &amp;&amp; ct.est &amp;&amp; !ct.rpl &amp;&amp; ct_mark.blocked == 0, priority 4, uuid 0b20013d\n    reg0[8] = 1;\n    reg0[10] = 1;\n    next;\n 9. ls_in_acl_action (northd.c:6764): reg8[30..31] == 0, priority 500, uuid 3eec76bb\n    reg8[30..31] = 1;\n    next(8);\n 9. ls_in_acl_action (northd.c:6764): reg8[30..31] == 1, priority 500, uuid b327f7af\n    reg8[30..31] = 2;\n    next(8);\n 9. ls_in_acl_action (northd.c:6753): 1, priority 0, uuid b0f710dc\n    reg8[16] = 0;\n    reg8[17] = 0;\n    reg8[18] = 0;\n    reg8[30..31] = 0;\n    next;\n15. ls_in_pre_hairpin (northd.c:7786): ip &amp;&amp; ct.trk, priority 100, uuid 88ef5011\n    reg0[6] = chk_lb_hairpin();\n    reg0[12] = chk_lb_hairpin_reply();\n    next;\n19. ls_in_acl_after_lb_action (northd.c:6764): reg8[30..31] == 0, priority 500, uuid 01461328\n    reg8[30..31] = 1;\n    next(18);\n19. ls_in_acl_after_lb_action (northd.c:6764): reg8[30..31] == 1, priority 500, uuid a22021af\n    reg8[30..31] = 2;\n    next(18);\n19. ls_in_acl_after_lb_action (northd.c:6753): 1, priority 0, uuid 2c98ee3d\n    reg8[16] = 0;\n    reg8[17] = 0;\n    reg8[18] = 0;\n    reg8[30..31] = 0;\n    next;\n27. ls_in_l2_lkup (northd.c:9407): eth.dst == { 0a:58:a9:fe:01:01, 0a:58:0a:f4:02:01 }, priority 50, uuid b29511a2\n    outport = \"stor-ovn-worker2\";\n    output;\n\negress(dp=\"ovn-worker2\", inport=\"default_fedora-deployment-7d49fddf69-chmvh\", outport=\"stor-ovn-worker2\")\n---------------------------------------------------------------------------------------------------------\n 0. ls_out_pre_acl (northd.c:5878): ip &amp;&amp; outport == \"stor-ovn-worker2\", priority 110, uuid 8b1bef96\n    next;\n 1. ls_out_pre_lb (northd.c:5878): ip &amp;&amp; outport == \"stor-ovn-worker2\", priority 110, uuid d7c53e71\n    next;\n 3. ls_out_acl_hint (northd.c:6297): !ct.new &amp;&amp; ct.est &amp;&amp; !ct.rpl &amp;&amp; ct_mark.blocked == 0, priority 4, uuid 26b08bb5\n    reg0[8] = 1;\n    reg0[10] = 1;\n    next;\n 5. ls_out_acl_action (northd.c:6764): reg8[30..31] == 0, priority 500, uuid ea58bb8e\n    reg8[30..31] = 1;\n    next(4);\n 5. ls_out_acl_action (northd.c:6764): reg8[30..31] == 1, priority 500, uuid 03897328\n    reg8[30..31] = 2;\n    next(4);\n 5. ls_out_acl_action (northd.c:6753): 1, priority 0, uuid bcfbe611\n    reg8[16] = 0;\n    reg8[17] = 0;\n    reg8[18] = 0;\n    reg8[30..31] = 0;\n    next;\n 9. ls_out_check_port_sec (northd.c:5843): 1, priority 0, uuid 79358872\n    reg0[15] = check_out_port_sec();\n    next;\n10. ls_out_apply_port_sec (northd.c:5848): 1, priority 0, uuid 12ba0dbe\n    output;\n    /* output to \"stor-ovn-worker2\", type \"patch\" */\n\ningress(dp=\"ovn_cluster_router\", inport=\"rtos-ovn-worker2\")\n-----------------------------------------------------------\n 0. lr_in_admission (northd.c:11790): eth.dst == { 0a:58:a9:fe:01:01, 0a:58:0a:f4:02:01 } &amp;&amp; inport == \"rtos-ovn-worker2\" &amp;&amp; is_chassis_resident(\"cr-rtos-ovn-worker2\"), priority 50, uuid e40942af\n    xreg0[0..47] = 0a:58:0a:f4:02:01;\n    next;\n 1. lr_in_lookup_neighbor (northd.c:11956): 1, priority 0, uuid 897d00f0\n    reg9[2] = 1;\n    next;\n 2. lr_in_learn_neighbor (northd.c:11965): reg9[2] == 1 || reg9[3] == 0, priority 100, uuid 234da6ee\n    next;\n12. lr_in_ip_routing_pre (northd.c:12190): 1, priority 0, uuid b1a3a6af\n    reg7 = 0;\n    next;\n13. lr_in_ip_routing (northd.c:10603): reg7 == 0 &amp;&amp; ip4.dst == 10.244.1.0/24, priority 73, uuid 1ed4e720\n    ip.ttl--;\n    reg8[0..15] = 0;\n    reg0 = 100.88.0.4;\n    reg1 = 100.88.0.2;\n    eth.src = 0a:58:a8:fe:00:02;\n    outport = \"rtots-ovn-worker2\";\n    flags.loopback = 1;\n    next;\n14. lr_in_ip_routing_ecmp (northd.c:12285): reg8[0..15] == 0, priority 150, uuid a1ea724a\n    next;\n15. lr_in_policy (northd.c:9741): ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 10.244.0.0/16, priority 102, uuid 1c6af09a\n    reg8[0..15] = 0;\n    next;\n16. lr_in_policy_ecmp (northd.c:12452): reg8[0..15] == 0, priority 150, uuid 3841a2fc\n    next;\n17. lr_in_arp_resolve (northd.c:12665): outport == \"rtots-ovn-worker2\" &amp;&amp; reg0 == 100.88.0.4, priority 100, uuid 792c14a1\n    eth.dst = 0a:58:a8:fe:00:04;\n    next;\n21. lr_in_arp_request (northd.c:13083): 1, priority 0, uuid f21b210a\n    output;\n\negress(dp=\"ovn_cluster_router\", inport=\"rtos-ovn-worker2\", outport=\"rtots-ovn-worker2\")\n---------------------------------------------------------------------------------------\n 0. lr_out_chk_dnat_local (northd.c:14444): 1, priority 0, uuid 2f6e84ed\n    reg9[4] = 0;\n    next;\n 6. lr_out_delivery (northd.c:13129): outport == \"rtots-ovn-worker2\", priority 100, uuid 81cdee53\n    output;\n    /* output to \"rtots-ovn-worker2\", type \"patch\" */\n\ningress(dp=\"transit_switch\", inport=\"tstor-ovn-worker2\")\n--------------------------------------------------------\n 0. ls_in_check_port_sec (northd.c:8583): 1, priority 50, uuid de664d3a\n    reg0[15] = check_in_port_sec();\n    next;\n 5. ls_in_pre_lb (northd.c:5875): ip &amp;&amp; inport == \"tstor-ovn-worker2\", priority 110, uuid 69169a39\n    next;\n27. ls_in_l2_lkup (northd.c:9329): eth.dst == 0a:58:a8:fe:00:04, priority 50, uuid da101703\n    outport = \"tstor-ovn-worker\";\n    output;\n\negress(dp=\"transit_switch\", inport=\"tstor-ovn-worker2\", outport=\"tstor-ovn-worker\")\n-----------------------------------------------------------------------------------\n 9. ls_out_check_port_sec (northd.c:5843): 1, priority 0, uuid 79358872\n    reg0[15] = check_out_port_sec();\n    next;\n10. ls_out_apply_port_sec (northd.c:5848): 1, priority 0, uuid 12ba0dbe\n    output;\n    /* output to \"tstor-ovn-worker\", type \"remote\" */\n\novn-trace source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\nI0823 21:33:18.858296 2457963 ovnkube-trace.go:704] Search string matched:\noutput to \"tstor-ovn-worker\"\n(...)\nI0823 21:33:19.169751 2457963 ovnkube-trace.go:693] ovs-appctl ofproto/trace source pod to destination pod Output:\nFlow: udp,in_port=7,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.6,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=12345,tp_dst=53\n\nbridge(\"br-int\")\n----------------\n 0. in_port=7, priority 100, cookie 0x6c1d0b4a\n    set_field:0x13-&gt;reg13\n    set_field:0xb-&gt;reg11\n    set_field:0x6-&gt;reg12\n    set_field:0x3-&gt;metadata\n    set_field:0x3-&gt;reg14\n    resubmit(,8)\n 8. metadata=0x3, priority 50, cookie 0xde664d3a\n    set_field:0/0x1000-&gt;reg10\n    resubmit(,73)\n    73. ip,reg14=0x3,metadata=0x3,dl_src=0a:58:0a:f4:02:03,nw_src=10.244.2.3, priority 90, cookie 0x6c1d0b4a\n            set_field:0/0x1000-&gt;reg10\n    move:NXM_NX_REG10[12]-&gt;NXM_NX_XXREG0[111]\n     -&gt; NXM_NX_XXREG0[111] is now 0\n    resubmit(,9)\n 9. metadata=0x3, priority 0, cookie 0x57b52622\n    resubmit(,10)\n10. metadata=0x3, priority 0, cookie 0x98964ef0\n    resubmit(,11)\n11. metadata=0x3, priority 0, cookie 0x72c60524\n    resubmit(,12)\n12. ip,metadata=0x3, priority 100, cookie 0xd9a60156\n    set_field:0x1000000000000000000000000/0x1000000000000000000000000-&gt;xxreg0\n    resubmit(,13)\n13. ip,metadata=0x3, priority 100, cookie 0x4f6825b1\n    set_field:0x4000000000000000000000000/0x4000000000000000000000000-&gt;xxreg0\n    resubmit(,14)\n14. ip,reg0=0x4/0x4,metadata=0x3, priority 110, cookie 0x82c039a6\n    ct(table=15,zone=NXM_NX_REG13[0..15],nat)\n    nat\n     -&gt; A clone of the packet is forked to recirculate. The forked pipeline will be resumed at table 15.\n     -&gt; Sets the packet to an untracked state, and clears all the conntrack fields.\n\nFinal flow: udp,reg0=0x5,reg11=0xb,reg12=0x6,reg13=0x13,reg14=0x3,metadata=0x3,in_port=7,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.6,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=12345,tp_dst=53\nMegaflow: recirc_id=0,eth,udp,in_port=7,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.128.0.0/9,nw_frag=no,tp_src=0x2000/0xe000\nDatapath actions: ct(zone=19,nat),recirc(0x12)\n\n===============================================================================\nrecirc(0x12) - resume conntrack with default ct_state=trk|new (use --ct-next to customize)\nReplacing src/dst IP/ports to simulate NAT:\n Initial flow: \n Modified flow: \n===============================================================================\n\nFlow: recirc_id=0x12,ct_state=new|trk,ct_zone=19,eth,udp,reg0=0x5,reg11=0xb,reg12=0x6,reg13=0x13,reg14=0x3,metadata=0x3,in_port=7,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.6,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=12345,tp_dst=53\n\nbridge(\"br-int\")\n----------------\n    thaw\n        Resuming from table 15\n15. ct_state=+new-est+trk,metadata=0x3, priority 7, cookie 0xf14d8019\n    set_field:0x80000000000000000000000000/0x80000000000000000000000000-&gt;xxreg0\n    set_field:0x200000000000000000000000000/0x200000000000000000000000000-&gt;xxreg0\n    resubmit(,16)\n16. ct_state=-est+trk,ip,metadata=0x3, priority 1, cookie 0x8dfd1f3e\n    set_field:0x2000000000000000000000000/0x2000000000000000000000000-&gt;xxreg0\n    resubmit(,17)\n17. reg8=0/0xc0000000,metadata=0x3, priority 500, cookie 0x3eec76bb\n    set_field:0x4000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,16)\n16. ct_state=-est+trk,ip,metadata=0x3, priority 1, cookie 0x8dfd1f3e\n    set_field:0x2000000000000000000000000/0x2000000000000000000000000-&gt;xxreg0\n    resubmit(,17)\n17. reg8=0x40000000/0xc0000000,metadata=0x3, priority 500, cookie 0xb327f7af\n    set_field:0x8000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,16)\n16. ct_state=-est+trk,ip,metadata=0x3, priority 1, cookie 0x8dfd1f3e\n    set_field:0x2000000000000000000000000/0x2000000000000000000000000-&gt;xxreg0\n    resubmit(,17)\n17. metadata=0x3, priority 0, cookie 0xb0f710dc\n    set_field:0/0x1000000000000-&gt;xreg4\n    set_field:0/0x2000000000000-&gt;xreg4\n    set_field:0/0x4000000000000-&gt;xreg4\n    set_field:0/0xc000000000000000-&gt;xreg4\n    resubmit(,18)\n18. metadata=0x3, priority 0, cookie 0xcae3d5b1\n    resubmit(,19)\n19. metadata=0x3, priority 0, cookie 0x66860608\n    resubmit(,20)\n20. metadata=0x3, priority 0, cookie 0x9db4b75e\n    resubmit(,21)\n21. metadata=0x3, priority 0, cookie 0x414a15a0\n    resubmit(,22)\n22. metadata=0x3, priority 0, cookie 0xebbc94cf\n    resubmit(,23)\n23. ct_state=+trk,ip,metadata=0x3, priority 100, cookie 0x88ef5011\n    set_field:0/0x80-&gt;reg10\n    resubmit(,68)\n    68. No match.\n            drop\n    move:NXM_NX_REG10[7]-&gt;NXM_NX_XXREG0[102]\n     -&gt; NXM_NX_XXREG0[102] is now 0\n    set_field:0/0x80-&gt;reg10\n    resubmit(,69)\n    69. No match.\n            drop\n    move:NXM_NX_REG10[7]-&gt;NXM_NX_XXREG0[108]\n     -&gt; NXM_NX_XXREG0[108] is now 0\n    resubmit(,24)\n24. metadata=0x3, priority 0, cookie 0x7fd98606\n    resubmit(,25)\n25. metadata=0x3, priority 0, cookie 0xd3d8976\n    resubmit(,26)\n26. metadata=0x3, priority 0, cookie 0x1ecb6f2e\n    resubmit(,27)\n27. reg8=0/0xc0000000,metadata=0x3, priority 500, cookie 0x1461328\n    set_field:0x4000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,26)\n26. metadata=0x3, priority 0, cookie 0x1ecb6f2e\n    resubmit(,27)\n27. reg8=0x40000000/0xc0000000,metadata=0x3, priority 500, cookie 0xa22021af\n    set_field:0x8000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,26)\n26. metadata=0x3, priority 0, cookie 0x1ecb6f2e\n    resubmit(,27)\n27. metadata=0x3, priority 0, cookie 0x2c98ee3d\n    set_field:0/0x1000000000000-&gt;xreg4\n    set_field:0/0x2000000000000-&gt;xreg4\n    set_field:0/0x4000000000000-&gt;xreg4\n    set_field:0/0xc000000000000000-&gt;xreg4\n    resubmit(,28)\n28. ip,reg0=0x2/0x2002,metadata=0x3, priority 100, cookie 0xd8583947\n    ct(commit,zone=NXM_NX_REG13[0..15],nat(src),exec(set_field:0/0x1-&gt;ct_mark))\n    nat(src)\n    set_field:0/0x1-&gt;ct_mark\n     -&gt; Sets the packet to an untracked state, and clears all the conntrack fields.\n    resubmit(,29)\n29. metadata=0x3, priority 0, cookie 0x41080b67\n    resubmit(,30)\n30. metadata=0x3, priority 0, cookie 0x936e8520\n    resubmit(,31)\n31. metadata=0x3, priority 0, cookie 0x6d369d0e\n    resubmit(,32)\n32. metadata=0x3, priority 0, cookie 0x119a6138\n    resubmit(,33)\n33. metadata=0x3, priority 0, cookie 0x1d30f590\n    resubmit(,34)\n34. metadata=0x3, priority 0, cookie 0x74ef3d9\n    resubmit(,35)\n35. metadata=0x3,dl_dst=0a:58:0a:f4:02:01, priority 50, cookie 0xb29511a2\n    set_field:0x1-&gt;reg15\n    resubmit(,37)\n37. priority 0\n    resubmit(,38)\n38. priority 0\n    resubmit(,40)\n40. priority 0\n    resubmit(,41)\n41. reg15=0x1,metadata=0x3, priority 100, cookie 0x7c79d0e2\n    set_field:0xb-&gt;reg11\n    set_field:0x6-&gt;reg12\n    resubmit(,42)\n42. priority 0\n    set_field:0-&gt;reg0\n    set_field:0-&gt;reg1\n    set_field:0-&gt;reg2\n    set_field:0-&gt;reg3\n    set_field:0-&gt;reg4\n    set_field:0-&gt;reg5\n    set_field:0-&gt;reg6\n    set_field:0-&gt;reg7\n    set_field:0-&gt;reg8\n    set_field:0-&gt;reg9\n    resubmit(,43)\n43. ip,reg15=0x1,metadata=0x3, priority 110, cookie 0x8b1bef96\n    resubmit(,44)\n44. ip,reg15=0x1,metadata=0x3, priority 110, cookie 0xd7c53e71\n    resubmit(,45)\n45. metadata=0x3, priority 0, cookie 0xe59d971f\n    resubmit(,46)\n46. ct_state=-trk,metadata=0x3, priority 5, cookie 0xd4c65410\n    set_field:0x100000000000000000000000000/0x100000000000000000000000000-&gt;xxreg0\n    set_field:0x200000000000000000000000000/0x200000000000000000000000000-&gt;xxreg0\n    resubmit(,47)\n47. metadata=0x3, priority 0, cookie 0x17ae0ddf\n    resubmit(,48)\n48. reg8=0/0xc0000000,metadata=0x3, priority 500, cookie 0xea58bb8e\n    set_field:0x4000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,47)\n47. metadata=0x3, priority 0, cookie 0x17ae0ddf\n    resubmit(,48)\n48. reg8=0x40000000/0xc0000000,metadata=0x3, priority 500, cookie 0x3897328\n    set_field:0x8000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,47)\n47. metadata=0x3, priority 0, cookie 0x17ae0ddf\n    resubmit(,48)\n48. metadata=0x3, priority 0, cookie 0xbcfbe611\n    set_field:0/0x1000000000000-&gt;xreg4\n    set_field:0/0x2000000000000-&gt;xreg4\n    set_field:0/0x4000000000000-&gt;xreg4\n    set_field:0/0xc000000000000000-&gt;xreg4\n    resubmit(,49)\n49. metadata=0x3, priority 0, cookie 0x39bbcf9\n    resubmit(,50)\n50. metadata=0x3, priority 0, cookie 0xaf9ffaea\n    resubmit(,51)\n51. metadata=0x3, priority 0, cookie 0xbee9000e\n    resubmit(,52)\n52. metadata=0x3, priority 0, cookie 0x79358872\n    set_field:0/0x1000-&gt;reg10\n    resubmit(,75)\n    75. No match.\n            drop\n    move:NXM_NX_REG10[12]-&gt;NXM_NX_XXREG0[111]\n     -&gt; NXM_NX_XXREG0[111] is now 0\n    resubmit(,53)\n53. metadata=0x3, priority 0, cookie 0x12ba0dbe\n    resubmit(,64)\n64. priority 0\n    resubmit(,65)\n65. reg15=0x1,metadata=0x3, priority 100, cookie 0x7c79d0e2\n    clone(ct_clear,set_field:0-&gt;reg11,set_field:0-&gt;reg12,set_field:0-&gt;reg13,set_field:0x8-&gt;reg11,set_field:0xe-&gt;reg12,set_field:0x1-&gt;metadata,set_field:0x2-&gt;reg14,set_field:0-&gt;reg10,set_field:0-&gt;reg15,set_field:0-&gt;reg0,set_field:0-&gt;reg1,set_field:0-&gt;reg2,set_field:0-&gt;reg3,set_field:0-&gt;reg4,set_field:0-&gt;reg5,set_field:0-&gt;reg6,set_field:0-&gt;reg7,set_field:0-&gt;reg8,set_field:0-&gt;reg9,resubmit(,8))\n    ct_clear\n    set_field:0-&gt;reg11\n    set_field:0-&gt;reg12\n    set_field:0-&gt;reg13\n    set_field:0x8-&gt;reg11\n    set_field:0xe-&gt;reg12\n    set_field:0x1-&gt;metadata\n    set_field:0x2-&gt;reg14\n    set_field:0-&gt;reg10\n    set_field:0-&gt;reg15\n    set_field:0-&gt;reg0\n    set_field:0-&gt;reg1\n    set_field:0-&gt;reg2\n    set_field:0-&gt;reg3\n    set_field:0-&gt;reg4\n    set_field:0-&gt;reg5\n    set_field:0-&gt;reg6\n    set_field:0-&gt;reg7\n    set_field:0-&gt;reg8\n    set_field:0-&gt;reg9\n    resubmit(,8)\n 8. reg14=0x2,metadata=0x1,dl_dst=0a:58:0a:f4:02:01, priority 50, cookie 0xe40942af\n    set_field:0xa580af402010000000000000000/0xffffffffffff0000000000000000-&gt;xxreg0\n    resubmit(,9)\n 9. metadata=0x1, priority 0, cookie 0x897d00f0\n    set_field:0x4/0x4-&gt;xreg4\n    resubmit(,10)\n10. reg9=0/0x8,metadata=0x1, priority 100, cookie 0x234da6ee\n    resubmit(,11)\n11. metadata=0x1, priority 0, cookie 0x7f28e0ff\n    resubmit(,12)\n12. metadata=0x1, priority 0, cookie 0x1ee2fb50\n    resubmit(,13)\n13. metadata=0x1, priority 0, cookie 0x17a7cfa8\n    resubmit(,14)\n14. metadata=0x1, priority 0, cookie 0x56f6cf85\n    resubmit(,15)\n15. metadata=0x1, priority 0, cookie 0x600f694f\n    resubmit(,16)\n16. metadata=0x1, priority 0, cookie 0xda085702\n    resubmit(,17)\n17. metadata=0x1, priority 0, cookie 0x30578698\n    resubmit(,18)\n18. metadata=0x1, priority 0, cookie 0x58ec3bea\n    resubmit(,19)\n19. metadata=0x1, priority 0, cookie 0xe6096bd7\n    resubmit(,20)\n20. metadata=0x1, priority 0, cookie 0xb1a3a6af\n    set_field:0/0xffffffff-&gt;xxreg1\n    resubmit(,21)\n21. ip,reg7=0,metadata=0x1,nw_dst=10.244.1.0/24, priority 73, cookie 0x1ed4e720\n    dec_ttl()\n    set_field:0/0xffff00000000-&gt;xreg4\n    set_field:0xa8fe0004000000000000000000000000/0xffffffff000000000000000000000000-&gt;xxreg0\n    set_field:0xa8fe00020000000000000000/0xffffffff0000000000000000-&gt;xxreg0\n    set_field:0a:58:a8:fe:00:02-&gt;eth_src\n    set_field:0x4-&gt;reg15\n    set_field:0x1/0x1-&gt;reg10\n    resubmit(,22)\n22. reg8=0/0xffff,metadata=0x1, priority 150, cookie 0xa1ea724a\n    resubmit(,23)\n23. ip,metadata=0x1,nw_src=10.244.0.0/16,nw_dst=10.244.0.0/16, priority 102, cookie 0x1c6af09a\n    set_field:0/0xffff00000000-&gt;xreg4\n    resubmit(,24)\n24. reg8=0/0xffff,metadata=0x1, priority 150, cookie 0x3841a2fc\n    resubmit(,25)\n25. reg0=0xa8fe0004,reg15=0x4,metadata=0x1, priority 100, cookie 0x792c14a1\n    set_field:0a:58:a8:fe:00:04-&gt;eth_dst\n    resubmit(,26)\n26. metadata=0x1, priority 0, cookie 0x8be057a3\n    resubmit(,27)\n27. metadata=0x1, priority 0, cookie 0x5ea30a7d\n    resubmit(,28)\n28. metadata=0x1, priority 0, cookie 0x79529c6a\n    resubmit(,29)\n29. metadata=0x1, priority 0, cookie 0xf21b210a\n    resubmit(,37)\n37. priority 0\n    resubmit(,38)\n38. priority 0\n    resubmit(,40)\n40. priority 0\n    resubmit(,41)\n41. reg15=0x4,metadata=0x1, priority 100, cookie 0x93ec5715\n    set_field:0x8-&gt;reg11\n    set_field:0xe-&gt;reg12\n    resubmit(,42)\n42. priority 0\n    set_field:0-&gt;reg0\n    set_field:0-&gt;reg1\n    set_field:0-&gt;reg2\n    set_field:0-&gt;reg3\n    set_field:0-&gt;reg4\n    set_field:0-&gt;reg5\n    set_field:0-&gt;reg6\n    set_field:0-&gt;reg7\n    set_field:0-&gt;reg8\n    set_field:0-&gt;reg9\n    resubmit(,43)\n43. metadata=0x1, priority 0, cookie 0x2f6e84ed\n    set_field:0/0x10-&gt;xreg4\n    resubmit(,44)\n44. metadata=0x1, priority 0, cookie 0x90d7e25c\n    resubmit(,45)\n45. metadata=0x1, priority 0, cookie 0xe36440a6\n    resubmit(,46)\n46. metadata=0x1, priority 0, cookie 0xd3dbde6f\n    resubmit(,47)\n47. metadata=0x1, priority 0, cookie 0x519ebaf8\n    resubmit(,48)\n48. metadata=0x1, priority 0, cookie 0xc8fb3dc1\n    resubmit(,49)\n49. reg15=0x4,metadata=0x1, priority 100, cookie 0x81cdee53\n    resubmit(,64)\n64. reg10=0x1/0x1,reg15=0x4,metadata=0x1, priority 100, cookie 0x93ec5715\n    push:NXM_OF_IN_PORT[]\n    set_field:ANY-&gt;in_port\n    resubmit(,65)\n    65. reg15=0x4,metadata=0x1, priority 100, cookie 0x93ec5715\n            clone(ct_clear,set_field:0-&gt;reg11,set_field:0-&gt;reg12,set_field:0-&gt;reg13,set_field:0x3-&gt;reg11,set_field:0x11-&gt;reg12,set_field:0xff0003-&gt;metadata,set_field:0x2-&gt;reg14,set_field:0-&gt;reg10,set_field:0-&gt;reg15,set_field:0-&gt;reg0,set_field:0-&gt;reg1,set_field:0-&gt;reg2,set_field:0-&gt;reg3,set_field:0-&gt;reg4,set_field:0-&gt;reg5,set_field:0-&gt;reg6,set_field:0-&gt;reg7,set_field:0-&gt;reg8,set_field:0-&gt;reg9,resubmit(,8))\n            ct_clear\n            set_field:0-&gt;reg11\n            set_field:0-&gt;reg12\n            set_field:0-&gt;reg13\n            set_field:0x3-&gt;reg11\n            set_field:0x11-&gt;reg12\n            set_field:0xff0003-&gt;metadata\n            set_field:0x2-&gt;reg14\n            set_field:0-&gt;reg10\n            set_field:0-&gt;reg15\n            set_field:0-&gt;reg0\n            set_field:0-&gt;reg1\n            set_field:0-&gt;reg2\n            set_field:0-&gt;reg3\n            set_field:0-&gt;reg4\n            set_field:0-&gt;reg5\n            set_field:0-&gt;reg6\n            set_field:0-&gt;reg7\n            set_field:0-&gt;reg8\n            set_field:0-&gt;reg9\n            resubmit(,8)\n         8. metadata=0xff0003, priority 50, cookie 0xde664d3a\n            set_field:0/0x1000-&gt;reg10\n            resubmit(,73)\n            73. No match.\n                    drop\n            move:NXM_NX_REG10[12]-&gt;NXM_NX_XXREG0[111]\n             -&gt; NXM_NX_XXREG0[111] is now 0\n            resubmit(,9)\n         9. metadata=0xff0003, priority 0, cookie 0x57b52622\n            resubmit(,10)\n        10. metadata=0xff0003, priority 0, cookie 0x98964ef0\n            resubmit(,11)\n        11. metadata=0xff0003, priority 0, cookie 0x72c60524\n            resubmit(,12)\n        12. metadata=0xff0003, priority 0, cookie 0x351dd7a3\n            resubmit(,13)\n        13. ip,reg14=0x2,metadata=0xff0003, priority 110, cookie 0x69169a39\n            resubmit(,14)\n        14. metadata=0xff0003, priority 0, cookie 0x5c78cf83\n            resubmit(,15)\n        15. metadata=0xff0003, priority 65535, cookie 0x8ac9010\n            resubmit(,16)\n        16. metadata=0xff0003, priority 65535, cookie 0x973723d7\n            resubmit(,17)\n        17. metadata=0xff0003, priority 0, cookie 0xc722ae8c\n            resubmit(,18)\n        18. metadata=0xff0003, priority 0, cookie 0xcae3d5b1\n            resubmit(,19)\n        19. metadata=0xff0003, priority 0, cookie 0x66860608\n            resubmit(,20)\n        20. metadata=0xff0003, priority 0, cookie 0x9db4b75e\n            resubmit(,21)\n        21. metadata=0xff0003, priority 0, cookie 0x414a15a0\n            resubmit(,22)\n        22. metadata=0xff0003, priority 0, cookie 0xebbc94cf\n            resubmit(,23)\n        23. metadata=0xff0003, priority 0, cookie 0x74d6cf40\n            resubmit(,24)\n        24. metadata=0xff0003, priority 0, cookie 0x7fd98606\n            resubmit(,25)\n        25. metadata=0xff0003, priority 0, cookie 0xd3d8976\n            resubmit(,26)\n        26. metadata=0xff0003, priority 0, cookie 0x1ecb6f2e\n            resubmit(,27)\n        27. metadata=0xff0003, priority 0, cookie 0xba824d52\n            resubmit(,28)\n        28. metadata=0xff0003, priority 0, cookie 0xa5e04afc\n            resubmit(,29)\n        29. metadata=0xff0003, priority 0, cookie 0x41080b67\n            resubmit(,30)\n        30. metadata=0xff0003, priority 0, cookie 0x936e8520\n            resubmit(,31)\n        31. metadata=0xff0003, priority 0, cookie 0x6d369d0e\n            resubmit(,32)\n        32. metadata=0xff0003, priority 0, cookie 0x119a6138\n            resubmit(,33)\n        33. metadata=0xff0003, priority 0, cookie 0x1d30f590\n            resubmit(,34)\n        34. metadata=0xff0003, priority 0, cookie 0x74ef3d9\n            resubmit(,35)\n        35. metadata=0xff0003,dl_dst=0a:58:a8:fe:00:04, priority 50, cookie 0xda101703\n            set_field:0x4-&gt;reg15\n            resubmit(,37)\n        37. priority 0\n            resubmit(,38)\n        38. priority 0\n            resubmit(,40)\n        40. reg15=0x4,metadata=0xff0003, priority 100, cookie 0xb6badb74\n            set_field:0xff0003/0xffffff-&gt;tun_id\n            set_field:0x4-&gt;tun_metadata0\n            move:NXM_NX_REG14[0..14]-&gt;NXM_NX_TUN_METADATA0[16..30]\n             -&gt; NXM_NX_TUN_METADATA0[16..30] is now 0x2\n            output:4\n             -&gt; output to kernel tunnel\n            resubmit(,41)\n        41. priority 0\n            drop\n    pop:NXM_OF_IN_PORT[]\n     -&gt; NXM_OF_IN_PORT[] is now 7\n\nFinal flow: recirc_id=0x12,eth,udp,reg0=0x300,reg11=0xb,reg12=0x6,reg13=0x13,reg14=0x3,reg15=0x1,metadata=0x3,in_port=7,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.6,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=12345,tp_dst=53\nMegaflow: recirc_id=0x12,ct_state=+new-est-rel-rpl-inv+trk,ct_mark=0/0xf,eth,udp,in_port=7,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.0/24,nw_ecn=0,nw_ttl=64,nw_frag=no\nDatapath actions: ct(commit,zone=19,mark=0/0x1,nat(src)),set(tunnel(tun_id=0xff0003,dst=172.18.0.2,ttl=64,tp_dst=6081,geneve({class=0x102,type=0x80,len=4,0x20004}),flags(df|csum|key))),set(eth(src=0a:58:a8:fe:00:02,dst=0a:58:a8:fe:00:04)),set(ipv4(ttl=63)),5\n\novs-appctl ofproto/trace source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\nI0823 21:33:19.170205 2457963 ovnkube-trace.go:704] Search string matched:\n-&gt; output to kernel tunnel\n(...)\n</code></pre></p>"}]}